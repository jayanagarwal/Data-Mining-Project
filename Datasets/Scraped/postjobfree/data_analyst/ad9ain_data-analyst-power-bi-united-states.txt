Manoj Gowda

PROFESSIONAL SUMMARY:
Around * years of experience as a Data Analyst in supporting business solution and analyzing business Data using Python, R, SQL, Power BI, and Tableau.
Extensive experience in project management best practices, processes, & methodologies including Waterfall, and Agile (SCRUM).
Strong professional experience with emphasis on Analysis, design, development, testing, maintenance and implementation of Data Mapping, Data Validation, and Requirement gathering in Data warehousing Environment.
Experience in collecting, processing, and performing statistical analyses on large volumes of data.
Strong working experience in the Data Analysis, Design, Development, Implementation and Testing of Data Warehousing using Data Conversions, Data Extraction, Data Transformation and Data Loading (ETL).
Experience in handling huge set of data using cloud clusters like Amazon Web Services (AWS) archiving the data.
Analyzed complex, high volume, high-dimensionality data from multiple sources using different data analysis techniques and/or tools to formulate recommendations, learning, and test plans.
Experienced R in data wrangling, cleansing, and preparation for regression analysis.
Outstanding pre-eminence in Data extraction, Data cleaning, Data Loading, Statistical Data Analysis, Exploratory Data Analysis, Data Wrangling, Predictive Modeling using R, Python and Data visualization using Tableau.
Experience in Cloud Services such as Amazon Web Services (AWS) - EC2, VPC, EMR, RDS, Redshift, Glue, Lambda functions, Step functions, cloud Watch, SNS, Dynamo, SQS, and S3 to assist with Big Data tools, solve the data storage issue.
Knowledge on HL7 standards, and HIPAA (Health information portability and accountability act), ICD-10 - International Statistical Classification of Diseases and Related Health Problems.
Knowledge of healthcare standard Health Level Seven (HL7). Excellent Knowledge in Electronic Medical Record (EMR) / Electronic Health Records (EHR) modules and process flow and Good Knowledge of various HL7 EMR systems.
Profound knowledge in Machine Learning Algorithms like Linear, Non-linear and Logistic Regression, Reinforcement Learning, NLP, Fuzzy Logic, Random Forests, Ensemble Methods, Decision Tree, Gradient Boosting, K-NN, SVM, Naive Bayes, Clustering (K-means), and Deep Learning.
Hands-on experience in importing and exporting data in various formats (CSV, Excel, SQL) using R.
Knowledge of health care services regulatory environment in compliance with HIPAA, ICD, MITA, and MMIS.
Extensive experience in using Python Libraries PySpark, Pytest, Pymongo, cxOracle, PyExcel, Boto3, Psycopg, embedPy, NumPy and Beautiful Soup.
Proficient knowledge in statistics, mathematics, and analytics to interpreting and analyzing data to drive successful business solutions.
Hands on experience in writing queries in SQL, Python, and R to extract, transform and load (ETL) data from large datasets using Data Staging.
Knowledgeable in OLTP (Transaction database), and OLAP Datawarehouse data management.
Expertise in data integration techniques and tools such as INFORMATICA, and SSIS for combining the data from different data bases or sources.
Strong understanding of OLAP, Data Warehousing, Dimensional Data Modeling, identifying Facts and Dimensions, designing Star and Snowflake Schemas.
Experience in providing custom solutions like Eligibility criteria, Match and Basic contribution calculations for major clients using Informatica and reports using Tableau/ Power BI.
Good Knowledge in NoSQL databases like MongoDB, Cassandra and HBase. Experienced in RDMBS such as Oracle, MySQL, and IBM DB2 databases.
Expertise in Data Extraction, Cleansing, Transformation, Data Bricks, Data Analysis, Logical/ Physical Relational/ Dimensional Database Modeling & Designing.
Strong working experience in Building, designing story-telling visualizations and dashboards from different databases and monitored workflows for managers at different departments using MS Excel, Tableau, and Power BI.
Extensive experience with Tableau Desktop and Tableau Server as a Data Analyst.
Good industry knowledge, analytical & problem-solving skills and ability to work well within a team as well as an individual.
Excellent communication skills, interpersonal skills, self-motivated, quick learner, team player, and multi-tasker.

Education:
B.E in Information Science and Engineering, Visvesvaraya Technological University (VTU), Bengaluru, India

Technical Skills:
Methodologies
Agile/ Scrum
Languages
Python, SQL, R
BI Tools
Tableau, Microsoft Power BI, PowerPivot
Data Warehousing Tools
Snowflake, Star Flake
Python Libraries
Scikitlearn, Pandas, NumPy, SciPy, Matplotlib, Seaborn, Plotly, PySpark, Pytest, Pymongo, cxOracle, PyExcel, Boto3, Psycopg, embedPy, Beautiful Soup
ETL
Informatica PowerCenter, AWS Glue
Microsoft Tools
Microsoft Office (Excel, PowerPoint, and Word), MS Project
Databases
SQL Server, MySQL, PostgreSQL, SQLite, MongoDB, Cassandra, Oracle
Data Analysis
Web Scraping, Statistical Modelling, Hypothesis testing, Predictive Modelling
Data Visualization
Tableau, Microsoft Power BI
Data Mining Algorithms
Decision Trees, Clustering, Random Forest, Regression

Professional Experience:

Client: Fidelity Investments, Boston, MA Sep 2023 – Till Date
Role: Data Analyst
Description: Fidelity Investments is a leading financial services firm known for its comprehensive offerings in investment management, brokerage services, and retirement planning. Fidelity provides a wide range of financial products, including mutual funds, ETFs, and retirement accounts. It is renowned for its robust research and technology platforms, which support both individual investors and institutional clients. The company emphasizes low-cost investment options and personalized financial advice.

Responsibilities:
Applied Software Development Life Cycle (SDLC) principles to plan, design, develop, and deploy software solutions, ensuring comprehensive project lifecycle management and successful delivery.
Applied Agile and Scrum methodologies to manage project timelines, deliverables, and team collaboration, leading to improvement in project efficiency.
Developed and implemented Python-based data analysis pipelines, leveraging Pandas, NumPy, and SciPy to extract, clean, and process large datasets.
Utilized Scikit Learn to build and evaluate machine learning models, enhancing predictive analytics capabilities and decision-making processes.
Created and maintained interactive visualizations using Matplotlib, Seaborn, and Plotly, enabling data-driven insights and facilitating stakeholder presentations.
Utilized R - dplyr and tidyr packages to perform data manipulation and cleaning, optimizing the preparation of datasets for analysis and modeling.
Developed and deployed custom R packages for statistical analysis and visualization, streamlining data exploration and reporting processes.
Leveraged AWS cloud services, including EC2 and S3, to design scalable infrastructure for data storage and computational needs, reducing costs.
Created interactive data visualizations with R libraries such as ggplot2 and plotly, improving data interpretation and stakeholder presentations.
Deployed and managed AWS VPC and Route 53 services to configure secure network environments and domain name systems, ensuring reliable access and security.
Implemented AWS CloudWatch for monitoring and logging, enabling proactive issue detection and performance optimization for cloud-based applications.
Configured AWS CloudFront and Glacier to optimize content delivery and implement cost-effective archival storage solutions, improving access speed and reducing storage costs.
Managed and automated AWS Elastic Load Balancer (ELB) configurations to ensure high availability and fault tolerance for web applications.
Engineered data integration solutions using Informatica, automating data workflows and ensuring accurate, timely data availability for analysis.
Utilized AWS RDS and EBS for efficient database management and elastic block storage, enhancing data accessibility and system performance.
Utilized AWS IAM for identity and access management, ensuring secure and compliant access control for cloud resources and data.
Developed custom Power BI reports with advanced DAX (Data Analysis Expressions) calculations, enabling detailed data analysis and accurate performance metrics.
Configured and managed AWS Elastic Search to implement powerful search capabilities within applications, improving data retrieval speed.
Developed data transformation workflows using AWS Glue to prepare data for analytics and reporting, enhancing data processing efficiency.
Executed data migration projects to Snowflake, optimizing data warehousing solutions and reducing query times.
Conducted advanced statistical analyses and modeling using R, applying various techniques to drive business insights and strategies.
Created complex data models and relationships in Power BI, integrating multiple data sources to provide comprehensive insights and drive business strategies.
Managed end-to-end deployment processes through the AWS console, coordinating infrastructure setup, application deployment, and ongoing maintenance.
Integrated Power BI dashboards to provide real-time insights and data-driven decisions, improving reporting efficiency.

Environment: SDLC, Agile/ Scrum, Python, Scikit Learn, Pandas, NumPy, SciPy, Matplotlib, Seaborn, Plotly, R, R Packages, AWS cloud services like EC2, S3, VPC, Route 53, CloudWatch, CloudFront, Glacier, ELB, AMI, Elastic Search, IAM, AWS console, RDS, EBS, Power BI, Informatica, AWS Glue, Snowflake

Client: Wingate Healthcare, Needham, MA Jan 2023 – Sep2023
Role: Data Analyst
Description: Wingate Living is a family owned and privately held company to meet the challenges facing the senior long term care industry. Wingate is the Northeast’s premier provider of healthcare and hospitality for seniors and delivers exceptional independent and assisted living, memory care, and skilled nursing. Wingate Healthcare specialized in long term care, Independent Living, Retirement Community, Respite Care, Memory Care, Dementia, Assisted Care, Medication Management, Homecare, Healthcare, and Orthopedic Rehab.

Responsibilities:
Worked in Agile/ Scrum framework throughout all the phases of Software Development Life Cycle (SDLC).
Automated reports which go out on weekly, monthly, and quarterly basis using tools such as Python, Pandas, R to support an error-free reporting environment, promote data security and data integrity.
Built various graphs for business decision making using Python Packages like NumPy, Pandas, and Matplotlib for Numerical analysis.
Participated in research planning and testing of data provisioning and analysis involving oracle databases and Amazon Web Services (AWS).
Managed and configured various AWS cloud services like EC2, S3 bucket, RDS, EBS, VPC, Glacier, ELB, AMI, Elastic Search, and IAM through AWS console and API integration.
Assisted with quantitative data analysis with Health Care, Biometric and Survey Data HIPAA EDI transactions such as 835, 837 (P, D, I) 276, 277, and 278.
Used HIPAA guidelines and regulations to keep track of healthcare transactions like Eligibility Request/ Response, Request and Response for Claims Status, Prior Authorization, Claims Vision, and Claims Payment.
Gathered user requirements and directed developers for practical deliverables with also patient domain configuration to accept HL7 messages into MDM.
Conducted testing to confirm that HL7 messages from each application confirm to the appropriate HL7 interface specification.
Responsible for performing DDL, DML and DCL operations on Snowflake tables as per the requirement.
Used R and Python to prepare tables to present data or results.
Interpreting data, analyzing results using statistical techniques
Developing and implementing data analyses, data collection systems and other strategies that optimize statistical efficiency and quality.
Creating complex SQL queries and scripts to extract and aggregate data to validate the accuracy of the data.
Created ETL (Extract, Transform, and Load) processes to extract data from various sources and load it into the Snowflake data warehouse.
Extensive use of Informatica development using Lookup, Normalizer, Stored Procedure, Filter, Expression, Aggregator, router, joiner transformation.
Established and monitored key performance indicators (KPIs) for logistics processes, creating real-time performance dashboards in Power BI.
Designed interactive data visualizations and user-friendly reports to enhance communication with non-technical stakeholders.
Utilized SQL queries to extract logistics data, ensuring data accuracy and consistency through data transformation and cleaning. Implemented automation in Power BI for efficient and timely distribution of critical logistics reports to stakeholders.
Create Power BI dashboard design incorporating user experience industry report standards.

Environment: SDLC, Agile/Scrum, Python, Pandas, R, Matplotlib, AWS (EC2, S3, RDS, VPC, Glacier, EBS, ELB, AMI, Elastic Search, IAM), HIPAA EDI Transactions (835, 837, 276, 277, 278), HL7 Messages, Snowflake (DDL, DML, DCL), ETL, Informatica, Power BI, SQL Queries, Power BI.

Client: DXL, Canton, MA Nov 2021 – Dec 2022
Role: Data Analyst
Description: Destination XL Group, Inc. (DXLG) is a leading retailer of Men's Big and Tall apparel with 290 retail and outlet store locations throughout the United States operated under the business subsidiaries DXL and Casual Male XL. DXL Big + Tall is an Omni channel clothing retailer offering the most extensive assortment of brands and exclusive styles allowing Big + Tall men the freedom to choose to wear what they want. DXL specialized in Retail Apparel, Men's Big & Tall Apparel, E-Commerce, plus size Home & Living, Extended size shoes, and Men's Clothing.

Responsibilities
Involved in working with Agile Framework throughout all the phases of Software Development Life Cycle (SDLC).
Developed and maintained extensive reports using Excel user defined python and R functions.
Built and maintained various machine learning models using Python that support important segments of the business operations such as logistics, on-site job scheduling or demand prediction.
Used Python packages (Pandas, NumPy, and Scikitlearn) for feature engineering and machine learning models.
Developed and implemented a predictive algorithm using Random Forest to forecast subscriber counts based on location and frequency data.
Construct the AWS data pipelines using VPC, S3, Auto Scaling Groups (ASG), EBS, Snowflake, IAM, CloudFormation, Route 53, CloudWatch, CloudFront, and CloudTrail.
Developed Python scripts for initiating and terminating EC2 instances in AWS. Used R Machine Learning library to build and evaluate different models.
Implemented Apache Airflow for authoring, scheduling, and monitoring Data Pipelines.
Used AWS S3 to store large amount of data in identical/ similar repository. Created and attached volumes on to EC2 instances.
Enhanced the algorithm’s performance through Hyperparameter Tuning utilizing GridSearchCV. This optimization improved the training process, resulting in boosted forecasting accuracy and operational efficiency.
Achieved an outstanding 88% accuracy rate in subscriber count predictions, showcasing the effectiveness of the model in capturing intricate patterns and relationships within the data.
Created Snowflake and SQL Server views and tasks optimized for data deliverability.
Working on snowflake schema for managing and arranging the data in order in data warehouses Learned and implemented data integration & managements using ETL (Informatica)
Extensively used INFORMATICA POWERCENTER for extraction, transformation, and loading process.
Managed Power BI Service functions including refresh schedules, datasets and workspace access, report lifecycle through development, test and production using Power BI Deployment Pipeline tools.
Designing and customizing data models for Data warehouse and supporting data from multiple sources on real time by using snowflake schema.
Performed advanced SQL data types, schemas, views, joins, groups, and querying.
Extract data of support tickets, work on data cleaning, analyze results using statistical techniques with Advanced MS Excel (Pivot table, VLOOKUP) as well as MS Power BI (DAX queries).

Environment: SDLC, Agile, Scrum, Python, R, Machine Learning, Pandas, NumPy, Scikit-learn, AWS Data Pipelines, VPC, S3, Auto Scaling Groups, EBS, IAM, CloudFormation, Route 53, CloudWatch, CloudFront, CloudTrail, EC2, Apache Airflow, GridSearchCV, Snowflake, SQL Server, ETL, Informatica PowerCenter, Power BI Service, Power BI, Snowflake Schema, SQL, MS Excel, MS Power BI, Pivot Table, VLOOKUP, DAX Queries.

Client: Global Logistics Solutions, Bangalore, India Aug 2018 - Oct 2021
Role: Data Analyst
Description: Global Logistics started with the vision to provide Easy, Efficient and Economical integrated logistics solutions. Global Logistics has grown to become one of the largest Neutral LCL consolidators and FCL freight forwarders in India by expanding geographically and by adding trade lanes for both Import and Export consoles.

Responsibilities:
Implemented agile methodologies to improve project efficiency and collaboration, resulting in streamlined development processes.
Developed a robust data deduplication program utilizing Python and SQL, successfully eliminating duplicate entries from a vast business database with an impressive 92% accuracy rate.
Performed exploratory data analysis (EDA) to find insight such as difference efficiency among different devices using ggplot2 in R.
Restructured complex spreadsheet information into user-friendly forms, enabling businesses to efficiently access and manage data. This reorganization resulted in an 80% resource reallocation efficiency increase.
Migrated scripts and programs from an on-premises environment to the Amazon Web Services (AWS) cloud environment.
Developed Python scripts for data pre-processing for predictive models, including missing value imputation, label encoding, and feature engineering.
Leveraged advanced data analysis and programming skills to streamline operations, enhance data quality through cleaning and deduplication, and achieve more effective resource allocation, resulting in substantial cost savings and operational efficiency improvements for businesses.
Performed Exploratory Data Analysis to identify trends using Tableau and Python (Matplotlib, Seaborn, and Plotly Dash).
Design and construct of AWS Data pipelines using various resources in AWS including AWS API Gateway to receives response from AWS lambda and retrieve data from snowflake using lambda function and convert the response into Json format using Database as Snowflake, DynamoDB, AWS Lambda function and AWS S3.
Developed various Mappings with the collection of all sources, targets, and transformations using Informatica Designer.
Implemented principles of Data warehousing, Fact Tables, Dimension Tables, and Star schema modeling.
Worked on Tools like Snowflake schema to manage and maintain the rows and columns of the tables in the data warehouse.
Utilized AWS Lambda functions to trigger data processing jobs, automate workflows, and respond to real-time events in the data ecosystem.
Managed and optimized SQL Queries with Views and Store Procedures.
Used analysis and visualization tools such as MySQL, Python, and Tableau to generate risk analyze report using operations data.
Involved in publishing of various kinds of interactive dashboards, and workbooks from Tableau Desktop to Tableau Servers.
Moving the profiled data from SQL databases to the Data Warehouse by mapping the data and using ETL Tools like Tableau.
Created Tableau Dashboards with interactive views, trends, and drill downs along with user level security.

Environment: Agile, Python, SQL, ggplot2, R, AWS, Tableau, Matplotlib, Seaborn, Plotly Dash, AWS Data Pipelines, AWS API Gateway, AWS Lambda, Informatica Designer, Star Schema Modeling, Snowflake Schema, AWS Lambda Functions, SQL Optimization, MySQL, Tableau, Tableau Server.
Contact this candidate