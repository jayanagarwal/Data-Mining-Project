Umadeepthi Antharvedi
Data Analyst
Email – ad7sp4@r.postjobfree.com Phone - +1-469-***-****

Summary:
•Data Analyst with 10 years of experience possesses a strong background in quantitative analysis, data visualization, and database management. With excellent communication skills and a keen attention to detail, I am committed to delivering high-quality results that exceed expectations.
•Worked in Various stages of Data warehousing life cycle of development database logical and physical design, ETL process, performance tuning, Developed and implemented data policies and standards.
•Strong working experience in the Data Analysis, Business Analysis, Design, Development, Implementation, Data Governance and Testing of Data Warehousing using Data Conversions, Data Extraction, Data Transformation and Data Loading (ETL) using Informatica and DataStage.
•Developed and maintained interactive data visualizations and dashboards using Tableau, enabling business stakeholders to easily understand and analyze complex data and creating score cards for the DQ rules.
•Experience in creating Data Governance Policies, Business Glossary, Data Dictionary, Reference Data, Metadata, Data Lineage, and establish Data Quality Rules and ensure these are met across the organization.
•Outstanding Data analysis skills including Data mapping from source to target database schemas, and implement data policies, standards, and procedures that align with business strategy and compliance.
•Involved in creating detailed mappings to illustrate how data will be extracted, transformed, and loaded from source to target systems using ETL tools like Informatica and Datastage.
•Strong Data Warehousing experience using Informatica Power Center 10.2/9.6/9.5/8.6, Data Stage, Power Exchange, IDQ, Hyperion and Power BI.
•Strong experience using Oracle 11i/10g/9i, SQL, PL/SQL, SQL Server 2008/2005, My SQL, Netezza, Mongo DB, and DB2.
•Strong end to end experience in Hadoop, Spark/Scala development using different Big Data tools.
•Write complex SQL scripts to analyze data present in different Databases/Data warehouses and Data lake house like Snowflake, Netezza.
•Experience in implementing stored procedures, functions, packages, tables, views, Cursors, triggers.
•Worked with the data steward and data owners in creating metadata, Lineage, Data Quality rules and guidelines.
•Experience in developing Python scripts for automation and performing functional tests and data validation.
•Experience in writing SQL Queries, Stored procedures, functions, packages, tables, views and triggers.
•In-depth knowledge in working with Distributed Computing Systems and parallel processing techniques to efficiently deal with Big Data.
•Experience in Data Engineer role dealing with AWS, Azure, and Apache Hadoop Ecosystem like HDFS, Map Reduce, Hive, Sqoop, Oozie, HBase, Spark-Scala, Kafka, and Big Data Analytics.
•Strong experience building Spark applications using Scala and Python as programming languages.
•Worked with Azure services such as HDInsight, Databricks, Data Lake, Blob Storage, Data Factory, Storage Explorer, SQL DB, SQL DWH, and Cosmos DB.
•Built data pipelines using Azure Data Factory, and Azure Databricks and loaded the data to Azure Data Lake, Azure SQL Database.
•Experienced in requirement analysis, application development, application migration, and maintenance using Software Development Lifecycle (SDLC) and Python/Java technologies.
•Hands-on experience in configuring a Hadoop cluster in a professional environment and on Amazon Web Services (AWS) using an EC2 instance.
•Detailed exposure on Azure tools such as Azure Data Lake, Azure Data Bricks, Azure Data Factory, HDInsight, Azure SQL Server, and Azure DevOps.
•Extensively worked on Amazon web service (AWS) using different services like EC2, S3, Relational Database Service (RDS), Dynamo DB, Elastic load balancing (ELB), Auto scaling, Elastic Block Store, Elastic Map Reduce (EMR)

Technical Skills:

Hadoop Ecosystem
HDFS, SQL, YARN, MapReduce, Hive, Sqoop, Spark, Yarn, Kafka
Programming Languages
Python, PL/SQL, Scala
Big Data Platforms
Hortonworks, Cloudera
Reporting Tools
Power BI, Tableau, Hyperion
Schedulers
Oozie, Control M, Redwood, Stone Branch, Autosys
Azure Platform
Azure Data Factory, Azure Data Lake Analytics, Data bricks, Azure Monitoring
AWS Platform
EC2, S3, EMR, Redshift, Denodo, DynamoDB, Aurora, Glue, Boto3, Lambda, Snowflake
Operating Systems
Linux, Windows, UNIX
Databases
Oracle, DB2, Netezza, MySQL, MongoDB, Snowflake, SQL Server, PostgreSQL, Redshift, Salesforce
Development Methods
Agile/Scrum, Waterfall
IDE’s
PyCharm, IntelliJ
Version Control
GitHub Actions, Jenkins, Bitbucket, SVN
Cloud Technologies
Amazon AWS, Microsoft Azure
Scripting
Bash/Shell, Python
ETL Technologies
Informatica PowerCenter, MDM, DataStage, Alteryx, Talend

Experience:

Verizon - Irving, TX Oct 2021 – Apr 2024
Senior Data Analyst
Performed Data Analysis and Data validation by writing complex SQL queries using TOAD against the ORACLE database.
Performed Data Analytics, Data Modelling, Data Mapping, Data Extraction, Data Cleaning, Data Validation, Data Manipulation, Compliance Reporting, Data Quality and ETL Process.
Performed data analysis with a solid understanding of Business Requirements Gathering, Data warehousing, Business Intelligence, Data Mapping, and Data Modelling.
Performed Metadata Management, Data Analysis, Data Integrity, Data Acquisition, Data Steward, Data Mining, Data Profiling & Quality, Data Governance.
Developed SQL Stored Procedures and Views and performed in-depth testing of new and existing systems.
Used Data Integrator to load data from various sources such as MS Access, MS Excel, and CSV files to MS SQL Server.
Experience in using AI and ML techniques to extract insights from data, identify patterns and trends, and develop data-driven solutions for business problems. Worked in Various stages of Data warehousing life cycle of development database logical and physical design, ETL process, performance tuning.
Played a pivotal role in optimizing the project's networking modules, ensuring seamless data exchange, and enhancing the overall user experience.
Implemented essential shell utilities in Python, improving the project’s efficiency and standardization of codebase.
Data Acquisition, Data Warehousing (gathering requirements, design, development, implementation, testing, and documentation), Data Modelling (analysis using Star Schema and Snowflake for Fact and Dimensions Tables), Data Processing and Data Transformations (Mapping, Cleansing, Monitoring, Performance Tuning).
Designed and implemented an enterprise data lake on AWS, integrating multiple data sources (CRM, ERP, external data) which resulted in a 40% increase in data processing efficiency.
Automated repetitive data processing tasks using Python, reducing the time required to generate reports.
Adept at writing Data Mapping Documents, Data Transformation Rules, and maintaining Data Dictionary and Interface requirements documents.
Utilized Python libraries like Pandas, NumPy, and Matplotlib, urllib2 to loading the dataset, summarizing the dataset, visualizing the dataset, evaluating some algorithms, and making some predictions.
Developed applications for different scientific, mathematical, and statistical calculations using the NumPy, Pandas, and other libraries.
Developed Python scripts for automation and performing functional tests and data validation.
Wrote SQL Queries and stored procedures, functions, packages, tables, views, and triggers.
•Involved in full software development life cycle, architecting scalable platforms, object-oriented programming, database design, and agile methodologies.
•Developed Web Services with Python programming language using MVC frameworks like Django.
•Experience in a Python based environment, along with data analytics, data wrangling and Excel data extracts.
•Proficient in all phases of software development life cycle (SDLC) including gathering requirements, Analysis, Design, Implementation, Testing, Maintenance and Support of Python web applications.
•Requirement gathering, Use Case development, Business Process flow, Business Process Modeling.
•Extensively used UML to develop various use cases, class diagram and sequence diagrams
•Used Scikit-Learn and Stats models in Python for Machine Learning and Data Mining.
•Dealt with AWS, Azure, Apache Hadoop Ecosystem like HDFS, Map Reduce, Hive, Sqoop, Oozie, HBase, Spark-Scala, Kafka and Big Data Analytics
•Worked on Amazon web service (AWS) using difference services like EC2, S3, Relational Database Service (RDS), Dynamo DB, Elastic load balancing (ELB), Auto scaling, Elastic Block Store, Elastic Map Reduce (EMR)
Environment: Git Hub, MySQL, LINUX. Spark, Spark-Streaming, Spark SQL, AWS EMR, AWS Athena, EC2, S3, Redshift, Apache, Python, Spark, Shell scripting, Jenkins, Eclipse, Oracle, Git, Alteryx, MySQL, Tableau

USAA (Bank)- Plano, TX July 2019 – Sep 2021
Data Analyst II
Developing Stored PL/SQL Procedures and Packages to automatically create and drop table indexes.
Conducted data analysis and modelling to extract insights and inform business decisions.
Worked in Various stages of Data warehousing life cycle of development database logical and physical design, ETL process, performance tuning.
Excellent knowledge in Data Analysis, Data Validation, Data Cleansing, Data Verification and identifying data mismatches.
Develop Spark jobs to transform data and apply business transformation rules to process data across enterprise and application specific layers.
Create Data Quality rules/mappings/standards to enhance data quality checks for domain specific data using DataStage.
Developed Python applications for data processing, analysis, and visualization.
Involved in monitoring and maintaining the quality of master data across various domains.
Involved in cleaning, enriching, and preserving the integrity of the data using Informatica MDM.
Used Data Integrator to load data from various sources such as MS Access, MS Excel, CSV files to MS SQL Server.
Extracted data from SQL Server database tables and Oracle database tables into flat data files and Excel sheet.
Used SQL Loader and SQL Developer to load data from flat data files and excel sheet into Oracle database.
Experience in developing Spark applications using Spark-SQL in Data bricks for data extraction, transformation, and aggregation from multiple file formats for Analyzing & transforming the data to uncover insights into the customer usage patterns.
Skilled in Azure cloud technologies like Azure Data Factory, Azure Databricks, Azure Data Lake Storage (ADLS), Azure Synapse Analytics, Azure SQL Database, Azure Analytical services.
Created CI-CD Build Release pipelines to automate the deployments to DEV, Stage and Production Environment using Azure DevOps and On-premises infrastructure.
Created Azure Resources via ARM templates for deploying Azure Web Apps and Virtual machines in Azure as part of Infrastructure as code.
Implemented Azure Services: Azure AD, Azure DNS, Azure Traffic Manager, Azure Web Apps, Azure ARM, Azure Service Fabric, Container Services, Azure Data Factory, Azure Functions, Azure Logic Apps.
Develop, integrate, and automate build and verify SharePoint pipelines.
Implemented large Lambda architectures using Azure Data platform capabilities like Azure Data Lake, Azure Data Factory, HDInsight, and Azure SQL Server.
Environment: Python, DataStage, Git Hub, MySQL, LINUX, Snowflake, Salesforce, Azure, Data Lake, Event hubs, Azure SQL, Azure Monitoring, Azure DevOps, Tableau.

UCLA Health, Dallas, TX Apr 2017- June 2019
Data Analyst
•Strong working experience in the Data Analysis, Business Analysis, Design, Development, Implementation and Testing of Data Warehousing using Data Conversions, Data Extraction, Data Transformation and Data Loading (ETL).
•Developed and maintained interactive data visualizations and dashboards using Power BI and Hyperion enabling business stakeholders to easily understand and analyze complex data.
•Working experience on interactive dashboard and Reports in Tableau for monitoring the operation performance on a day-to-day basis.
•Experience in creating Data Governance Policies, Business Glossary, Data Dictionary, Reference Data, Metadata, Data Lineage, and Data Quality Rules.
•Outstanding Data analysis skills including Data mapping from source to target database schemas, Data Cleansing, and processing, writing data extract scripts/programming of data conversion, and researching complex data problems.
•Used IBM's DataStage software, an ETL (Extract, Transform, Load) tool used primarily for integrating and transforming large volumes of data.
•Extract data from various source systems, transforming it to fit operational needs, and loading it into the end target - often a data warehouse or data mart.
•Developed and maintained data pipelines to process and transform large datasets for use in Power BI visualizations, using technologies like SQL and Python.
•Write complex SQL scripts to analyze data present in different Databases/Data lake houses and warehouses like Snowflake, Teradata.
•Designed front end and backend of the application using Python on Django Web Framework.
•Wrote Python scripts to parse XML documents and load the data in the database.
•Identify and correct errors or inconsistencies in claims data such as duplicates, missing values, or format issues.
•Process claims data from various sources, including direct uploads, data feeds and manual data entry.
•Integrate data from different sources to create a unified database that provides a comprehensive view of all claims.
•Developed Merge jobs in Python to extract and load data into MySQL database.
•Wrote SQL Queries and implemented stored procedures, functions, packages, tables, views, Cursors, triggers.
•Worked on database migration projects from Oracle to Mongo DB and oracle to S3.
•Used collections in Python for manipulating and looping through different user defined objects.
•Developed and executed User Acceptance Testing portion of test plan.
Environment: Python, Informatica, Power BI, Hyperion, SQL, GitHub, SQLite, MYSQL, Mongo DB, GitHub, Django, Oracle, and Windows.

Century Ply – Hyderabad, IND Jan 2012 – Apr 2015
Business Analyst
•Gathering Functional Requirement and preparing Business Requirements Document (BRD).
•Preparing Technical Design Document and Preparing Microsoft Project Plan.
•Create daily, weekly, and monthly reports and visualization.
•Development of Custom Web Parts and Creating External Content Types.
•Coding for Retrieving and updating data from SharePoint to SAP.
•Unit testing of functionality and giving demos.
•Interacting with client on requirements.
Environment: Windows, Microsoft Office, SAP, Cognos.

Education:
Master of Business Administration (M.B.A) from Badruka Institute of Foreign Trade (Hyderabad, India)- HR & Marketing- May 2011.
Bachelor of Technology (BTECH) from GMRIT (Rajam, India) - Information Technology – Apr 2008.
Contact this candidate