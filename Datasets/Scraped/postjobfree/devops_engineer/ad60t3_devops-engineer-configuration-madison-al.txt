Arunakumari Makkena
Email:
ad60t3@r.postjobfree.com
Mobile: 256-***-****
Address: *** ***** **** ******* **
35758. Working: Remote
DevOps Engineer with 12 years of extensive experience in orchestrating and optimizing cloud- based infrastructure and automation pipelines. Proficient in leveraging cutting-edge technologies and methodologies to streamline development, deployment, and operations processes. Adept at designing, implementing, and maintaining scalable and resilient architectures on Azure. Skilled in version control, containerization, configuration management, and continuous integration/continuous deployment (CI/CD) practices. Proven ability to collaborate effectively with cross-functional teams to drive innovation and deliver business value. Seeking to leverage expertise in Azure, Git, Docker, Ansible, Kubernetes, Maven, and Jenkins to drive organizational success and foster continuous improvement in a dynamic environment.
Technical Skills
Version Control System: GIT.
Cloud: Azure, GCP, AWS
DevOps tools: Docker, Kubernetes, Terraform
CI/CD Tools: Azure DevOps, Jenkins
Build tools: Maven
Scripting Language: Shell scripting and Bash
Ticketing tools: Jira, ServiceNow.
Configuration Management: Ansible
Virtualization and Containerization: Docker, Kubernetes. Summary
Cloud Infrastructure: Extensive experience in designing, deploying, and managing cloud infrastructure on Azure, including EC2, S3, VMs, Lambda, and more. Proficient in optimizing Azure environments for scalability, reliability, and cost-efficiency.
Version Control Systems: Expertise in utilizing Git and GitLab for version control management, including branching, merging, and repository administration, ensuring efficient collaboration and code management workflows.
Containerization: Hands-on experience with Docker for containerization, including container orchestration and management using Kubernetes and OpenShift. Proficient in building and deploying containerized applications for improved scalability and portability.
Configuration Management: Skilled in configuration management tools such as Ansible and Chef for automating infrastructure provisioning, configuration, and management across heterogeneous environments, ensuring consistency and repeatability.
Scripting Languages: Strong proficiency in shell scripting for automation of system administration tasks and Python scripting for infrastructure automation, monitoring, and deployment tasks, enhancing operational efficiency and productivity.
Continuous Integration/Continuous Deployment (CI/CD): Experienced in setting up and maintaining CI/CD pipelines using Jenkins, CloudBees and Maven, automating software build, testing, and deployment processes to accelerate time-to-market and improve software quality.
Monitoring and Logging: Familiarity with monitoring and logging solutions such as ELK Stack, Prometheus, and Grafana for real-time monitoring, logging, and alerting, enabling proactive identification and resolution of issues.
Additional Tools: Experience with Maven for building and managing Java-based projects, facilitating efficient software development workflows and dependency management.
Professional Experience
Client: CGI Duration: Oct 2018 to Mar 2024
The Project is for an American worldwide health services organization. It offers Medicare and Medicaid products and health, life, and accident insurance coverages primarily to individuals in the U.S and selected international markets.
Role: Devops Engineer
Job Role and Responsibility:
Demonstrated hands-on experience in designing, implementing, and managing Azure cloud infrastructure.
Proficient in utilizing azure services such as EC2, S3, RDS, Lambda, and VMS for building scalable and reliable cloud solutions. Experienced in building and maintaining CI/CD pipelines using tools like Jenkins, GitLab CI/CD, and Bamboo.
Proficient in Infrastructure as Code principles and tools such as Terraform and CloudFormation for automating infrastructure provisioning and management.
Manage and optimize CI/CD pipelines using tools such as Jenkins or CloudBees to automate software delivery processes, including building, testing, and deployment.
Implement infrastructure as code (IaC) using tools like Terraform or CloudFormation to automate the provisioning and configuration of cloud resources.
Focuses on building, testing, and deploying code using CloudBees pipelines.
Designs and manages CI/CD pipelines using code (YAML) for infrastructure as code (IaC) principles.
Designed, implemented, and managed Azure cloud infrastructure, including EC2 instances, S3 buckets and Lambda functions, resulting in improved scalability and reliability.
Automated deployment processes using Docker and Kubernetes, reducing deployment time by 50% and minimizing downtime.
Implemented CI/CD pipelines with Jenkins, GitLab CI/CD, and AWS Code Pipeline to enable rapid and reliable software delivery.
Utilized Ansible and Chef for configuration management, ensuring consistency across multiple environments.
Developed and maintained shell scripts and Python utilities for system automation, monitoring, and troubleshooting.
Collaborated with development teams to optimize application performance and ensure adherence to DevOps best practices.
Conducted regular performance tuning and capacity planning to optimize resource utilization and cost efficiency on Azure.
Worked on Azure DevOps service to create catalogs, databases, tables and running queries to get the log results stored in S3 bucket and Automate Athena set up using terraform scripts.
Created cloud watch dashboards and work with application SRE teams to debug the Prod related issues on containerized applications.
Developed Terraform templates to create the ECS Cluster, ECS services along with the capacity provider to handle autoscaling part with high availability and fault tolerance.
Worked on Terraform enterprise tool to manage terraform builds by integrating with bitbucket and azure cloud.
Worked on Kubernetes clusters for deploying containerized applications across multiple nodes.
Worked on Splunk monitoring tool to create alerts and find the root causes in case of an application failures and support SRE teams in case of any blockers. Client: Continuity of Care Record (CCR) Duration: Jan 2015 to Oct 2018 Worked with AWS deployment team, being involved with deployment of updates and operations on applications hosted on Web Application Servers. Role: Software Engineer
Job Role & Responsibilities:
• Worked closely with development, messaging, network support and database teams, as the application was integrated with other products.
• Involved extensively in trouble shooting the issues and finding out root cause analysis.
• Gathering and analyzing the requirements from the business users to provide the compatible solution.
• Creating the Ingestion pipelines using ADF (Azure Data Factory) based on the Source requirements.
• Scheduled the ADF Pipeline with Event based and Scheduled triggers.
• Working and analyzing the data with Adobe analytics in cloud computing.
• Used Python and Shell scripts to Automate Teradata ELT and Admin activities.
• Developed UNIX scripts to automate different tasks involved as part of loading process.
• Worked on Tableau software for the reporting needs.
• Ingesting source data from various relational and non-relational data sources into Azure Data Lake through ADF pipelines.
• Collecting business requirement from Client, and converting huge volume of Data to required ready to use information to the end users.
• To identify & fix the bugs in the code using SQL Server Management Studio.
• Develop and maintain data warehouse solutions, such as using Amazon Redshift, Google Big Query, or Snowflake.
• Implement star schema and snowflake schema designs for analytical processing.
• Developed and designed an automation framework using Python.
• Extract Transform and Load data from On-prem Systems to Azure Data Storage services using a combination of Azure Data Factory, Spark SQL. Data Ingestion to one or more Azure Services
- Azure Data Lake, Azure Storage, Azure SQL and processing the data in Azure Data bricks.
Clint: BNP Paribas Duration: Sep 2011 to Dec 2014
Role: ETL Developer
Job role & Responsibilities:
• Designing developing and deploying ETL processing using Talend.
• Design ETL workflows and mapping to meet data integration and transformation needs.
• Develop Talend jobs for data extraction, transformation, and loading
(ETL) processes.
• Integrate data from various sources, including databases, flat files, and APIs.
• Develop solutions for real-time data integration and processing.
• Perform data transformation and cleansing using Talend components.
• Implement data validation and quality checks within ETL processes.
• Work with stakeholders to gather and understand data requirements.
• Collaborate with business analysts and data scientists to ensure data solutions meet business needs.
• Develop and execute tet plans to ensure the accuracy and reliability of ETL processes.
• Validate the output of Talend jobs against business requirements and data duality standards.
Education:
MSc (Computer science) from Bharathidasan University with 72% as aggregate in 2008 from India.
Contact this candidate