Mahitha.H
+1-678-***-**** ad34uw@r.postjobfree.com GitHub

PROFESSIONAL SUMMARY

Highly motivated data analyst with 3 years of experience in leveraging data to drive insights and inform business decisions. Proven ability to wrangle, analyze, and visualize data from diverse sources to identify trends, solve problems, and support strategic initiatives. Skilled in SQL querying, data manipulation, and data visualization tools. Eager to contribute data expertise to a dynamic team and help organizations achieve their goals.
TECHNICAL SKILLS

Programming Languages : Python, SQL, Bash/Unix Shell Scripting, PySpark, Linux, Java, HTML, CSS
Python Libraries : Pandas, NumPy, Matplotlib, TensorFlow
ETL Tools : Informatica Power Center, SSIS, Azure Data Factory, SSIS, DBT
Cloud Tools : Snowflake, Google Cloud Platform (GCP), Microsoft Azure
Databases & Data Warehouses : MS SQL Server, MySQL, Teradata, Synapse Analytics, MongoDB.
Reporting Tools : Power BI, Excel, Tableau, SSRS
Big Data Framework : Spark, Hadoop
Other Tools and Services : JIRA, ServiceNow, Git, GitHub, Bitbucket.
PROFESSIONAL EXPERIENCE problem solving

DATA Analyst Adpmn Jan 2023 – Present
Bridge the gap between business and technical teams by gathering, analyzing, and translating business requirements into actionable technical specifications for data-driven projects. Proactively collaborate with other departments to collect client needs and access relevant data resources.
Leverage data analysis, statistical modeling, and strategic recommendations to optimize operational efficiency, reduce costs, and enhance revenue streams for the organization.
Champion data quality and consistency by leading the Data Correction and Validation process. Utilized data utilities to identify and resolve mismatches between different departments, ensuring data integrity and reliability.
Possess expertise in querying diverse data sources including Oracle DB, Teradata, and Azure Synapse Analytics using complex SQL queries. Performed data analysis and profiling to gain deeper insights.
Uncover root causes of data discrepancies between different business systems through thorough analysis of business rules and data models. Communicated these findings to the development/bug fix team for resolution.
Utilize Python for data gathering, cleaning, and wrangling, ensuring data preparation for further analysis. Participated in the Data Reconciliation Process, validating loaded data against user reports for accuracy.
Demonstrate proficiency in working with various data formats (CSV, XML, JSON, flat files, TSV, XLSX, PDF, parquet) for effective data manipulation and management.
Develop advanced SQL queries to troubleshoot tables, functions, joins, triggers, views, and stored procedures, contributing to efficient database management.
Transform data into compelling narratives using Power BI. Employed DAX functions, slicers, and filters to manipulate data and generate interactive reports, effectively communicating insights to stakeholders.
Control user and group access to published dashboards within the Power BI environment, ensuring data security and accessibility based on user roles.
Monitor and report on trends in key performance indicators (KPIs), providing actionable data-driven insights with root cause analysis. Highlighted potential areas of concern for informed decision-making.
Possess extensive knowledge of Azure services including Data Factory, Data Lake Storage, Synapse Analytics, and Azure SQL Database, leveraging them for data management and analysis tasks.
Proficiently utilize Jira, Confluence, and ServiceNow for comprehensive project management, documentation, and IT service management, ensuring project efficiency and effective collaboration.
DATA Analyst People Prime worldwide Jan 2019 - Nov 2020
Conducted cost-benefit analysis comparing on-premises and Azure Cloud hosting solutions, identifying efficiency gains and cost reductions.
Collaborated with Application Owners and Cloud Architects to develop a phased migration strategy, minimizing operational disruptions and optimizing resource utilization.
Prepared and delivered monthly reports to stakeholders, providing insights into budget utilization, cost savings, and post-migration performance metrics.
Utilized MS SQL Server for efficient data analysis, mapping, and manipulation, writing complex SQL queries to extract and analyze large datasets.
Designed and implemented data storage solutions, including NoSQL MongoDB databases, ensuring data quality, governance, and scalability.
Developed ETL workflows using Informatica Power Center, reducing data processing time by 20% with improved accuracy.
Created Python scripts to extract and transform data from internal client sites, facilitating seamless data loading into databases.
Generated reports in Power BI on infrastructure budget analysis, inventory auditing, and financial statements, presenting findings to stakeholders.
Developed interactive dashboards and reports in Power BI using DAX formulas, Power Pivot, and Power Query, enabling insightful analysis and reporting.
Collaborated with cross-functional teams to develop data solutions, improve business processes, and develop industry trend reports and dashboards.
Utilized ServiceNow for issue tracking and resolution, ensuring consistent service level agreements.
Managed tasks and data analysis workflows using Agile methodology through JIRA.

EDUCATION

Wichita State University - Wichita, KS
Master of Science – Computer Science, May 2023

ACADEMIC PROJECTS

Automated Customer Support System Wichita State University: Developed inventory management system using Azure Functions for serverless computing. Utilized Azure SQL Database for data storage. Integrated Azure Cognitive Services for image recognition. Implemented Azure DevOps for continuous deployment. Deployed on Azure App Service for scalability.
Real-Time Fraud Detection System Wichita State University: Developed a real-time fraud detection system for financial transactions. Utilized Support Vector Machines (SVM) and advanced classification algorithms. Employed feature selection methods and Principal Component Analysis (PCA) for efficient data processing. Built a scalable pipeline for processing large volumes of transactions in real-time. Designed and implemented a web-based dashboard for monitoring and visualization of fraud detection alerts.

CERTIFICATIONS

Microsoft Certified: Azure Data Engineer Associate (DP-203)
Google Certification: Proven Python Skills
IBM Certification: Completed Artificial Intelligence Course
Contact this candidate