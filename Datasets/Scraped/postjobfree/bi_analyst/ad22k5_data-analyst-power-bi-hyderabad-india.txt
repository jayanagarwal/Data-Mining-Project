Data Analyst
Name: Swathy Hari Prasad
E-Mail ID: ad22k5@r.postjobfree.com / Phone No: +1-254-***-****
Location: Denver, Colorado

Professional Summary:

Extensive experience as a business data analyst in various sectors, including e-commerce, banking, healthcare, and fund companies, with professional knowledge in database and data analysis tools.
Proficiency in using a variety of data analysis tools and programming languages, including SQL, Python, R, Tableau, MATLAB, Excel, AWS, Google Cloud, and Power BI.
Worked with both relational databases (MySQL, PostgreSQL) and non-relational databases (MongoDB, DynamoDB), utilizing AWS services for efficient data management.
Created tables and views, wrote complex SQL queries, stored procedures, and functions in MySQL and PostgreSQL using Python.
Utilized Python for ETL (Extract, Transform, Load) and data manipulation processes in MySQL databases, ensuring efficient data movement and transformation in AWS environments.
Hands-on experience in creating complex NoSQL databases in MongoDB and connecting PyMongo for ETL processes using Python.
Implemented data warehousing solutions, designing and maintaining structured data repositories for optimal performance and scalability on AWS.
Conducted data modeling to define the structure and relationships of data, ensuring effective organization and retrieval in databases within AWS.
Extensive experience in data cleaning, data mining, and data wrangling with unorganized, inconsistent, and incorrect data using NumPy and Pandas in Python.
Applied machine learning models, explored hyperparameters, and evaluated ML models using Python libraries such as Scikit-Learn and Statsmodels on AWS infrastructure.
Built statistical and machine learning models in Python using Statsmodels and Scikit-Learn, incorporating AWS services for scalability.
Implemented machine learning models such as logistic regression, random forest, cross-validation, and k-nearest neighbor in Python on AWS.
Created data visualizations in Python using Matplotlib, Seaborn, Plotly, Power BI, and Tableau, often integrating AWS data sources.
Developed automated reports and dashboards in Tableau and Power BI, providing stakeholders with real-time insights and analytics, leveraging AWS for data storage and retrieval.
Utilized advanced Business Analysis techniques to extract actionable insights from large datasets, contributing to strategic decision-making on AWS.
Expertise in leveraging visualization tools such as Tableau and Power BI to create compelling charts, graphs, and dashboards for effective data communication on AWS.
Hands-on experience using PySpark in Python for big data analysis, integrating ETL processes seamlessly into big data workflows on AWS.
Conducted statistical analysis, including hypothesis tests, linear regression, logistic regression, and time series analysis in R on AWS.
Generated data visualizations in R with ggplot2 and produced reports using R Markdown, often connecting to AWS data sources.
Proficient in connecting various local and live data sources, such as Excel, JSON, MySQL databases, and more, in Tableau and Power BI using Python and AWS.
Used Tableau and Power BI to create various charts, maps, reports, dashboards, and stories, often leveraging Python for data manipulation and analysis on AWS.
Applied machine learning models like Lasso Model, SVM, and SVD in MATLAB, incorporating ETL processes into the overall machine learning workflow, often using AWS for data storage and processing.
Developed and maintained automated reports and dashboards in Excel, enhancing data visualization and accessibility, with consideration for AWS integration.
Utilized workflow tools like Jira for viewing, managing, and reporting on work.
Strong written and verbal communication skills.
Experience of cooperating with multi-groups to communicate and negotiate for projects.
Combine patience, determination, and persistence to troubleshoot client issues and strong problem-solving and analytical skills.
Technical Skills:
Programming languages: Tableau, SQL, Python, R, MATLAB, Excel, Hive, Spark
Databases: MySQL, PostgreSQL, MongoDB, DynamoDB
Big Data: Hadoop, Hive, Pig, Spark
Reporting Tools: Tableau, Power bi, PowerPoint, Google Slides
Data Modeling: MS Visio, ER Studio, ERWIN
Packages: NumPy, Pandas, Seaborn, Matplotlib, Sklearn
Analysis Methods: A/B testing, Multiple Linear Regression, Logistic Regression, time series analysis, Knearest neighbor clustering, cross validation, and hypothesis testing
Operating Systems: UNIX, Linux, Windows

Education:
Bachelor’s degree: (Applied Electronics and Instrumentation Engineering) LBS Institute of Technology, Kerala, India, 2012
Masters: (Computer Science) Colorado State University, Fort Collins, USA, 2021

Professional Experience:
Client: Fifth Third Bank, OH July, 2022 – Till Now
Role: Data Analyst
Responsibilities:
Transferred various warehouse product data and transportation data from different databases, such as MySQL and NoSQL (MongoDB), into Hadoop using Sqoop and Python scripts on AWS to improve big data store availability.
Reduced ETL (Extract, Transform, Load) process time by deploying Hive and Spark into the Hadoop framework based on AWS EC2, improving data processing efficiency. Utilized PySpark for loading and processing large-scale data in Python on AWS.
Met with company executives to make recommendations based on careful research and predictions during a monthly Sales and Operations Planning (S&OP) planning process, leveraging AWS analytics services.
Collaborated with colleagues from various departments, including operations, sales, design, production, marketing, customer service, and project management teams, to address consumer inquiries, analyze customer needs, and boost sales using AWS data services.
Communicated with vendors to address problems, negotiate better deals, and build relationships, with AWS providing a platform for secure and scalable interactions.
Implemented data warehousing solutions on AWS, designing and maintaining structured data repositories for optimal performance and scalability.
Conducted data modeling to define the structure and relationships of data, ensuring effective organization and retrieval in databases on AWS.
Performed comprehensive data analysis using complex SQL queries with nested subqueries, aggregation functions, and window functions, complemented by Python scripts. This approach enhanced running efficiency on large datasets, reducing duplicated and unknown data in warehouse product and transportation data on AWS.
Utilized Python and SQL queries to extract recent product data from MySQL databases on AWS, conducting data munging and cleaning (e.g., merging tables, renaming variables, changing data types, and creating new features) with NumPy, Pandas, and Datetime.
Conducted Exploratory Data Analysis (EDA) in Python with visualization packages like Seaborn, Matplotlib, and Plotly to explore datasets, draw plots, identify possible trends, and reveal critical relationships between demand and supply on AWS.
Created various metrics in Python on AWS to define the effectiveness of reactions in supply chain issues and connected a live MySQL database to Tableau on AWS for further analysis.
Generated automated reports with interactive dashboards and various charts in Tableau using SQL queries and Python scripts on AWS to illustrate the effectiveness of reactions in supply chain issues, demand fulfillment, and inventory optimization. Discussed findings with team managers to optimize the supply chain and inventory on AWS.
Applied historical demand data to predict the future demand curve using machine learning models such as linear regression, random forest, and classification trees in Python with Statsmodels and Scikit-Learn on AWS, aiming to improve the supply chain and inventory optimization.
Conducted A/B tests in R on AWS, leveraging SQL queries and Python scripts to evaluate the demand fulfillment and supply chain efficiency for different transportation methods.
Utilized RMarkdown in R on AWS to generate statistical reports about the distribution of demand and supply data, including normal, gamma, and exponential distributions.
Designed automated dashboards for five features in Tableau using SQL queries and Python scripts on AWS to present findings and results to team managers, incorporating their suggestions to modify prediction models in Python.
Provided troubleshooting, analysis, and solutions for warehouse and supply chain data-related issues using Python and SQL for data retrieval and analysis on AWS.
Contributed to identifying supply chain and business problems causing concern and suggested improvements to cut costs and enhance the entire process using Python and SQL for data analysis on AWS.
Trained new employees on effective data entry techniques using Python and SQL for database interactions on AWS.
Environment: MySQL, Excel, NumPy, Pandas, Matplotlib, Seaborn, Spark, Hive, Hadoop, Sqoop, PySpark, EDA, Ploty, R, Power bi, Tableau, SQL, python, ETL
Client: AbbVie HealthCare, Chicago July, 2021– June,2022
Role: Data Analyst
Responsibilities:
Operated structured and unstructured data from various sources such as MySQL, NoSQL, Excel files, and transferred them into MS SQL Server using the ETL tool SSIS for efficient data integration.
Enhanced data processing efficiency by simplifying complex SQL queries with advanced functions such as window functions, aggregation functions, and nested subqueries.
Utilized AWS services such as Amazon RDS (Relational Database Service) for MySQL, NoSQL databases, and S3 (Simple Storage Service) for efficient data storage and integration.
Used SQL, Tableau, Power BI, and Python with AWS Glue for ETL processes, data warehousing, and data modeling to visualize and analyze data, creating compelling dashboards and reports to extract meaningful insights.
Leveraged AWS services like SageMaker for machine learning tasks, analyzing statistical distributions, and seeking patterns in customer rating data.
Applied ggplot2, Esquisse in R, and visualization libraries in Python on AWS for ETL processes, data warehousing, and data modeling to create visually appealing data visualizations, including Q-Q plots, density plots, and distribution plots, to identify trends, relationships, and behavior in customer rating data.
Conducted A/B tests in R using the abtest library and implemented statistical testing in Python and SQL on AWS for ETL processes, data warehousing, and data modeling to determine the top three features influencing customer ratings. Created new metrics to define customer ratings and tested them with A/B tests.
Implemented machine learning models like random forest, regression trees, and linear regression in Python with AWS Sagemaker integration for ETL processes, data warehousing, and data modeling to predict customer ratings. Employed the Recursive Feature Elimination function in Python on AWS to optimize results and improve accuracy.
Utilized the smartEDA library in R and Python on AWS for ETL processes, data warehousing, and data modeling to perform Exploratory Data Analysis (EDA) and integrated the outcomes into R-markdown reports and Jupyter Notebooks.
Combined MS SQL and AWS services like Amazon Redshift, Athena, or Aurora with Tableau/Power BI for ETL processes, data warehousing, and data modeling to create and modify reports, dashboards, and visualizations. Utilized DAX (Data Analysis Expressions) in Power BI for diverse analytics such as trend analysis, forecasting, and distribution bands.
Generated clear PowerPoint presentations with data visualization results from Tableau/Power BI, Python, R, and SQL on AWS for ETL processes, data warehousing, and data modeling, presenting them to the customer service team.
Collaborated with Agile and Waterfall teams on AWS for ETL processes, data warehousing, and data modeling to tailor each project component within short timeframes.
Designed and developed data analytics dashboards for customer retention, acquisition, satisfaction, sales revenue, and volume growth in Tableau/Power BI with AWS integration for ETL processes, data warehousing, and data modeling to inform financial targets, business development, and sales incentive programs.
Collaborated with the programming team on AWS for ETL processes, data warehousing, and data modeling to enhance the internal system for optimal efficiency.
Communicated with vendors on AWS for ETL processes, data warehousing, and data modeling to obtain accurate product information, learn about upcoming products, and understand pricing policies.
Partnered with User Experience Teams on AWS for ETL processes, data warehousing, and data modeling to develop enhanced customer experiences and conceptualized best-in-class digital experiences to document the connected relationship between touchpoints and the full end-user experience.
Developed and managed End-User Testing Strategies on AWS for ETL processes, data warehousing, and data modeling to identify failures throughout the user experience. Utilized collected data on AWS for ETL processes, data warehousing, and data modeling to make assumptions and recommendations for reporting results.
Automated report generation, dashboard creation, and analytics processes on AWS to streamline data-driven insights and decision-making.
Applied advanced Data Analysis techniques on AWS to uncover patterns, trends, and insights in large datasets, enhancing overall business intelligence and decision-making processes.
Environment: MySQL, Excel, SQL, Python, NumPy, Pandas, R, SSIS, ETL, Tableau, Power bi, UX, ETL
Client: Nordstrom, Seattle, WA Oct,2019-July,2021
Role: Data Analyst
Responsibilities:
Operated ETL processes to manage customers' credit card transaction data from various sources, including Excel files and JSON files, and transferred them into a MySQL database for improved storage and processing performance using SSIS.
Wrote complex SQL queries, including window functions, aggregation functions, and nested subqueries, to extract and analyze the credit card transaction data.
Loaded the current month's credit card transaction data into Python for additional data cleaning and data wrangling tasks, such as merging tables, filling null values, and checking for outliers using libraries like NumPy and Pandas.
Applied Exploratory Data Analysis (EDA) and created data visualizations using Python libraries like Seaborn and Matplotlib to identify important features that influence possible fraudulent payments.
Conducted statistical analysis for credit card transaction data, including T-tests, ANOVA, and F-tests, using Python (Statsmodels library) and integrated the outcomes into the analysis in Python.
Built interactive dashboards in Tableau using Python integration for dynamic content, incorporating features such as slicers, filters, and custom visuals to present insights to managers.
Generated reports in MS Word with graphs, tables, and findings from Python, Tableau, and Excel.
Created interactive PowerPoint slides based on fraud detection reports for presentation to other product teams, integrating Python visuals for dynamic content.
Drafted and prepared bilingual (English and Chinese) marketing materials, including pitch decks, teasers, and investor presentations, incorporating Python insights for enhanced data-driven narratives.
Provided execution support by researching public filings, market-industry data, and databases to compile company profiles. Prepared pitch books, memorandums, market updates, Public Information Books, and PowerPoint presentations for senior bankers and clients, integrating Python and SQL visuals for comprehensive financial analysis.
Updated these points with Data Analysis, emphasizing the integration of various data analysis tools and techniques in the overall process.
Environment: MySQL, Excel, JSON, SQL, Python, NumPy, Pandas, Matplotlib, Seaborn, Tableau, Power BI, ETL

Client: Infosys, India Oct,2012 – Dec,2016
Role: Data Analyst/ ETL Developer
Responsibilities:
Designed, developed, and maintained ETL (Extract, Transform, Load) and ELT (Extract, Load, Transform) processes in Teradata, Oracle, and Java, leveraging Unix shell scripts for automation of batch processes.
Executed data analyst roles in the retail domain, collaborating with US clients such as Nordstrom and Staples, focusing on customer and sales analytics.
Collaborated with the data warehouse team, providing Customer Data in clean, customized formats as input for end-user sentiment analysis.
Demonstrated expertise in creating data quality and data monitoring solutions, alongside SQL Query Tuning and Performance optimization across various databases.
Played a key role in assisting decision support for advanced analytics teams by providing Data and Trend Analysis, highlighting deviations and changes in customer purchase behavior across different channels.
Designed and created a Monthly Dashboard on Customer Data for business analysis using Tableau, generating weekly and monthly reports for system audit.
Contributed to the database migration initiative from On-Premise Teradata systems to AWS Redshift.
Developed and implemented new database objects, including tables, views, triggers, stored procedures, user-defined functions, complex SQL queries, and clustered/non-clustered indexes to implement business logic.
Engaged in Implementation Support, analyzing and implementing hotfix/break fix solutions for issues reported from business for various backend systems.
Successfully managed large-scale Data Warehouse and decision support systems, also led and mentored a team of 8 techies.
Applied data analyst roles and responsibilities by providing actionable insights through data analysis, contributing to strategic decision-making processes, and ensuring data quality and integrity throughout the data lifecycle.
Environment: SQL, Teradata, PL/SQL, Python, Data Warehousing, Big Data, Hadoop, Apache Spark, Apache Airflow, Tableau, Machine Learning, Java, Junit5, Unix, Teradata Utilities (SQL, B-TEQ, Fastload, MultiLoad, FastExport), Control-M, Agile Methodology, Data, Visualization and Modeling, ETL and ELT.
Contact this candidate