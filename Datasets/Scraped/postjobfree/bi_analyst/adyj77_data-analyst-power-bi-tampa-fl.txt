Abhishanth Penta
***** ******* **** **, *** *** 813-***-**** adyj77@r.postjobfree.com Abhi P LinkedIn

SUMMARY
Over 5 years of experience in the Insurance, Annuity, and Healthcare domains, specializing in ETL processes and experience in the design, development, and implementation of data pipelines.
Experience in using Hadoop ecosystem tools like MapReduce, HDFS, Hive, Kafka, and Spark.
Working knowledge of AWS services (RDS, S3, Redshift, DynamoDB, Glue Catalog, EC2 and S3) and
Experienced in working with structured, semi-structured, and unstructured data in various formats (text, zip, XML, and JSON).
Developed a data pipeline using Kafka and Spark Streaming to store data in HDFS and performed real-time analytics on the incoming data.
Demonstrated ability to extract, transform, and load data into data warehouses or data marts.
Working knowledge of HDFS Filesystem and Hadoop Daemons such as Resource Manager, Node Manager, Name Node, Data Node, Secondary Name Node, etc.
Working knowledge of Mainframe including TSO, ISPF, COBOL, JCL, VSAM, and DB2.
Well-versed in the complete Software Development Life Cycle (SDLC), encompassing requirements definition, design, programming/testing, and implementation of major systems.
Knowledge of SQL database design, maintenance, performance tuning, and development of SQL queries, scripts, and stored procedures.
Experienced in Agile Scrum and Waterfall methodologies, ensuring efficient project management.
Effective communication and collaboration abilities across cross-functional teams and non-technical stakeholders.
EDUCATION
Master of Science in Business Analytics and Information Systems Jan 2022 – May 2023
University of South Florida, Tampa, FL GPA: 3.54
Bachelor of Technology in Electronics and Communication Engineering Aug 2012 – May 2016 SRM University, Chennai, India GPA: 3.1
TECHNICAL SKILLS
Tech stack – Python, R, SQL, MapReduce, HDFS, Hive, Kafka, Spark, Java, SQL, COBOL, JCL, DB2, MS Access, Data Mining techniques
Tools – GIT, JIRA, R Studio, Azure ML, Weka, Tableau, MS Excel, CI/CD Pipelines
Cloud & Reporting: AWS, GCP, Tableau, Power BI, R
Process –Agile Scrum, Waterfall Model
Domains –Insurance and Annuities, Health Care, Process Improvement.

Professional Experience
Client: Elevance Health Inc. (Anthem Inc.) Dec 2019 to Dec 2021 Legato Health Technologies (Carelon Global Solution): Data Engineer Hyderabad, India
Responsibilities:

Developed ETL data pipelines using Spark, Spark SQL, and Oozie.
Used Spark for interactive queries, processing of streaming data, and integrated with popular NoSQL databases.
Experience with AWS Cloud, Data Pipeline, EMR, S3, EC2.
Supporting Continuous storage in AWS using Elastic Block Storage, S3, and Glacier. Created Volumes and configured Snapshots for EC2 instances.
Experience in Developing Spark applications like Spark-SQL for data extraction, transformation, and aggregation from multiple file formats for analyzing & transforming the data to uncover insights into customer usage patterns.
Developed the batch scripts to fetch the data from AWS S3 storage and do require transformations.
Created Oozie workflow engine to run multiple Spark jobs.
Replaced the existing Map Reduce programs and Hive Queries into the Spark application.
Developed Kafka consumer API in Scala for consuming data from Kafka topics.

Client: Voya Financial Nov 2017 to Nov 2019 Cognizant Technology Solutions: Data Analyst Hyderabad, India
Responsibilities:

Extensively involved in the Installation and configuration of Hadoop Distribution.
Implemented advanced procedures like text analytics and processing using in-memory computing capabilities like Apache Spark written in Scala.
Developed spark applications for performing large-scale transformations and denormalization of relational datasets.
Evaluated data import-export capabilities, and data analysis performance of the Apache Hadoop framework.
Supported big data platforms and contributed to the modernization and data center migration process.
Used Spark Streaming API for performing transformations and actions on the fly for building a common learner data model which gets data from Kafka in real-time.
Developed and implemented complex technical designs, including data mappings, for data warehousing and batch processing environments.
Applied strong analytical skills in analyzing data, complex processes, and systems, contributing to effective decision-making.
Utilized Unix shell scripting to automate tasks and streamline processes.

Client: Lincoln Financial Group July 2016 to Oct 2017 Cognizant Technology Solutions: Data Analyst Hyderabad, India
Responsibilities:

Developed and maintained SQL scripts, queries, and stored procedures to extract and manipulate data from databases.
Created complex SQL queries to generate reports and perform data analysis tasks.
Utilized SQL functions and joins to retrieve and merge data from multiple database tables.
Optimized SQL queries for performance by analyzing query plans and indexing strategies.
Developed ETL (Extract, Transform, Load) processes using SQL to integrate data from various sources into a centralized data warehouse.
Collaborated with cross-functional teams to gather business requirements and translate them into SQL queries and data visualizations.
Created interactive and visually appealing dashboards in Tableau and Power BI to present data insights to stakeholders.
Designed and implemented Tableau visualizations, including bar charts, line charts, scatter plots, and geographical maps, to visualize trends and patterns in data.
Developed Python code to gather the data from the SQL server and designs the solution to implement using Python.
Contact this candidate