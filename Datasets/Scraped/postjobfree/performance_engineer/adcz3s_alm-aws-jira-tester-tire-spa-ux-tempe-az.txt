OBJECTIVE
My goal is to become associated with a company in order to develop my skills and achieve the goals of the organization to increase revenue growth.
SUMMARY
Over 8 years of diversified experience in IT. Extensive experience in gathering requirements, planning performance strategy, setting up the environment, designing the scripts using HP Vugen, performance testing using HP LoadRunner, load test analysis using HP analysis, designing the regression test scripts for automation testing using HP UFT, application monitoring and performance engineering using APM, Dynatrace and CloudWatch tools. I also have an experience in project management and handling multiple projects at the same time.

EDUCATION
Master’s in Information Technology & Management (ITM) May 2013
University of Texas at Dallas, Texas, USA
Bachelor’s in Computer Science Engineering (CSE) June 2011
Jawaharlal Nehru Technological University, Hyderabad, India

AREAS OF EXPERTISE

HP LoadRunner 9.5/11.0/11.5/12.0/12.02/12.50/12.55/12.63
Performance, Load and Stress Testing
Performance Center 9.5/11.0/11.5/12.0/12.02/12.50/12.55/12.63
Baseline, Benchmark, Soak & Network testing.
Performance Analysis of Systems.
Quick Test Professional and Quality center.
Creating and maintaining test environments
Test Strategies
Test Plans
Test Scripts
Executing and Validating Tests
Managing Multiple Projects
Test Coordination
AWS CloudWatch
Testing of GUI and Web based applications
Functionality and GUI Testing
Integration and System Testing
Backend Testing using SQL queries
UNIX Environment
Mainframe Environment
Test and Work Breakdown Structure Matrix
Monitoring Test Technicians and QC Analysts
Lotus Notes, and Test Director
Microsoft Excel.
Knowledge of Oracle, SQL.
HP ALM
HP Unified Functional Testing
Dynatrace

TECHNICAL SKILLS
Operating Systems
AIX, HP-UX, Solaris, UNIX, Windows XP,2003,2000, Vista, Windows 7, Windows8 and Linux
Languages
C, C++, JavaScript, VB Script, XML.
Databases
Oracle, DB2, SQL Server, MS-ACCESS, MySQL
Web Related
DHTML, XML, HTML
Testing Tools
HP LoadRunner, Performance Center, WinRunner, Quick Test Pro, TOAD, HP UFT.
Web / Application Servers
Apache, Tomcat, WebLogic, WebSphere, IIS
Methodologies
RUP, Agile, Performance Testing and Automation Testing.
Project Management /Analysis
MS Project, MS Visio, SharePoint, ClearQuest, Rational Requisite Pro and UML
Other
SiteScope, Team Quest, CA Wily, HP Quality Center, APM and HP ALM, New Relic, AWS CloudWatch and Dynatrace.

PROFESSIONAL EXPERIENCE

Role: Sr Performance Engineer
Client: Discount Tire Corporation
Location: Tempe, AZ
Duration: Aug 2019 – Present
Responsibilities:
Joined website project team to support day to day performance testing activities at Discount Tire corporation.
Supported performance testing for My Account, suggested selling, Check fit, Staggered experience projects.
Discount Tire web application is currently on Hybris SAP architecture.
Involved in the new project which is to upgrade the current website to single page application (SPA) using Graph QL.
Responsible for designing the scripts in Web HTTP/HTML to test response times for the API calls on discount tire website application.
Responsible for designing the scripts in Ajax TruClient to measure the browser-based UI response times on the website application.
Developed Load Test Plan and Load Test scenario documents.
Participated in QA demos to understand the enhancements or defect that are going to implemented.
Responsible for coordinating with offshore team to get the project work completed.
Identified performance related bugs and created tickets in JIRA.
Supported two builds every week. As there are two build (enhancements and production support) every week, regression performance testing must be done.
As part of every new build, responsible for executing baseline, peak load tests.
Responsible for preparing the load test reports for each of the load tests and then compile the results to one pager report.
Participated and supported the team for upgrading the tool HP ALM/PC to the newest version. Responsible for upgrading software on all the existing remote machines as well the load generators.
Involved in engaging the application support team and infrastructure team to participate in the debugging sessions.
Experience using AWS CloudWatch to monitor all the server related metrics including CPU utilization, thread connections etc.
Experience using Dynatrace to monitor the requests generated from web api and ajax calls using the pure paths.
Responsible for automating the load tests using performance center plugin in Jenkins automation tool.
Designed best practice documents for tool upgrade, version controlling, scripting functions.
Responsible for implementing Version controlling feature in HP ALM/PC.
Currently supporting the new APOLLO project which is the website upgrade project.
Did some POC to identify the best feasible tool for automation and performance testing. Ready API from Smart bear has been identified.

Role: Sr Performance Engineer
Client: Follett Corporation
Location: Westchester, IL
Duration: Jan 2019 – Aug 2019
Responsibilities:
Joined the performance testing team to support the v9 performance testing activities for the Follett transformation project.
Responsible for designing the scripts in Web HTTP/HTML to test response times for the API calls on Follet application.
Responsible for designing the scripts in Ajax TruClient to measure the browser-based response times on the Follet application.
Developed Load Test Plan and Load Test scenario documents.
Participated in all phases of testing, Initial Reviews, Estimations, Test Planning, Design, Execution and Analysis.
Responsible for coordinating with offshore team to get the assigned tasks completed.
Identified performance related bugs and created tickets in JIRA to track.
Experience in using Microsoft teams to communicate and store all the project related activities.
Responsible for creating load test strategy to calculate the thinktime and to determine number of virtual users given the throughput.
Responsible for creating new tests and assigning to a test set.
Responsible for creating baseline, breakpoint and debug scenarios.
Responsible for configuring the load generator properties.
Involved in engaging the application support team and infrastructure team to participate in the load testing.
Participated in war room sessions with the application architects, security team, network team, database team and infrastructure teams to fine tune the application.
Identified the performance bottlenecks by executing the debug load test.
Experience in using Dynatrace and New relic to monitor the server traffic and the performance on API requests.

Role: Performance Engineer
Client: Northern Trust Bank
Location: Chicago, IL
Duration: July 2017 – Dec 2018
Responsibilities:
Responsible for engaging in the load test service request process with application team.
Responsible for carrying out the entire load testing process.
Worked on creating estimations for the service request.
Experience in providing the level of effort (LOE) estimates, attending joint application design (JAD) sessions and participating in review of test cases
Developed Load Test Plan and Load Test scenario documents.
Participated in all phases of testing, Initial Reviews, Estimations, Test Planning, Design, Execution and Analysis.
Developed scripts using Ajax TruClient protocol.
As part of the performance testing life cycle, we determine on which protocol is compatible for the application that requires testing.
Followed Northern Trust’s standard procedure for load testing. Conducted Baseline, Duration and Spike Testing.
Experience in handling multiple projects at a time.
Responsible for handling a maximum of 5 applications at a given point of time.
Responsible for configuring the load generator properties.
Involved in engaging the application support team and infrastructure team to participate in the load testing.
All the service requests related to the load testing are created in Resource Management database.
Tracked all the defects in HP ALM.
Testing activities are recorded in SharePoint.
Used APM, Pandora and OEM for monitoring the web logic and application servers.

Role: Test Engineer
Client: Scope Infotech Inc.
Location: Columbia, MD
Duration: Apr 2017 – Jun 2017
Responsibilities:
Created detailed test plans based on analysis of requirements and design documents.
Developed Test Case Specification document.
Developed Test Cases for the planned release.
Involved in testing migration projects and the new applications.
Tracking defects and reported through the SharePoint and coordinated with developers to resolve the defects and close them.
Responsible for analyzing change requests (CRs) for each release, reviewing requirement artifacts and test cases.
Experience in providing the level of effort (LOE) estimates, attending joint application design (JAD) sessions and participating in review of test cases
Experience in testing of web-based applications and client-server applications.
Supporting development of test plans, and creating various testing documentation including Test Metrics, Test Summary reports and Release Readiness reports.
Involved in functional testing, system tests Integration and end to end scenarios of the application.
Performed functional, system, integration and regression testing, web testing.
Performed compatibility testing of application under various Internet Browsers.
Executed test cases to validate changes to the HEDIS PLD application, a system that stores and administers Patient level data for MA providers.
Performed data extract testing.

Role: Performance Tester
Client: DCCA
Location: Columbia, MD
Duration: Jun 2015 – Mar 2017
Responsibilities:
Responsible for developing the Vugen scripts as per the release requirement.
Developed Vugen scripts using various protocols, such as Web HTTP/HTML, AJAX TRUCLIENT.
Developed Ajax TruClient protocol Vugen scripts mainly for smoke testing.
Performance test scenario had mix and match of both web http/html and TruClient scripts.
Conducted release load test using 3000 Vusers to see if the results were matched with the benchmark results.
Conducted performance testing using 4000 Vusers to see if the application was stable.
Responsible for sending out the load test schedule out to various teams who are part of the application.
Used to collect the benchmark results before any maintenance activity.
Responsible for smoke testing the application after every code change.
Used to compare the response times using web http/html and TruClient protocol scripts.
Ajax TruClient protocol scripts took less scripting time than any other protocol scripts.
Depending upon the requirements from production, we used to design the thinktime calculations.
Created the load test scenarios in performance center.
Maintained separate folders for every load test in test lab as well as test plan.
Used to create new scenarios of every load test.
Responsible for requesting the support from all the support teams who are directly/indirectly associated the application.
Conducted the load test in Implementation region as this environment was like production path.
Worked on HP UFT to develop automation scripts.
Developed the scripts for Regression testing using HP UFT.
Created Automated scripts for Regression testing using descriptive programming.
Developed scripts by using XPATHS.
Used UFT to create data for performance testing.
Analyzed the load test results by using HP LoadRunner Analysis tool.
Developed smoke testing scripts using Jmeter.
Used JMeter scripts for the data setup.
Created Jmeter scripts for the data validation after the database refresh in the environments.
Responsible for extracting the results and shared it across different groups who were related to the application.
Defined and configured SLAs for hits/sec, throughput, transactions per second in HP LoadRunner.
Responsible for monitoring different graphs such as Throughput, Hits/Sec, Transaction Response time and Windows Resources while executing the scripts from HP LoadRunner.
Created detailed test results report which included the observations made during the test, recommendations for improvement and go- no go decisions, detailed response time graphs for the critical transactions and detailed server metrics graphs.
Proficient in creating detailed test results report for developer community as well as high level results report for the upper level management.
Identified and logged defects in QC for every release and followed up with them until they were closed or deferred to the next release cycle.

Role: Performance Tester
Client: QSSI
Location: Columbia, MD
Duration: Oct 2013 – May 2015
Responsibilities:
Worked with development team and support groups to understand the application architecture and to simulate realistic production scenarios for load and stress testing.
Responsible for developing the Vugen scripts for every release.
Developed Vugen scripts using multiple protocols such as Web Http/Html and Ajax TruClient.
All scripts used for smoke testing have been recorded using TruClient protocol.
Couple of Ajax TruClient protocol scripts are used in load test scenario.
Release performance test scenario had combination of web http/html and TruClient protocol scripts.
Enhanced the TruClient protocol scripts to work on various scenarios.
Customized the code for Ajax TruClient protocol scripts.
Conducted performance testing for 5000 concurrent Vusers to test whether the application was able to handle the load on various parts of the web application.
Responsible for monitoring the performance test by providing timely update of total number of Vusers and average transaction response time in the system.
Participated in live monitoring support, where people from different support groups would discuss about the application behavior in live.
Ajax TruClient protocol scripts usually take less scripting time.
Used the compare the scripts recorded with TruClient protocol to others.
Responsible for organizing a bridge call during load testing.
Involved in performance testing calls where people of all support groups would participate and provide their comments.
Developed the Vugen scripts prior to each load test depending upon the requirements.
Maintained and created new tests in performance center for every load test.
Simulated the performance test scenario using HP LoadRunner to make sure that all the settings are correct.
Depending upon the requirement from developers. Developed all the Vugen scripts with the thinktime and necessary calculations.
Responsible for assigning the correct number of load generators to the virtual Users (Used to follow this ratio 1 load generator for every 700 Vusers).
Analyzed the load test results by using HP LoadRunner Analysis tool.
Responsible for extracting the results and shared it across different groups who were related to the application.
Used to populate the average transaction response times into a baseline excel sheet where all the numbers from previous tests were stored.
Coordinated with different teams to know their findings from the load test.
Participated in weekly performance calls.
Involved in developing Regression scripts using HP UFT.
Developed scripts for Regression testing using HP QTP.
Worked on page checkpoints, web elements and web tables while developing the regression suite scripts which were part of application testing.
Logged defects into HP ALM. Tested the defects as it was deployed in code fix.
Performed functional testing after every code deployment.
Had a chance to test in testing environment as well as implementation environment.
Worked with BA testing team to suggest the test case scenario and develop a plan to write test cases.

Role: Big Data Tester
Client: Aatrix Software
Location: Grand Forks, ND
Duration: Jul 2013 – Oct 2013
Responsibilities:
Involved in setting up big data cluster which is to be tested for performance.
Identified and designed corresponding workloads.
Worked in special environment due to large data size and files.
Created job flows in AWS console to store the data using Elastic Map Reduce Technique and few other approaches.
Worked on cleaning the data making sure the data is not lost.

Role: Quality Analyst Intern
Client: CTRLS Data Center
Location: Hyderabad, India
Duration: Jan 2010 – Jul 2011
Responsibilities:
Ensured that all products met functional and design specifications.
Isolated replicated and reported any observed defects in parts and products.
Determined test requirements and developed customized test plans.
Reported any observed quality issues to management immediately.
Complied with all established testing guidelines and perimeters.
Logged all tests performed and noted results.
Inspected and reviewed design specifications to identify possible design flaws.
Evaluated user problems and issues with software performance.
Provided feedback to developer’s vendors and clients.
Documented quality issues with software products.
Contact this candidate