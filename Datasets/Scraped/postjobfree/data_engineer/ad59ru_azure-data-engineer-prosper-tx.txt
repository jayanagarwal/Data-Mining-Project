AMULYA
682-***-****
ad59ru@r.postjobfree.com
www.linkedin.com/in/amulya1003
SUMMARY
As an Azure Data Engineer at Concora Credit, I leverage my Microsoft certification and over 10 years of experience in software development, data engineering, and cloud computing to transform and analyze complex and multi-dimensional data. I have worked with different industries, such as finance, telecom, retail, and manufacturing, and developed data solutions that add value to the business and its customers. Technical Skills
Languages: Pandas, PySpark, Power BI
Data Management: Oracle, SQL Server, MySQL, Cosmos DB, MongoDB, SnowFlake Development Tools: IntelliJ, PyCharm, Azure Databricks Cloud: Azure Databricks, Azure Data Lake, Azure SQL Database, Azure Data Factory, Azure Synapse Analytics.
DevOps & CI/CD: Jenkins, Docker, Git
EXPERIENCE
Concora Credit Oregon, Portland
Azure Data Engineer Apr’2021-Present
Responsibilities:
● Accomplished a 30% increase in efficiency by designing and implementing modern data solutions on Azure.
● Developed PySpark applications to extract diverse data, enhancing accessibility and analysis accuracy.
● Achieved a 25% increase in data integrity by orchestrating ETL processes with Azure Data Factory and PySpark.
● Building data pipelines on Azure to load the data from ADLS to snowflake.
● Created pipelines in ADF using linked services/Datasets/Pipeline to extract, transform and load the data from different sources like ADLS, blob storage to snowflake.
● Enhanced data handling and analysis efficiency by implementing data ingestion and processing in Azure Databricks.
● Established ADF Pipelines for seamless data integration, improving overall data management.
● Improved data warehouse query performance by 40% through Azure Synapse Analytics implementation.
● Streamlined data processing and analysis tasks by automating Databricks notebooks using PySpark and SQL.
AMULYA
682-***-****
ad59ru@r.postjobfree.com
www.linkedin.com/in/amulya1003
Environment: Apache Spark, Python, Azure Data Lake, Azure Databricks, Azure Blob storage, Azure DataFactory, Azure Synapse.
AT&T Dallas, Texas
Azure Data Engineer May’2018 – Feb’2021
Responsibilities:
● Achieved a 30% increase in data processing speed in Spark using Python and Spark SQL API, enhancing overall efficiency.
● Managed ingestion, transformation, and computation of over 10TB of data in a Spark across diverse sources.
● Executed seamless data ingestion into Azure Data Lake and Azure Data Factory pipeline from 15+ sources, including web servers, RDBMS, and Data APIs.
● Implemented modern data solutions using Azure services, resulting in a 20% reduction in data processing time.
● Executed ETL processes for transferring 5TB of data from Azure Data Lake to SnowFlake, reducing processing time by 25%.
● Developed and deployed Spark applications handling over 100GB of data, revealing significant insights into customer usage patterns.
● Successfully managed Azure Data Lakes (ADLS) and integrated with 10+ other Azure Services, ensuring smooth data flow.
● Implemented medium to large-scale BI solutions on Azure, resulting in a 40% improvement in data analysis efficiency.
● Developed Python (PySpark) Scripts for custom UDFs, optimizing data manipulations and cleaning tasks, reducing processing errors by 15%.
● Created Pipelines in ADF for Extract, Transform, and Load operations, resulting in a 30% reduction in data transfer time.
● Designed end-to-end scalable architecture using Azure Components, leading to a 50% decrease in infrastructure costs.
● Supported clients by utilizing data to describe and model the outcomes of investment and business decisions, resulting in a 15% increase in ROI.
Environment: Apache Spark, Python, Azure Data Lake, Azure Databricks, Azure SQL Database, SnowFlake.
AMULYA
682-***-****
ad59ru@r.postjobfree.com
www.linkedin.com/in/amulya1003
Progressive Insurance Austin, TX
Software Engineer Oct’ 2016 – Apr’2018
Responsibilities:
● Collaborated with cross-functional teams to lead the successful migration of a large-scale Hadoop infrastructure to Microsoft Azure, involving over 500 TB of data.
● Conducted a comprehensive analysis of the existing Hadoop ecosystem, evaluating 100% of components to formulate an effective migration strategy.
● Configured Azure environment to mirror 100% of the existing Hadoop setup, including data storage and HDInsight clusters.
● Ensured seamless data migration with less than 2 hours of downtime and maintained 100% data integrity throughout the process.
● Implemented optimization measures, improving system performance on the Azure platform by 35%.
● Designed and implemented robust security protocols, achieving 100% compliance with industry standards for data security during migration and post-migration phases.
● Established comprehensive monitoring tools, tracking system health and performance with 99.9% uptime.
● Documented the entire migration process, creating guides for 100% knowledge transfer and future reference.
● Provided ongoing support to internal teams, resolving 95% of Azure Hadoop-related queries within 24 hours.
● Stayed updated on emerging technologies, driving a 20% improvement in project outcomes through continuous optimization.
● Actively participated in daily Scrums, contributing to the development of the application. Environment: Hadoop, Microsoft Azure (Azure Data Lake Storage, Azure HDInsight), Optimization, Security Protocols, Monitoring Tools, Documentation. Peerless Mfg, Dallas, TX
Software Engineer Jun’ 2014 – Sep’ 2016
Responsibilities:
● Significantly streamlined the data extraction, transformation, and loading (ETL) processes, cutting data processing time by 82% and enhancing overall system efficiency within three months of taking the role.
●Introduced data validation checks, which reduced errors by 56% and improved data reliability substantially in the first year.
●Expertly managed Hadoop clusters, maintaining 99.9% uptime and ensuring scalability to handle over 220 terabytes of data.
AMULYA
682-***-****
ad59ru@r.postjobfree.com
www.linkedin.com/in/amulya1003
● Optimized MapReduce jobs, resulting in a 77% increase in query performance for complex analytical tasks.
● Involved in AGILE/SCRUM-based development environment, like having daily stand-ups, storytime, sprint planning, and sprint review.
Environment: Hadoop ecosystem, MapReduce, Apache Spark, Agile development methodologies, Project management tools (Jira), Version control systems (Git.), Visual Studio Code. Satyam Computers, Hyd, IN
Software Developer Feb’ 2007– Apr’ 2009
Responsibilities:
● My role as a UI developer is to develop a new portfolio of applications with a completely new design that will be compatible with existing applications.
● Worked with the internal teams through several meetings and got the requirements for the project.
● Implemented custom authentication and role-based authorization in JSP to authenticate the users.
● Developed various interface screens using Web Forms, JSP, and HTML controls for Client-Server interaction.
● Designed and implemented the website using HTML, XHTML, XML, CSS, Flash, MySQL, and JavaScript.
● Designed, developed, and updated User Interface Web Forms using CSS, Dreamweaver, and JavaScript.
● Tested cross-browser compatibility on a range of projects.
● Creating rapid prototypes of interfaces to be used as blueprints for technical development.
● Worked closely with the Technical Development Manager to help track, prioritize, estimate, and recommend resources for all Technical Development Team projects.
● Have created a data fix script to fix the data issue in the production environment. Environment: HTML, CSS, JavaScript, DOM, MySQL, XML. EDUCATION: Bachelor of Technology from JNTU - Hyderabad, India
Contact this candidate