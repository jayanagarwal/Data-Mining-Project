DEEKSHITHA S DATA ENGINEER
+1-737-***-**** ad6y66@r.postjobfree.com Tx
PROFESSIONAL SUMMERY
* ***** ** ************ ********** in IT industry with expert-level skills in Big Data Hadoop Ecosystem, Apache Spark, PySpark, Scala, Python, Kafka, Data Warehousing, Data Pipeline, Business Intelligence, Snowflake, Data Analytics and Azure – Data Lakes, Amazon Web Services (AWS) – EC2, S3, EMR, ETL, Informatica, Google cloud Platform (GCP) Glue and Presto and Data bricks.
TECHNICAL SKILLS
Big Data Tools : Hadoop, Hive, Apache Spark, PySpark, HBase, Kafka, Pig, Map Reduce, Zookeeper
Cloud Technologies : Azure, AWS, Snowflake, GCP.
Containerization / Orchestration: Dockers, Kubernetes, Airflow.
ETL Tools : SSIS, DBT, Boomi, Informatica.
Relational Databases : MS SQL Server, MySQL, Oracle, PostgreSQL, Netezza.
No SQL Databases :Cassandra, MongoDB, HBase
Programming / Query Languages: Python, R, Scala, JSON, HTML, SQL, T-SQL, PL/SQL, DAX, LookML.
Scripting : Python, Power Shell scripting
Version Control :GIT, SVN
CI/CD : Azure, Azure Devops.
Machine Learning : Linear Regression, Logistic Regression, Decision Tree, SVM, KNN, K mean.
Packages : NumPy, Pandas, Matplotlib, Scikit-learn, Seaborn, PySpark.
Reporting Tools : Tableau, Power BI, Looker, SSRS.
Operating Systems : Windows, Linux, MacOS.
WORK EXPERIENCE
Fannie Mae, Plano, Texas Data Engineer Aug 2023 – Till Date
Responsibilities:
Develop standardized Azure Data Factory pipelines for ingesting diverse data sources into Azure Data Lake Storage (ADLS).
Established metadata framework within Azure Data Factory for improved data management.
Applied Scala functions for the data filter, Mining, aggregate, and join datasets, applying business logic and data transformation rules as needed.
Parameterize linked services, datasets in Azure Data Factory to enhance pipeline flexibility, reusability.
Utilize Databricks notebooks with PySpark to register and transform raw data into structured formats.
Write SparkSQL transformations in Databricks notebooks to facilitate data movement across different layers in ADLS and Databases.
leveraged Spark's Scala API to build distributed data processing workflows that can scale horizontally across clusters.
Implement automation workflows using Azure Logic Apps for efficient task management.
Contribute to SQL Server database development and optimization.
Perform performance tuning and query optimizations to enhance database efficiency.
Processed the HDFS framework within the Spark eco system for seamless data integration.
Experienced in writing real-time processing and core jobs using Kafka as a data pipeline system.
Migration of on-premises data (SQL Server / MongoDB) to Azure Data Lake Store (ADLS) using Azure Data Factory (ADF V1/V2).
Wrote many DAX functions for accurate and meaningful visualizations, insightful data reports.
worked on all power BI components such as Power Query, Pivot, View.
Environment: Azure (Data factory, Databricks, SQL data warehouse, Synapse, Data Lake), Power BI, Kafka, HDFS, MS SQL Server, SSIS, SSRS, Microsoft Visual Studio, SQL Server Management Studio, Azure Devops, Dockers, Airflow, SQL, T-SQL, DAX, Pyspark, Scala, Big Data, Agile methodology.

Change Healthcare, Lombard, IL Data Engineer Dec 2022 – July 2023
Responsibilities:
Leveraged AWS Glue for Data integration, data cataloging, schema discovery, and automated ETL job generation, reducing manual effort and improving efficiency.
Handled importing of data from various data sources, performed data control checks using PySpark and loaded data into HDFS.
Utilized AWS Lambda for data processing tasks like Data Validation, Enrichment, real-time event processing.
Developed python scripts, UDFs using both Data frames/SQL/Data sets and RDD in Spark for data Aggregation, queries and writing data back into OLTP system through Sqoop.
Involved in Real-time data processing, core jobs using Spark Streaming with Kafka as a data pipe-line system.
leveraged Scala libraries and frameworks to interact with databases (e.g., JDBC), message queues (e.g., Kafka), cloud storage (e.g., AWS S3, Google Cloud Storage), and other data sources or APIs.
Designed Data Marts by following Star Schema, Snowflake Schema Methodology, using Data Modeling tools.
wrote Scala code to connect to data sources, extract data, and load it into Spark's Resilient Distributed Dataset (RDD) or Data Frame for further processing.
Involved in performing unit, System, Regression, integration testing.
I have substantial work experience in software projects development life cycle, utilizing the core principles of Agile methodologies.
I have experience in closely collaborating with offshore development and production support teams.
Environment: AWS Storage accounts, AWS Glue, Snowflake, HDFS, Hive, Kafka, Spark 1.8, Linux, Python 2, SQL Server, SQL, Jira, Service Now, Scala, Agile methodologies, AWS (EC2, S3, EMR, Lambda, Step Function).

RYAN, Hyderabad, INDIA GCP Data Engineer Sep 2018 – July 2021
Responsibilities:
Developed multiple data pipelines using cloud services, worked on Map reduce for data distribution to reduce the data load.
Worked on Cloud Storage, Dataflow, Cloud Composer, Bigquery, Cloud Pub/sub and Dataproc.
Experience in testing the data through streaming jobs for Events and Outages.
Writing Python scripts to load the data from Bigquery to Bigquery using Dataflow and Composer.
Experience in data validation and analysis for Prod defects.
Running Cron Jobs using the Omega Data to GCP and checking the logs in Omega.
Experienced in GCP Jobs and Egress jobs testing once the migration is done.
Experience in writing and creating Hive tables in Omega and data validation.
Working experience with Support team and took the responsibility for the issues in production.
Experience in solving priority issues and involving in SOC calls while there is any production issues.
Handling all priority incidents created by the end users & providing the solution on time via Service Now.
Experience in creating Teradata scripts, to load the data from Teradata tables to Hadoop.
Responsible for L2 support for environment and application related issues.
Handled Change Requests and Service Requests.
Responsible for Google Production Support for environments and application related issues.
Environment: GCP, Cloud SQL, Big Query, Cloud DataProc, GCS, Cloud SQL, Cloud Composer, Hadoop, Hive, Map reduce, Teradata, SAS, Teradata, Python, SQL Server, Service Now.

Ceequence Technologies Hyderabad, India Jr. Data Engineer Aug 2014 – Sep 2018
Responsibilities:
Designed, Deployed, and Managed data solutions using Azure cloud services such as Azure Data Factory, Azure SQL Database, Azure Databricks, Azure Synapse Analytics (formerly SQL Data Warehouse), and Azure Cosmos DB.
Built and Maintained data pipelines and ETL processes using Azure Data Factory to orchestrate data movement and transformation across various sources and destinations.
worked with both SQL and NoSQL databases on Azure, such as Azure SQL Database, Azure Cosmos DB, and Azure Table Storage, ensuring optimal database design, performance tuning, and data management.
Performed data warehousing solutions using Azure Synapse Analytics (formerly SQL Data Warehouse), including schema design, data partitioning, indexing strategies, and optimizing query performance.
Fabricated data modeling techniques for OLAP (Online Analytical Processing) and OLTP (Online Transaction Processing) environments, ensuring efficient data retrieval and analysis capabilities for reporting and business intelligence purposes.
Implemented data encryption, access controls, and auditing to ensure data protection, security and regulatory compliance.
Wrote PL/SQL, SQL Queries for Data Analysis, Data Validation, Data Transformation.
Performed ETL process using Informatica for on premises data to Oracle Data base for Data process.
Monitored Azure data solutions for performance metrics, troubleshooting issues, and implementing optimization strategies to improve data processing efficiency and reduce costs.
Documented skills and experience collaborating with cross-functional teams, stakeholders, and business users to gather requirements, define data architecture, and deliver scalable data solutions on Azure.
Environment: Azure, Oracle, SQL, PL/SQL, Informatica, SSIS, Import and Export Data wizard, TFS, Power BI.
EDUCATION QUALIFICATIONS:
Master’s (Business Analytics), from Texas A&M University – Dec 2022.
Bachelor’s (Computer Science & engineering), from JNTU-Anantapur – 2014.
Contact this candidate