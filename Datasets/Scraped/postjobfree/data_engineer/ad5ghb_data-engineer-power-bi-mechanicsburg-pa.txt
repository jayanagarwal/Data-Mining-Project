MOUNIKA CH
SUMMARY
I’m a data engineer with *+ years of experience, Proven expertise in data using tools like Python and SQL. I have a strong background in data integrations, specializing in ETL processes. I've worked with various data formats and tools, including Python, Apache Spark, Informatica, and AWS Glue. My experience includes designing, implementing, and optimizing data pipelines to seamlessly integrate data from diverse sources SKILLS
Data Ecosystem: Hadoop, MapReduce, YARN, Hive, Sqoop, HBase, Kafka, Kubernetes, Spark, Airflow, Informatica, Snowflake, Data Bricks ETL Tools: Talend, Informatica, Apache NiFi, Terraform Cloud Platforms: AWS (S3, EC2, Cloud Formation, Cloud Watch, Data pipeline, Glue, Athena, DynamoDB, Kinesis), Azure, GCP Data Warehousing: Snowflake, Redshift, Big Query, Teradata Languages: Python, Scala, SQL, NO-SQL, java, PostgreSQL, R language Database Management Systems: Oracle, MySQL, MS SQL Server, PostgreSQL, MongoDB Monitoring and Logging: Splunk, elastic search
CI/CD: Jenkins, GitLab CI/CD
API TESTING: POSTMAN
Automated Testing: Selenium, Cucumber, TestNg
Frameworks: Spring, Apache
Reporting and ETL Tools: Tableau, Power BI, Rust
Collaboration and Communication: Ability to work closely with data scientists, analysts, and other stakeholders to translate business requirements into technical solutions. Strong documentation skills for pipeline design, data flow diagrams and design docs WORK EXPERIENCE
DATA ENGINEER 01/2022 to Current
WELLS FARGO
• Implemented end-to-end ETL framework using Apache Spark, reducing data processing time by 60% and enabling real-time analytics for global clients
• Optimized big data handling pipeline by leveraging 80% Apache Spark and components, integrating Sqoop for seamless data extraction, resulting in a 50% drop-in handling time and improved data accuracy
• Constructed 90% ETL pipelines for Snowflake ingestion and employed Spark Streaming and Apache Kafka for real-time data Environment: Mongo DB, Elastic Search, Mongo DB, Power BI, ETL, Kafka, RESTful API, java, Splunk, Apache Airflow, ETL, Snowflake, Python, Apache Spark, Oracle, GIT, CI/CD, Jenkins, SQL, Jira, Hadoop, Map reduce, Agile, Meta data, Tensor flow, Data bricks DATA ENGINEER 06/2019 to 12/2020
OPTUM
• Streamlined data architecture by implementing a cloud-based data warehouse on Amazon Redshift, resulting in a 40% increase in data accessibility and a 30% reduction in data retrieval time for cross-functional teams
• Engineered end-to-end data management operations on AWS, optimizing ingestion, cleansing, and transformation procedures, resulting in an 80% decline in data errors and enabling seamless scaling for a high-volume payment handling system
• Created Python scripts to extract data from REST APIs and load in AWS S3. Developed Python script that streamlines crucial data extraction from ~100k documents for downstream operations every day Environment: Agile, AWS, Cloud Watch, Cloud Trial, Cloud Formation, AWS Glue, Docker, ETL, snowflake, MapReduce, Sqoop, API, Jira, Zookeeper, Pentaho, GIT, Python, Hadoop, Map Reduce, HDFS, Hive, Apache, Python, DB2, Streaming, SQL, Amazon EC2, S3, Spark, Scala, AWS, Git, Kafka, RedShift, DynamoDB, PostgreSQL, RedShift, RDS, Snowflake, data bricks EDUCATION AND COURSEWORK
MASTERS 01/2021 to 05/2022
University of Missouri - Kansas City
Coursework- Cloud Computing, Principles of big data, blockchain, Design and analysis of algorithm, computer security BACHELORS 06/2016 to 03/2020
Sathyabama University
Coursework- C programming, Data Structures, Java programming, Machine Learning, Data mining and warehousing CERTIFICATIONS
AWS CERTIFIED DEVELOPER – ASSOCIATE (CERT LINK)
PYTHON CERTIFICATION -INTERSHALA
+1-913-***-**** ad5ghb@r.postjobfree.com GITHUB LINKEDIN
Contact this candidate