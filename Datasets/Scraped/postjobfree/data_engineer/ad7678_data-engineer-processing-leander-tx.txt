Sri Spandana Paleru
Email: ad7678@r.postjobfree.com
Phone: 704-***-****
LinkedIn: https://www.linkedin.com/in/sri-spandana-paleru/ Professional Summary:
Experienced Data Engineer with 7+ years of expertise in designing, implementing, and optimizing data solutions. Proﬁcient in Python, SQL, Snowﬂake, Azure, Databricks, CI/CD pipelines, AWS Glue, S3, Athena, Airﬂow, ETL, NoSQL databases, and DataStage. Adept at building scalable data pipelines, managing databases, and deploying data infrastructure in cloud environments. Strong problem-solving skills and a proven track record of improving data processing eXiciency.
Skills
• Programming Languages: Python, SQL, Shell Scripting, NoSQL (MongoDB)
• Cloud Platforms: AWS (S3, Glue, Athena), Snowﬂake, Azure, Databricks
• Data Engineering Tools: Apache Airﬂow, CI/CD pipelines
• Databases: Snowﬂake, SQL Server, Oracle, Teradata, MySQL,Couchbase, MongoDB, Cassandra, PL/SQL, RDBMS, Oracle, AWS, Microsoft SQL Server
• Version Control: GitHub, GitLab, Jenkins, BitBucket
• ETL Processes: AWS Glue, Python ETL scripts, DataStage
• Scheduling: Autosys, One Automation
• Monitoring & Logging: Splunk
Professional Experience Data Engineer Tabner Inc Client: TIAA Dec 2022 - Present
• Developed custom Python scripts to automate data cleaning and transformation tasks, reducing manual eXort by 50%.
• Created reusable Python modules for common ETL tasks, standardizing data processing across projects.
• Integrated PySpark with AWS Glue for scalable and serverless ETL processing.
• Designed and optimized Snowﬂake schemas for eXicient data storage and retrieval.
• Utilized Snowﬂake's built-in functions for advanced analytics and data transformations.
• Optimized data storage and retrieval using Amazon S3, reducing data access time by 25%.
• Managed and administered Snowﬂake data warehouse, enabling advanced analytics capabilities.
• Developed robust ETL pipelines using Python and SQL to streamline data integration from various sources into Teradata and Oracle databases.
• Conducted performance tuning and optimization for large-scale data warehouses in Teradata and Oracle environments.
• Utilized One Automation to automate complex data workﬂows, signiﬁcantly reducing manual intervention and improving eXiciency.
• Created interactive dashboards and reports using MicroStrategy to provide actionable insights for business stakeholders.
• Managed code repositories, performed code reviews, and collaborated with cross- functional teams on GitHub to ensure code quality and consistency.
• Deployed and maintained data infrastructure on AWS, utilizing services such as S3, Redshift, and Lambda for scalable data processing and storage solutions.
• Developed complex SQL queries for data extraction and analysis from SQL Server, Teradata, and Oracle databases.
• Implemented ETL processes using Azure, enhancing data integration and workﬂow eXiciency.
• Managed version control and code collaboration using GitLab, ensuring streamlined development processes.
• Automated data processing workﬂows using Autosys and One Automation, improving eXiciency by 30%.
• Utilized Splunk for monitoring and logging data pipeline activities, ensuring data integrity and system performance.
Data Engineer Tabner Inc Client: Charter Communications May 2018 - Dec 2022
• Established a primary data lake for storing raw data from diverse sources using AWS S3.
• Automated ETL processes through AWS Glue for data cataloging and transformation.
• Developed Airﬂow jobs to schedule PySpark scripts.
• Implemented cloud-based data warehousing solutions (Redshift & Snowﬂake) for complex queries and analysis.
• Engineered scalable data processing workﬂows using PySpark, ensuring eXicient transformations and aggregations.
• I developed, implemented & managed a robust data transformation processes using ETL frameworks, SQL, Python . This involved extracting data from various legacy systems(BHN, CHTR, TWC), transforming it to meet the required formats, and loading it into Snowﬂake. Utilized AWS Glue to automate and manage ETL workﬂows, ensuring eXicient data processing and integration.
• Utilized version control and collaborative development, ensuring high code quality and traceability.
• Analyzed large datasets to identify trends and insights using statistical analysis tools.
• Implemented data governance frameworks to ensure data quality and compliance with industry standards.
• Actively participated in Scrum and Agile development methodologies to deliver the project in iterative phases. Collaborated with cross-functional teams, including data engineers, analysts, and business stakeholders, to ensure the project met all requirements and delivered maximum value.
• Conducted regular sprint reviews, retrospectives, and planning sessions to maintain project momentum and address any emerging challenges promptly. Graduate Research Assistant South Dakota State University Aug 2016 - Mar 2018
• Conﬁgured reports from diXerent data sources using ﬂat ﬁles, CSV, Excel, MySQL Server, Oracle database.
• Created custom calculations in Tableau including string manipulation, basic arithmetic calculations, and custom aggregations.
• Processed large datasets for data association and provided insights into meaningful trends.
• Developed statistical models for predicting products for commercialization using Machine Learning algorithms.
Data Analyst ATOM SOLUTIONS Jan 2015 - Jun 2016
• Created a web application to narrow job search for job seekers.
• Scheduled jobs to automate database activities such as backups and monitoring disk space.
• Applied text mining techniques to infer information from unstructured data.
• Used clustering algorithms to categorize customers into groups.
• Performed exploratory data analysis and feature engineering to ﬁt regression models.
• Implemented forecasting models to predict car sales for future seasons. Education:
Master’s in Data Science South Dakota State University Aug 2016 – Dec 2018 Bachelor’s in Computer Science GITAM University May 2012 – May 2016
Contact this candidate