SAEED SHAHMIRI
Sr. Big Data Engineer Data Analytics

Phone: 470-***-****, Email: ad7lwz@r.postjobfree.com

Professional Summary
I bring 13+ years of experience in IT to the table, with 12+ years focused on Big Data Engineering. I am skilled in designing, building, and managing data pipelines across all major cloud platforms (AWS, GCP, Azure) and on-premises environments. My expertise spans data ingestion, transformation, visualization, and leveraging machine learning for valuable insights.

AWS Expertise:
•Skilled in leveraging AWS services such as Amazon S3, EC2, RDS, Lambda, and DynamoDB to develop scalable and resilient cloud-based applications.
•Adept at deploying and managing containerized applications on AWS using Amazon ECS and EKS for container orchestration.
•Skilled in configuring AWS Kinesis Data Streams to collect and process large volumes of data from various sources, such as IoT devices, application logs, clickstreams, and social media feeds, in near real-time for analytics and decision-making.
•Proficient in designing and implementing serverless architectures on AWS using services like AWS Lambda, API Gateway, and DynamoDB.
•Experienced in setting up and managing AWS VPCs, subnets, security groups, and IAM policies for secure and isolated cloud environments.
•Strong background in implementing Continuous Integration/Continuous Deployment (CI/CD) pipelines using tools like Jenkins, GitLab CI/CD, and AWS CodePipeline.

Azure Expertise:
•Experienced in employing Azure services like Azure SQL Database and Azure Data Factory for data analytics and ETL processes.
•Expert in designing, building, and maintaining end-to-end data pipelines on Azure using Azure Data Factory, ensuring efficient and reliable data ingestion, transformation, and loading processes.
•Experienced in integrating diverse data sources and destinations, including Azure Blob Storage, Azure SQL Database, Azure Data Lake Storage, Azure Cosmos DB, and third-party systems within Azure Data Factory pipelines.
•Skilled in executing data transformation activities using Azure Data Factory Data Flows, including cleansing, aggregation, enrichment, and normalization to prepare data for analytics and reporting.
•Proficient in deploying and managing Azure Databricks clusters for big data analytics and machine learning workloads, including data exploration, feature engineering, model training, and inference within integrated data pipelines.
•Experienced in using Azure HDInsight to run Apache Hadoop, Spark, and HBase clusters on Azure infrastructure, enabling distributed processing of large-scale datasets and advanced analytics.
•Adept at integrating Azure Event Hubs with other Azure services like Azure Functions, Azure Stream Analytics, Azure Databricks, and Azure Data Lake Storage to build end-to-end data pipelines for real-time analytics and monitoring.

Google Cloud Platform (GCP) Expertise:
•Proficient in utilizing Google Cloud Platform (GCP) services including Cloud Functions, Cloud Storage, Data Prep, Data Flow, Big Table, Big Query, and Cloud SQL to build efficient data pipelines.
•Experienced in designing and architecting data solutions within the GCP ecosystem, ensuring high availability, reliability, and performance.
•Skilled in implementing data governance policies and security standards on GCP, ensuring compliance with industry regulations and best practices.
•Proficient in utilizing Google Cloud AI and Machine Learning services for predictive analytics and data-driven insights.
•Experienced in deploying and managing Kubernetes clusters on GCP for containerized applications and microservices architectures.
•Skilled in utilizing Google Cloud Pub/Sub for real-time messaging and event-driven architectures.
•Adept at optimizing cost and resource utilization on GCP using tools like Cost Explorer and Budgets to monitor and manage cloud spending effectively.

On-Premises and General Data Expertise:
•Experienced in managing on-premises data analytics and processing environments, including Hadoop, Spark, and relational databases.
•Proficient in manipulating, analysing, and visualizing large datasets using SQL, Python, R, and Tableau.
•In-depth knowledge of SQL and NoSQL databases, including incremental imports, partitioning, and bucketing concepts in Hive and Spark SQL.
•Skilled in extending HIVE core functionality using custom User Defined Functions (UDF), User Defined Table-Generating Functions (UDTF), and User Defined Aggregating Functions (UDAF).

Additional Skills:
•Proficient in setting up automated testing frameworks and conducting unit tests, integration tests, and end-to-end tests to ensure software quality and reliability.
•Experienced in gathering and analysing business requirements, translating them into technical specifications, and managing project timelines and deliverables.
•Strong background in Agile methodologies, including Scaled Agile Framework (SAFe) practices for project management.
•Excellent analytical, logical, communication, and problem-solving skills, with a proactive approach to self-learning and adapting to new technologies.

Technical Skills
Big Data
RDDs, UDFs, Data Frames, Datasets, Pipelines, Data Lakes, Data Warehouse, Data Analysis, NiFi

Hadoop
Hadoop, Cloudera (CDH), Hortonworks Data Platform (HDP), HDFS, Hive, Zookeeper, Sqoop, Oozie, Yarn, MapReduce

Spark
Apache Spark, Spark Streaming, Spark API, Spark SQL

Apache
Apache Kafka, Apache Maven, Apache Oozie, Apache Pig, Apache Sqoop, Apache Flume, Apache Hadoop, Apache HBase, Apache Cassandra, Apache Lucene, Apache SOLR, Apache Airflow, Apache Camel, Apache Mesos, Apache Tez, Apache Zookeeper

Programming
PySpark, Python, Scala, Java, SQL

Development

Query Language

Databases/ Data Warehouses

File Management

Cloud Platform

Security & Authentication

AWS Amazon Components

Azure Components

GCP Components

Virtualization

Data Visualization

Cluster Security

Query Processing

Agile, Scrum, Continuous Integration, Test-Driven Development (TDD), Unit Testing, Functional Testing, Git, GitHub, Jenkins CI (CI/CD for continuous integration)

SQL, Spark SQL, HiveQL

MongoDB, AWS Redshift, Amazon RDS, Apache HBase, Elasticsearch, Snowflake

HDFS, Parquet, Avro, Snappy, Gzip, Orc, JSON, XML

AWS Amazon Web Services, Microsoft Azure, GCP (Google Cloud Platform)

AWS IAM, Kerberos

AWS Lambda, AWS S3, AWS RDS, AWS EMR, AWS Redshift, AWS Kinesis, AWS ELK, AWS CloudFormation, AWS IAM

Azure Functions, Azure Blob Storage, Azure SQL Database, Azure HDInsight, Azure Synapse Analytics, Azure Event Hubs, Azure Monitor, Azure Resource Manager, Azure Databricks

Cloud Functions, Cloud Storage, Cloud SQL, Dataproc, BigQuery, Cloud Pub/Sub, Stackdriver, Deployment Manager, Identity and Access Management (IAM)

VMware, VirtualBox, OSI, Docker

Tableau, Kibana, Crystal Reports 2016, IBM Watson

Ranger, Kerberos

Spark SQL, HiveQL, Data Frames
Professional Experience

TD Bank, Cherry Hill, New Jersey September 2023 – Present
Sr. Big Data Engineer

•Established a robust big data infrastructure at TD Bank by implementing Azure Functions, Azure Blob Storage, and Azure SQL Database, leveraging cloud-based solutions for enhanced data management and processing.
•Administered Azure Synapse Analytics for efficient data storage, retrieval, and analysis, capitalizing on Azure's comprehensive data services.
•Utilized Azure HDInsight to manage and analyze extensive datasets, showcasing expertise in configuring and utilizing various Hadoop ecosystem components on Azure to meet diverse data processing requirements.
•Utilized Azure Monitor to proactively monitor system health and diagnose potential issues, employing Azure's advanced monitoring and logging services.
•Created AWS Lambda functions in Python for deployment management and developed external tables with partitions using Hive and AWS Redshift, enhancing cross-cloud data operations.
•Leveraged Azure Event Hubs for real-time data ingestion and processing, ensuring seamless integration and consistency within the Azure data ecosystem.
•Stored and transformed large datasets in Azure Blob Storage, maintaining data integrity and accessibility across Azure data storage solutions.
•Loaded data into Azure Blob Storage, facilitating efficient data ingestion and storage within Azure's data infrastructure.
•Managed and analyzed data using Azure SQL Database, ensuring high availability and reliability of Azure-based data storage solutions.
•Provided technical assistance and troubleshooting for data processing tasks on Azure HDInsight, ensuring smooth operation within the Azure environment.
•Exported analyzed data to relational databases using Azure SQL Database, enabling data-driven decision-making through visualization and report generation.
•Assisted in setting up Azure Resource Manager templates and updating configurations to implement data processing tasks, ensuring data accuracy and consistency across azure environments.
•Implemented Azure Data Factory pipelines to orchestrate ETL workflows and data processing tasks, ensuring seamless data integration and management on the Azure cloud platform.
•Utilized Azure Synapse Analytics for building and running enterprise-scale analytics and data warehousing solutions on Azure.
•Designed and implemented data pipelines in Azure Synapse Analytics to ingest, transform, and analyze large volumes of structured and unstructured data from diverse sources.
•Developed scripts in Scala on Azure Databricks to automate data transformation tasks, enhancing processing efficiency within Azure services.
•Configured Azure Event Hubs to receive and process real-time data using Azure Functions, enabling dynamic data analytics within the Azure environment.
•
Netflix, Los Gatos, CA Jan 2021 – Aug 2023
Sr. Big Data Engineer

•Utilized cloud services such as AWS EMR, AWS S3, Data Pipeline, and RDS (PostgreSQL) to design, implement, and manage scalable data processing workflows.
•Developed and implemented User-Defined Functions (UDFs) in Scala to automate business logic within applications deployed on AWS, streamlining processes and improving overall efficiency.
•Utilized Apache Spark distributed framework along with Scala, SparkSQL, and CI/CD tools to design, implement, and maintain data pipeline applications.
•Managed, built, deployed, and maintained continuous integration systems within a cloud computing environment using AWS services like CodeBuild and CodeDeploy, ensuring seamless development processes and rapid software delivery.
•Implemented Amazon Kinesis as a highly scalable messaging service for building event-driven architectures and real-time data processing pipelines on AWS, integrating with services like S3, Redshift, Lambda for efficient data ingestion, processing, and analysis.
•Integrate various data sources into the Big Data platform using Python-based frameworks.
•Leveraged Amazon Kinesis and CloudTrail logs for analysis, error identification, and troubleshooting performance issues within data processing workflows on AWS.
•Optimize Python scripts and data processing jobs for performance and scalability.
•Designed & implemented configurable data delivery pipelines for scheduled updates to customer-facing data stores built with Python.
•Continuously evaluate and improve NiFi workflows and configurations to enhance performance and scalability.
•Develop and deploy machine learning models and pipelines using Scala and related libraries.
•Built, tested, and maintained Spark Scala Applications, and deployed codes in different environments (DEV, QA, STAGING, and PROD) using CI/CD tools like IntelliJ, GitHub, Jenkins, and Airflow.
•Utilized AWS Glue to define data schema, crawl data sources, and transform data for loading into the data lakes.
•Leveraged AWS Lambda functions to automate data processing tasks within the data lakes pipeline.
•Conduct code reviews and refactor existing Python code to improve efficiency and maintainability.
•Leveraged Amazon EMR or Amazon Glue to orchestrate big data processing workflows, transforming & loading data into data lakes.
•Created & managed ETL data pipelines on AWS EMR (Elastic MapReduce) cluster using Apache Spark and Hadoop ecosystem tools.
•Keep up to date with the latest advancements in Big Data technologies and Scala ecosystem.
•Utilized AWS Lake Formation to streamline data lake creation and management for large-scale projects.
•Created test case frameworks for data validation and wrote complex SQL queries to conduct database testing, identifying and analyzing data discrepancies and data quality issues.
•Ensured data consistency and integrity by working on data validation and testing.
•Created Python scripts to monitor server load performance in production environments and horizontally scale servers by deploying new instances.
•Utilized Spark Scala, Spark SQL, Spark RDD, Spark DataFrame, HDFS, Hive, and JDBC to handle big data tasks such as ETL and data warehousing.
•Participated in daily stand-ups within a Scaled Agile Framework (SAFe) to ensure seamless collaboration and efficient project execution.
•Collaborated with teams on product grooming, user stories, and implementing services for efficient data management and analysis.

Ikea, Atlanta GA Nov 2018 – Dec 2020
Sr. Big Data Engineer

•Spearheaded the design and implementation of ETL processes on AWS Redshift, leveraging AWS Glue for efficient data extraction, transformation, and loading operations.
•Utilized AWS Step Functions to automate and refine data pipelines, enhancing efficiency and reliability.
•Created custom data connectors to integrate Hudi with various sources, including Apache Kafka and AWS S3, to support diverse data processing needs.
•Established comprehensive data quality checks and monitoring mechanisms within the AWS infrastructure, enabling real-time detection and resolution of anomalies.
•Enhanced ETL performance and scalability on AWS, adeptly managing large data volumes.
•Developed reusable Terraform modules for infrastructure components, promoting code maintainability and consistency.
•Designed and implemented IaC using Terraform for cloud deployments on the AWS platform.
•Develop and implement efficient Spark jobs for data ingestion, processing, and transformation using Scala, Python, or Java.
•Employed AWS Lambda to enhance ETL processes and streamline data workflows through serverless computing.
•Collaborated with data analysts and data scientists to design data lakes, facilitating data exploration and analysis.
•Design, develop, and maintain scalable and high-performance Big Data applications using Scala.
•Employed Amazon S3 for cost-effective storage of raw and processed data, ensuring accessibility and durability.
•Used AWS Glue Data Catalog to maintain metadata repositories, facilitating efficient data discovery and cataloging.
•Prepared automation scripts using Python & Scala for ingestion from various sources such as APIs, AWS S3, Teradata, and Snowflake.
•Integrated AWS CloudWatch for real-time monitoring and logging of ETL workflows, enabling proactive performance issue identification and mitigation.
•Perform code reviews and refactor existing Scala code to improve efficiency and maintainability.
•Monitor and troubleshoot Spark jobs to ensure high availability and reliability of data processing workflows.
•Applied AWS IAM policies & security best practices to ensure data confidentiality, integrity, availability within AWS environments.
•Utilized Python libraries like Boto3 and NumPy for AWS-related tasks.
•Integrate NiFi with other Big Data technologies like Hadoop, Spark, Kafka, and various cloud services for comprehensive data processing solutions.
•Used Python in AWS Glue to unnest and process JSON data from iOS and Android analytics SDKs, loading the data into AWS Redshift.
•Utilized AWS Redshift Spectrum for querying and analyzing data stored in Amazon S3 efficiently.
•Improved data lake schema and partitioning strategies for efficient querying using tools like Amazon Athena and Redshift Spectrum.
•Employed AWS Data Pipeline to automate the movement and transformation of data between different AWS services, reducing manual intervention and streamlining workflows.
•Design and build scalable ETL pipelines to handle large datasets and optimize performance.
•

Elevance Health Inc., Indianapolis, Indiana Oct 2016 – Oct 2018
Sr. Data Engineer

•Employed GCP Data Prep for data pre-processing and cleaning tasks, alongside tailored on-premises tools to meet specific data requirements.
•Leveraged GCP Data Flow for cloud-based data transformation and processing, while utilizing on-premises frameworks like Apache Spark for similar tasks.
•Utilized Google Cloud Platform (GCP) Cloud Storage for raw data storage, complemented by on-premises solutions such as Hadoop Distributed File System (HDFS).
•Utilized GCP BigQuery as a highly scalable data warehouse solution, while leveraging on-premises data warehousing solutions for specific business needs.
•Implemented custom business logic and triggered data processing events using GCP Cloud Functions, alongside on-premises event-driven architectures for similar functionalities.
•Loaded data into GCP Bigtable for scalable access, in conjunction with on-premises NoSQL databases such as MongoDB for storing and querying processed data.
•Configured GCP security and access controls to protect sensitive data and ensure compliance with data privacy regulations, while implementing similar measures in the on-premises environment.
•Utilized GCP Pub/Sub for real-time messaging and event-driven data processing, ensuring seamless integration between cloud and on-premises systems.
•Develop custom NiFi processors using Java or other supported languages to extend NiFi capabilities for specific use cases.
•Employed GCP AI Platform for building, deploying, and managing machine learning models to derive insights from large datasets.
•Used shell scripts to transfer data from MySQL to HDFS within the Hadoop environment.
•Implemented error handling and logging mechanisms within the on-premises data processing framework to ensure the reliability and accuracy of the data pipeline.
•Leveraged GCP Dataproc for running fast, easy, and cost-effective clusters for data processing, integrating with on-premises Apache Hadoop and Spark systems.
•Conducted data quality checks to ensure completeness and accuracy across GCP Cloud Storage, Bigtable, BigQuery, and on-premises storage and processing systems.
•Developed and managed data visualization dashboards using tools like Tableau within the on-premises environment, delivering actionable insights to business users and executives.
•Implemented data governance and security policies within the on-premises environment to maintain data quality, integrity, and confidentiality.
•Provided training on data resource management to staff members, ensuring effective utilization of on-premises data processing capabilities.

Chevrolet, Detroit, Michigan Dec 2014 – Sep 2016
Data Engineer

•Established secure data pipelines to extract data from diverse sources and integrated them into on-premises data processing frameworks like Apache Kafka. Implemented custom scripts for data pre-processing and cleansing to ensure data integrity before storage.
•Built and deployed high-throughput data pipelines using Apache Kafka to enable real-time data ingestion and processing.
•Utilized DevOps practices within the on-premises environment, including infrastructure as code, automated testing, and CI/CD pipelines to enhance agility and expedite time to market for data-driven solutions.
•Implemented robust security protocols for Kafka clusters, encompassing authentication, authorization, and data encryption to safeguard sensitive information.
•Defined project scope and delivered Big Data solutions that aligned perfectly with business objectives, all within the on-premises environment.
•Fine-tuned Apache Spark configurations and custom ETL processes to optimize data pipeline performance. Adjusted hardware resources as needed to ensure efficient data processing.
•Established robust on-premises security measures and access controls to safeguard sensitive data. Ensured compliance with data privacy regulations.
•Employed on-premises NoSQL databases such as MongoDB for scalable and performant storage and querying of processed data.
•Implemented comprehensive data quality checks to guarantee the completeness and accuracy of data loaded into on-premises storage solutions and NoSQL databases.
•Managed data movement and transformation across various on-premises services using custom-built, high-performance ETL pipelines and workflow management tools.
•Collaborated with diverse teams to identify Big Data requirements and designed tailored on-premises data processing systems to meet those needs.
•Collaborated with software R&D teams to seamlessly integrate on-premises data processing systems with existing company applications.
•Implemented disaster recovery and backup solutions on-premises to ensure business continuity and maintain data availability in the event of disruptions.

Gilead Sciences, Foster City, CA Jan 2012 – Nov 2014
Hadoop Administrator

•Installed, configured, and managed Hadoop clusters, including components like Hive, HBase, Zookeeper, Sqoop, and potentially others relevant to Gilead's data ecosystem.
•Ensured seamless Cloudera CDH or Hortonworks HDP deployments & upgrades across various environments, minimizing disruption.
•Managed Data Node commissioning, decommissioning, and recovery for optimal cluster health.
•Collaborated with Linux server admins to optimize server hardware and OS functionality for efficient Hadoop operations.
•Utilized shell scripting to automate tasks for data extraction and other processes.
•Developed & implemented data security policies, procedures, standards following industry best practices and HIPAA compliance.
•Continuously monitored & evaluated the effectiveness of data security controls through metrics, audits, performance assessments.
•Performed regular security assessments and penetration tests to identify and address vulnerabilities.
•Provide support for production NiFi environments, including monitoring, maintenance, and incident resolution.
•Integrated CDH clusters with Active Directory and strengthened authentication with Kerberos for enhanced security.
•Administered Sqoop for seamless data exchange between RDBMS, HDFS, and Hive, ensuring efficient data transfer.
•Orchestrated Hive Jobs using Oozie workflow engine to automate data processing tasks.
•Enhanced MapReduce job execution efficiency through targeted monitoring and optimization.
•Utilized Hive for dynamic table creation, data loading, and UDF development for flexible data analysis.
•Leveraged Spark for faster processing of specific datasets when needed.
•Managed & potentially worked with NoSQL databases like MongoDB, Cassandra, HBase depending on Gilead's data storage needs.
•Partnered with application teams for OS updates and Hadoop version upgrades.

Sumo Logic, Redwood City, CA Jan 2011 – Dec 2011
Data Analyst

•Utilized SQL queries and other analytical tools to clean, transform, and prepare data for analysis.
•Developed and monitored key performance indicators (KPIs) to track the success of various business initiatives.
•Documented analytical methodologies, data sources, and processes for future reference and reproducibility.
•Applied advanced analytical techniques such as machine learning, predictive modeling, and optimization algorithms.
•Lead and managed data analysis projects from inception to completion.
•Engaged with key stakeholders to understand their data needs and provide tailored analytical solutions.
•Ensured data accuracy, consistency, and integrity across different systems and sources.
•Implemented and adhered to data governance policies and best practices.
•Developed and refined statistical models to predict outcomes and identify potential risks
•Worked closely with various teams such as supply chain, quality, and business units
•Provided regular updates and reports on these metrics to leadership teams.
•Identified opportunities for process improvements based on data analysis.
•Recommended and implemented changes to enhance operational efficiency and effectiveness.

Education
Ph.D. Candidate in Mechanical Engineering
Lamar University

M.Sc. in Electrical Engineering
South Dakota School of Mines

B.Sc. in Applied Mathematics
Polytechnic University
Contact this candidate