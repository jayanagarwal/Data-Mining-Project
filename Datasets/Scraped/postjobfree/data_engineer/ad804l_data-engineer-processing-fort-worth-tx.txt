SINDHU KASIREDDY
Irving, TX, USA +1-469-***-**** ad804l@r.postjobfree.com

SUMMARY
● Data Engineer with over 4 years of experience in designing and implementing scalable data solutions in the financial
and healthcare sectors.
● Proficient in leveraging technologies such as Apache Spark, AWS (Redshift, S3, Lambda, Kinesis), Kafka, and
Snowflake to enhance data processing speed, accuracy, and accessibility.
● Adept at migrating legacy systems to modern cloud architectures, resulting in significant cost reductions and
performance improvements.
● Skilled in implementing real-time data processing systems for high-frequency trading environments, ensuring sub-
second latency and reliability.
● Experienced in data governance, security, and compliance, with a focus on safeguarding sensitive information in
regulated industries.
● Proven ability to collaborate with cross-functional teams, translating business requirements into technical solutions
that drive data-driven decision-making.
● Committed to continuous improvement through automation, performance tuning, and the application of best
practices in data engineering and cloud infrastructure management.
PROFESSIONAL EXPERIENCE
Goldman Sachs Group Remote, USA
Data Engineer Jun 2023 - Current
● Designed and implemented robust ETL pipelines utilizing Apache Spark and Python, resulting in a 40% improvement
in data processing speed for financial data streams.
● Led the migration of legacy data systems to cloud-based solutions using AWS Redshift and S3, reducing data retrieval
time by 30% and storage costs by 20%.
● Engineered seamless data integration processes from multiple sources, including real-time trading platforms, using
Kafka and Airflow, enhancing data accuracy and availability.
● Conducted performance tuning of complex SQL queries and database systems, which improved query efficiency by 50%
and reduced system lag during peak trading hours.
● Implemented stringent data governance and security measures, including encryption and access control protocols,
ensuring compliance with internal and regulatory standards.
● Worked closely with quantitative analysts, data scientists, and business stakeholders to translate business requirements
into technical specifications, driving data-driven decision-making.
● Automated repetitive data engineering tasks through the use of Python scripting and Jenkins CI/CD pipelines, leading
to a 25% reduction in manual workload.
● Designed and implemented real-time data processing systems for high-frequency trading (HFT) environments, ensuring
sub-second latency and reliability in data delivery.
● Developed data validation frameworks to ensure the integrity and accuracy of financial data, resulting in a 99.9% data
accuracy rate for downstream analytics.
● Tech Stack: Apache Spark, Python, AWS (Redshift, S3), Kafka, Airflow, SQL, Jenkins, CI/CD, Data Governance,
Encryption, High-Frequency Trading (HFT), Data Validation.
Sigma Info Solution Bengaluru, India
Data Engineer Jul 2020 - Jan 2022
● Developed and maintained scalable data pipelines using AWS services such as S3, Lambda, and Redshift, ensuring
efficient and reliable processing and storage of healthcare data.
● Designed and implemented data warehouse solutions with Snowflake, enhancing data accessibility and improving query
performance by 40% through optimized schema design and data partitioning strategies.
● Engineered automated ETL processes using AWS Glue and Apache Airflow, resulting in a 30% reduction in data
processing times by automating data ingestion, transformation, and loading.
● Utilized AWS EMR and Spark for big data processing, enabling the handling of large-scale healthcare datasets and
performing complex data transformations and aggregations.
● Integrated real-time data streaming solutions using AWS Kinesis and Kafka, providing up-to-the-minute insights and
enhancing decision-making capabilities for healthcare operations.
● Implemented robust data security practices with AWS IAM, VPC, and KMS, ensuring compliance with HIPAA regulations
and safeguarding sensitive healthcare information.
● Optimized Snowflake performance by implementing materialized views, clustering, and query optimization techniques,
leading to a 50% reduction in query execution times.
● Automated infrastructure deployment using AWS CloudFormation and Terraform, ensuring consistent and
reproducible deployment of data infrastructure across multiple environments.
● Developed custom Python and SQL scripts for data extraction, transformation, and analysis, automating routine data
tasks and increasing efficiency by 35%.
● Tech Stack: AWS (S3, Lambda, Redshift, Glue, EMR, Kinesis, IAM, VPC, KMS, CloudFormation, CloudWatch), Snowflake,
Apache Airflow, Apache Spark, Kafka, Python, SQL, Terraform, Data Wrangler.
Magna Info Tech Bengaluru, India
Data Engineer Jul 2019 - Jun 2020
● Utilized advanced statistical techniques and Python to derive actionable insights from vast financial datasets,
significantly enhancing data-driven decision-making and strategic financial planning.
● Developed and optimized SQL queries, achieving a 20% improvement in data retrieval times for financial reporting.
Designed robust ETL pipelines to ensure accurate and timely data integration from multiple financial systems.
● Designed and implemented A/B testing frameworks to optimize user engagement and conversion strategies across
digital platforms, resulting in a 35% increase in customer conversion rates and providing valuable user experience
insights.
● Created customized financial dashboards and reports in Power BI by closely collaborating with finance managers,
reducing the reporting workload by 30%.
● Led a data quality initiative that reduced financial data discrepancies by 25% and authored comprehensive
documentation to support ongoing data quality efforts.
● Leveraged AWS services to implement scalable cloud-based data storage and processing solutions, improving data
accessibility and ensuring high availability for financial analytics, leading to a 20% increase in processing efficiency.
● Architected and maintained data pipelines using Apache Airflow and Kafka to streamline the ingestion, processing, and
transformation of large-scale financial data, ensuring high reliability and performance.
● Developed and maintained data models and schemas using dbt and Snowflake, facilitating efficient data warehousing.
● Tech Stack: Python, SQL, Power BI, AWS, Apache Airflow, Kafka, dbt, Snowflake, A/B Testing Frameworks.

PROJECTS
Credit Risk Prediction:
● Combined the training and validation set and performed data pre-processing on both numerical and categorical
columns.
● Conducted up and down sampling techniques for the highly imbalanced target variable.
● Performed various classification algorithms Gaussian Naive Bayes baseline model, knn, Random Forest Classifier
achieving the best accuracy for the last model.
Optimization of Hospital Data Exchange System via Advanced ETL Processes:
● Developed a robust ETL framework using Python and SQL Server Integration Services (SSIS), which streamlined the
extraction, transformation, and loading of over 1 TB of clinical and administrative data daily, achieving a 30% reduction
in cycle times.
● Implemented automated data quality checks and balances within the ETL pipelines, which increased data accuracy by
40%. Integrated compliance modules to ensure adherence to HIPAA and GDPR, significantly minimizing risk exposures.
● Engineered data integration solutions that connected Epic Systems with third-party applications, enhancing data
interoperability. This allowed for seamless data flows that supported advanced analytics and real-time health
monitoring, benefitting over 10,000 healthcare providers.
TECHNICAL SKILLS
Methodologies: SDLC, Agile/ Scrum, Waterfall
Language & Databases: Python, SQL, R, SCALA, MySQL, MS SQL Server, ETL.
Python Packages: Pandas, NumPy, Matplotlib, SciPy, Scikit-Learn, SeaBorn, PyTorch, ggplot2, Plotly
Data Components: HDFS, Hue, MapReduce, PIG, Hive, HCatalog, HBase, Sqoop, Impala, Zookeeper, Flume, Kafka, Yarn,
Cloudera Manager, Kerberos, Pyspark Airflow, Kafka Snowflake
Data Analytics Skills: Data Manipulation, Data Cleaning, Data Visualization, Exploratory Data Analysis, Data Analysis
Others: AWS, AZURE(Databricks), NLP, A/B Testing, Hypothesis testing, ETL, Hadoop, Spark, Big Query, Apache Airflow,
Tools: Tableau, Power BI, Advanced Excel, Visual Studio, GIT, Jupyter Notebook
Version Control: Git, GitHub
Operating Systems: Windows, macOS
EDUCATION
Master of Science in Computer/Information Technology Services Aug 2023
Lindsey Wilson College, Columbia, KY, USA
Contact this candidate