Vishnupriya Rachakatla
Tampa,FL +1-217-***-**** ad73jp@r.postjobfree.com Linkedin GitHub Portfolio Tableau PROFESSIONAL SUMMARY
Self-Motivated and Accomplished Data Engineer adept at leveraging SQL, Python, and Scala to drive robust data solutions, visualization, and machine learning across diverse platforms. Proficient in:
• Data Processing & Analysis: Managed 50+ petabytes of streaming data using Apache Spark, Apache Flink, and Kafka, ensuring actionable insights and outcomes.
• ETL Expertise: Orchestrated transformations of 100+ terabytes into Amazon Redshift and Google BigQuery, ensuring data integrity via Talend, Apache NiFi, and stringent ETL processes.
• Database Management & Optimization: Fine-tuned query performance across 5 different DBMS, handling 500+ queries hourly, bolstering efficiency and system availability.
• Cloud-Based Solutions and Big-data: Designed and deployed cloud solutions across AWS, GCP, and Azure, managing 1,000+ data workflows, ensuring seamless operations.
• Data Quality & Security: Implemented robust data quality checks, security measures, and compliance standards using Talend, Apache NiFi, and security protocols.
• Automation & Optimization: Automated deployment processes, orchestrated Kubernetes, improving system efficiency by 20% through performance tuning and optimized data pipelines.
• Data Visualization: Crafted professional visualizations using Tableau, Power BI, enhancing data representation for stakeholders, enabling insightful decision-making.
• Machine Learning Insights: Derived actionable insights, built 30+ predictive models using Python, R, TensorFlow, PyTorch, contributing to data-driven decision-making and efficiency.
• API Development and Integration: Proficient in designing and implementing APIs to facilitate seamless data exchange and drive efficient communication between systems, enhancing overall data accessibility and real-time insights.
• Proficient professional with comprehensive expertise in Unix systems, adept coding skills, and a mastery of Python programming language. Experienced in RESTful API development and implementation, coupled with a strong background in Artificial Intelligence (AI) technologies. Excels in leveraging these skills to engineer innovative solutions and deliver robust, data-driven applications in dynamic environments
EDUCATION
Master’s in Management Information’s systems (CGPA 3.65) August 2022-December 2023 University of Illinois at Springfield, IL USA
SKILLS
Programming : R, PYTHON, SQL, NoSQL, C, C++, JAVA, JAVASCRIPT, Scala, bash, Unix, SAS Data Modelling & ETL : Spark, Flink, Nifi, Kafka, Informatica, Talend, Data Bricks Database Management Systems : Hadoop, Cassandra, Hive, Apache HBase, Amazon Redshift, Google BigQuery, Microsoft SQL Server, MySQL, PostgreSQL, Oracle, MongoDB, Data ware Housing : Amazon Redshift, Google BigQuery, Snowflake, Teradata, Microsoft Azure Synapse Analytics Data Visualization : Tableau, Power BI
Cloud Platforms : Amazon Web Services (AWS), Google Cloud Platform (GCP), Microsoft Azure, Snowflake Data Modelling and SQL : SQL, Apache Avro, Parquet, Schema Registry Version Control : Git, GitHub
Containerization : Docker, Kubernetes
Monitoring and Logging : ELK Stack (Elasticsearch, Logstash, Kibana), Prometheus, Grafana, Splunk, New Relic Data Quality and Validation : Nifi, Talend, Trifacta, Great Expectations Data Streaming : Kafka, Apache Pulsar, AWS Kinesis, Google Cloud Pub/Sub, Azure Event Hubs, Flink Data Security and Encryption : Apache Ranger, Apache Knox, Amazon KMS, Azure Key Vault, GCP Key Management Service Job Scheduling and Orchestration : Apache Airflow, Kubernetes, Luigi Data Serialization : JSON, Avro, Parquet, ORC, XML, Protobuf Machine learning tools : TensorFlow, Pandas, NumPy, PyTorch, scikit-learn, PySpark Soft skills : Problem solving skills, mentoring, strong critical thinking. HIGHLIGHTED PROJECTS
• Tokyo Olympic Data Analysis with Azure: Transformed Olympic data into actionable insights using Azure Data Factory, Data Lake Gen 2, Synapse Analytics, and Azure Databricks.
• Tableau: Created various DVs using Tableau, Power BI, Qlik, and Looker.
• Tic Tac Toe Game by Java: Built a Tic Tac Toe game with Java, JavaFX, IntelliJ IDEA, Git, and JUnit.
• Dice Game by Python: Developed an interactive Dice Game in Python using Tkinter/ PyQt, with GitHub for version control.
• Quiz Game by Python: Crafted a dynamic Quiz Game in Python with a sleek Tkinter/PyQt interface, using GitHub for collaborative version control.
CERTIFICATIONS
• IBM Data Engineer Professional certificate
• Machine learning specialization by the University of Washington PROFESSIONAL EXPERIENCE
Data Engineer RVO Health South Carolina October 2022 – September 2023
• Led data solution projects, ensuring seamless integration and implementation of data solutions across various projects.
• Worked with cross-functional teams, collaborating with business analysts, architects, engineers, data analysts, and data scientists to gather and understand business and technical requirements.
• Built data pipelines, designing and developing pipelines from various data sources to target data warehouses using batch data load strategies.
• Created infrastructure, conceptualizing and generating infrastructure to facilitate efficient data access and analysis.
• Documented database designs, preparing comprehensive documentation including data models, metadata, ETL specifications, and process flows for business data integrations.
• Conducted code reviews, performing periodic code reviews and developing test plans to ensure data quality and integrity.
• Contributed to strategic planning, providing input into strategies to drive the team forward, leveraging technical acumen and delivery of value.
• Executed proof of concepts, implementing proof of concepts to enhance technical processes and validate new approaches.
• Worked with Spark and Scala/Python, utilizing Spark (RDDs / Data Frames / Dataset API) with Scala/Python to build and maintain complex ETL pipelines.
• Utilized AWS technologies, leveraging AWS services such as Kinesis, S3, RedShift, DMS, and Athena for efficient data management and processing.
• Researched and resolved issues, investigating and troubleshooting potential issues presented by stakeholders within the data ecosystem.
• Implemented CI/CD processes, applying GitHub and CI/CD processes for version control and continuous integration, ensuring smooth development workflows.
• Employed compute technologies, using EMR and Databricks for efficient and scalable data processing.
• Managed job orchestration, handling job orchestration using tools like Airflow and Databricks Workflows to automate and streamline data workflows.
• Worked with advanced data platforms, engaging with platforms like DBT, Snowflake, and DeltaLake to deliver advanced data solutions and maintain robust data architectures
Data Engineer Tech Mahindra Hyderabad, India September 2019- August 2022
• Spearheaded data engineering initiatives on cloud platforms such as Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP), leveraging their respective services for scalable and cost-effective data solutions.
• Designed and implemented data pipelines on AWS using services like Amazon S3 for storage, AWS Glue for ETL, and Amazon Redshift for data warehousing.
• Utilized Microsoft Azure services such as Azure Data Lake Storage, Azure Databricks, and Azure Synapse Analytics to build end-to-end data solutions tailored to business needs.
• Implemented data processing workflows on Google Cloud Platform, utilizing Google Cloud Storage, Google BigQuery, and Google Dataflow for seamless data ingestion, transformation, and analysis.
• Demonstrated exceptional communication skills, effectively conveying complex technical concepts to diverse audiences through written documentation and oral presentations.
• Cultivated a collaborative and inclusive work environment, fostering teamwork, creativity, and mutual respect among team members.
• Acted as a mentor and technical guide to junior engineers, providing support, guidance, and knowledge transfer to foster their professional growth and development.
• Demonstrated adaptability and resilience in a fast-paced environment, effectively managing competing priorities and delivering high-quality results under tight deadlines
• Actively pursued continuous learning and professional development, staying abreast of emerging technologies, industry trends, and best practices.
• Exhibited strong problem-solving skills, identifying and resolving complex technical challenges to ensure the integrity and reliability of data solutions.
• Fostered strong relationships with stakeholders and business partners, collaborating closely to understand their needs and deliver solutions.
• Demonstrated a commitment to quality and excellence, implementing rigorous testing and validation processes to ensure the accuracy and reliability of data solutions.
• Proactively identified opportunities for process improvement and optimization, driving efficiency gains and cost savings in data engineering processes.
• Upheld high standards of professionalism and integrity, demonstrating honesty, transparency, and accountability in all interactions and decision-making processes
• Contributed to a positive and inclusive organizational culture, embodying the company's values and promoting diversity, equity, and inclusion in the workplace.
Contact this candidate