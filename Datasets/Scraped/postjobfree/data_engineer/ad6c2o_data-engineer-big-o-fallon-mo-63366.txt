SAMYUKTHA POTLA
DATA ENGINEER
636-***-**** ad6c2o@r.postjobfree.com St. Louis, MO
SUMMARY
• 3+ years of experience as a Data Engineer with a proven track record of designing and building scalable data solutions. Skilled in leveraging Cloud Data Engineering tools (AWS, Azure) to deliver Data Warehouses, Hadoop ecosystems, Big Data analytics pipelines, and Data Visualization and Reporting solutions.
• Experience in using Apache Spark for large-scale data processing tasks, processing terabytes of log data in under 2 hours to generate daily user activity reports.
• Proven ability to leverage Hadoop for big data storage, processing, and analysis and skilled in utilizing tools like Hive, Apache Airflow, DataBricks, Apache Kafka, Apache Spark and Apache Flink.
• Proficient in developing and implementing automated data quality checks within Snowflake pipelines, identifying and resolving data discrepancies.
SKILLS
Programming Language: Python, R, SQL
IDE’s: PyCharm, Jupyter Notebook
Big Data Ecosystem: Hadoop, Hive, Apache Airflow, Apache Kafka, Apache Spark, Apache Flink, DataBricks Cloud Technologies: AWS (EC2, S3, RDS, Lambda, Glue, Athena, AWS Pipeline, Redshift), Azure, GCP Visualizations: Tableau, Power BI, Excel
Packages & Data Processing: NumPy, Pandas, Matplotlib, Seaborn, TensorFlow, PySpark, Data Pipelines, Jenkins Version Control & Database: GitHub, Gitlab, SQL Server, PostgreSQL, MongoDB, DynamoDB, MySQL, Snowflake Operating Systems: Windows, MacOS
EDUCATION
Master of Science in Computer Science May 2023
Northwest Missouri State University, Maryville, Missouri Bachelor of Technology Electrical and Electronics of Engineering Aug 2021 BVRIT Hyderabad College of Engineering for Women, Hyderabad, India EXPERIENCE
PNC Financial Services, IL Jan 2023 – Current
Data Engineer
• Built and orchestrated data pipelines using Airflow to automate data ingestion, processing, and transformation tasks.
• Developing HiveQL queries to extract, transform, and load (ETL) data from various sources into the data warehouse
• Optimized Spark jobs using partitioning and caching techniques, achieving a 3x performance improvement for complex data aggregations and feature engineering tasks.
• Utilized Databricks libraries and tools for data visualization and model deployment, streamlining the transition from data processing to model production.
• Boosted data pipelines using AWS Glue and Spark to improve data processing efficiency, resulting in a 20% reduction in data processing costs.
• Implementing AWS Lambda functions triggered by incoming files in S3 to perform data validation and transformation before loading into Amazon Redshift, enhancing data quality and pipeline efficiency.
• Improved data warehouse loading efficiency by 25% by optimizing Snowflake data ingestion pipelines using clustering and partitioning techniques.
XLogic Technologies, India Jan 2020 - Nov 2021
Data Engineer
• Increased data pipeline efficiency by 20% by optimizing Hive queries for large-scale data warehousing, reducing processing time for daily sales reports.
• Automated daily data ingestion pipelines using Airflow, ensuring timely data availability for machine learning models, resulting in a 10% improvement in model accuracy.
• Designed and deployed a real-time Kafka streaming platform to capture website clickstream data, enabling real-time customer behavior analysis and personalization.
• Constructed interactive Power BI dashboards for senior management, reducing report generation time and improving data accessibility.
• Calculated and managed Azure Data Lake Storage (ADLS) for scalable storage of structured, semi-structured, and unstructured data.
• Facilitated cross-functional collaboration by integrating Azure DevOps with collaboration tools, a 30% improvement in communication and information sharing across development teams.
• Migrated existing batch processing pipelines to Databricks, leveraging its cloud-based scalability to handle increasing data volumes and reduce on-premise hardware costs.
• Enhanced data pipeline efficiency by 15% through code optimization and leveraging PySpark's in-memory processing capabilities.
Contact this candidate