: https://www.linkedin.com/in/gowthamid***/
Gowthami D
Sr DevOps /Cloud Engineer
: +1-917-***-****
:ad0cqe@r.postjobfree.com
Professional Summary:
An enthusiastic DevOps Engineer with 8+ years of IT experience, specializing in Cloud, DevOps tools, CI/CD pipelines, Configuration Management, Monitoring, Linux, and Windows system administration. Expertise in cloud services, automation, and networking across several platforms in dynamic work environments.
Extensive working experience on AWS Cloud Services like EC2, ELB, AWS Batch, Auto Scaling, AMI, IAM, SQS, Dynamo DB, VPC, Route53, RDS, S3 Buckets, Elastic search, Elastic Load Balancer, Elastic Bean Stalk, EFS, Dynamo DB, CloudFront, Route 53, AKS, Cloud Formation, CloudWatch, Cloud Trail, Cloud Security, Lambda, Service Catalog, AWS SQS, SNS, ECS, ECR, Fargate, Kinesis, Redshift.
Established a Secure Network foundation by setting up VPC Private Link, Direct Connect, Transit VPC, VPC Peering, IPsec VPN, and CDN for data transfer and connections across AWS services and accounts, including seamless VPC peering integration.
Created APIs with AWS Lambda for server management and code execution. Well-versed in IP networking, VPNs, DNS, load balancing, and firewall setup. Managed AWS EMR clusters and security groups.
Worked on Azure Development services like Azure web application, Azure storage, Azure SQL DB, App services, Azure VMs, App Insights, Azure Kubernetes Service, Azure Container Registry, Azure Functions, Azure Resource Manager, Azure DNS, Azure VPN Gateway, Azure AD, Azure Search, and Notification hub.
Expertise in Azure Platform Development, Deployment Concepts, hosted Cloud Services, platform service, and close interface with Windows Azure Multi-factor Authentications and Continually changing architecture to shift software system offerings to a distributed environment and service-based architecture.
Responsible for configuring shared access signatures (SAS) tokens, storage access policies, and implementing Azure Cloud Infrastructure.
Experience in GCP services like GCP Compute Engine, GCP Cloud Load Balancing, GCP Cloud storage, GCP Cloud SQL, GCP Cloud Monitoring, GCP Cloud Deployment Manager, and GCP Kubernetes Engine.
Experience in managing IAM policies with active directory integration to manage security in GCP.
Proficient in Terraform for resource provisioning in both AWS and Google Cloud Platform, including GCP's deployment manager. Implemented key Terraform features such as defining Terraform modules, execution plans, resource graphs, and change automation.
Experience with airgap installation of Terraform along with configuration with Forseti and Cloud Armor and provisioned large-scale environments using Terraform.
Production experience in large environments using configuration management tools Chef, Ansible, and Puppet.
Experience with Ansible playbooks, Vault, and Tower as a configuration management tool to automate repetitive activities, deploy apps, manage changes, and automate software updates and functionality verification.
Encompassed Ansible playbooks with Python SSH as the Wrapper to manage configurations of OpenStack Nodes and test Ansible playbooks on AWS instances using Python.
Experience in Chef Server Enterprise on-premises, workstation, bootstrapped the nodes using the knife and automated by testing Chef Recipes/Cookbooks with Test-kitchen.
Worked on a configuration management Puppet tool which includes installing Puppet master, agents, and writing manifests from scratch, and pushing them to agents for CI &CD.
Configured Cloud Compute systems using OpenStack on Ubuntu, collaboration using Orchestration with Keystone, and Kubernetes within OpenStack.
Proficient in utilizing Jenkins to orchestrate and automate end-to-end CI/CD workflows by creating Jenkins pipelines and architected clusters ensuring efficient build, deployment, and troubleshooting within DevOps environments.
Build management, set up CI/CD pipeline, and deployed using Azure DevOps (both classic and yaml pipelines).
Experience in migrating on-prem infrastructure to cloud platforms like AWS and performing virtualization using Vagrant with VMware.
Migration of builds from TeamCity and Octopus Deploy into Azure DevOps by creating new pipelines in conjunction with Microsoft Azure.
Expertise in Configuring, and managing Docker Containers, Docker Images for Web Servers, and Applications servers such as Apache, and Tomcat using Docker and integrated with Amazon RDS database.
Experience in scheduling, deploying, & and managing container replicas onto a node cluster using Kubernetes, worked on building K8’s run time environment of the CI/CD system to build, test & and deploy in an open-source platform.
Skilled in implementing and optimizing build tools like Maven, Yarn, Bamboo, Gradle, GoCD, and Ant within DevOps workflows to enhance software development and deployment efficiency.
Experience with Splunk architecture and various components including Search Heads, Indexers, Deployment servers, Deployer, License Master, and Heavy/Universal Forwarders.
ELK clusters (Elasticsearch, Logstash, Kibana) are created and maintained for enterprise logging. Involved in evaluating system logs using ELK stack to assess the infrastructure needs for each application and deploy it on the Azure platform.
Experience in Server monitoring and application monitoring tools like Prometheus, Grafana, AppDynamics, Icinga, New Relic, and Datadog.
Utilized cloud-based Application Insights monitoring tool to gain comprehensive insights into application performance, identifying issues, and optimizing user experiences.
Automated the process using Shell, Python, Ruby, PowerShell, JSON, YAML, and Groovy scripting languages.
Integrated and Developed Dynamic infrastructure with Groovy and Python to Automate AWS Operations.
Enhanced and deployed Microservices-based applications using Spring Boot and Spring Cloud and created dynamic documentation for RESTFUL web service using Swagger.
Experience in creating and using schema for SQL & NoSQL databases, MySQL, MongoDB, and DynamoDB in AWS.
Expertly utilized JFrog, CloudRepo, Sona-type Nexus, and npm tools to seamlessly manage artifact repositories.
Experience with Security and scanning tools such as HP Fortify, Veracode, SonarQube, and Black Duck.
Enhanced AWS DevSecOps security with effective implementation of AWS GuardDuty, ensuring proactive threat detection, streamlined incident response, and real-time monitoring for potential vulnerabilities.
Managed Kafka clusters, ensuring high availability, scalability, and reliability by conducting regular monitoring, and performance tuning of brokers, topics, and partitions for optimal message throughput.
Using tools, performed Static Application Security Testing (SAST) and Dynamic Application Security Testing (DAST).
Configured and used Twistlock to scan and implement security controls.
Integrated Redmine, and JIRA with version control systems like Git and SVN, enabling a seamless flow of information between code changes and tasks.
Proficient in managing the source code control of multiple development efforts using Git, GitLab, GitHub, Bitbucket, SVN, and Perforce version control tools.
Hands-on experience in Configuration Management (CM) policies and approaches regarding software development life cycle (SDLC) along with automated scripting using Bash, PowerShell, Perl, and Python scripting.
Exposure to all aspects of the Software Development Life Cycle (SDLC) such as Analysis, Planning, Development, Testing, Implementation, and Post-production analysis of the projects.
Experienced in development methodologies including Waterfall, Scrum, Agile, and hybrid.
Knowledge of using Routed Protocols like TCP FTP, SSH, HTTP, ICMP, UDP, ACLs, HTTPS, and direct connect and experience with installations, support, configuration, and maintenance of Red Hat Enterprise Linux, CentOS, and Ubuntu.
Technical Summary
Scripting Languages Bash, Power shell, Perl, Ruby, Type script, JSON, Groovy, YAML Version control tools Git, Gitlab, Bitbucket, TFS, Subversion SVN CI/CD Jenkins, GitLab, Bamboo, Maven, Concourse, U-Deploy, Octopus, Azure DevOps Automation Tools Ansible, Terraform, Chef, Puppet, kickstart Containerization Tools Docker, Kubernetes, ECR, ACR, EKS, AKS, Openshift Artifact Managing Tools Nexus, JFrog, CloudRepo, Sona-type Nexus, Web servers Tomcat, Nginx, Apache, Web Logic, Web Sphere, IIS Infrastructure Tools Terraform, CloudFormation, Azure Resource Manager, HashiCorp Packer Database Oracle MySQL, MongoDB, AWS RDS, Milvus vector database, Dynamo DB, Code Security/Security tools SonarQube, Forseti Security, Cloud Armor, Dev SecOps, Fortify, Twistlock Vaults Hashicorp Vault, Azure key Vault, Azure Recovery Services Vault Monitoring Tools Splunk, Nagios, CloudWatch, ELK, Prometheus, Grafana Reporting & Ticketing Tools Service Now, Redmine JIRA, Azure Boards, Sr. AWS DevOps Engineer AUG 2022-Present
Client: MEDPACE
CINCINNATI, OHIO
Responsibilities:
• Designing highly available, cost-effective, and fault-tolerant systems using EC2 instances, S3 buckets, (AMI), Maintained roles and groups using AWS IAM, also maintained user accounts, RDS, Route 53, VPC, RDB, Dynamo DB, Code Commit, SES, SQS & SNS services in AWS.
• Used the security groups, Elastic Load Balancer and AMIs, and Auto scaling to design affordable, redundant, and highly reliable systems. Designed and developed AWS Cloud Formation templates to create custom VPC, Subnets, and NAT to ensure deployment of web applications.
• Built up a fully managed, scalable video processing application on AWS and scaled on AWS ECS Fargate for application and events with SQS and Lambda.
• Managed migration of on-premises applications to the cloud, and employed essential AWS tools, including ELBs and Auto-Scaling, ensuring scalability, elasticity, and high availability.
• Implemented EFS storage to mount on different EC2 instances for sharing the Volume. Implemented a serverless architecture using API Gateway, Lambda, and Dynamo DB and deployed AWS Lambda code from Amazon S3 buckets.
• Designed a comprehensive AWS project pipeline, utilizing Groovy for Jenkinsfile scripting, Python for dynamic AWS interactions, and Jenkins for continuous integration and deployment, resulting in streamlined and efficient cloud workflows.
• Utilized AWS S3 for snapshots and configured lifecycle management for application logs, ensuring deletion of old logs and archival based on retention policies. Managed data through S3 buckets, and established instance backups to the S3 bucket.
• Created numerous Lambda scripts for encrypting unencrypted EBS volumes, Created Boto3-based Lambda functions to efficiently manage EBS snapshots, eliminate unused volumes, and optimize costs by removing outdated AMIs.
• Used CloudFront to deliver content from AWS edge locations to users, allowing for further reduction of load on front- end servers.
• Led successful V2V migrations and SAP system transfers, optimizing RTO and RPO by expertly planning backup schedules, replication, and failover procedures.
• Implemented GCP Firewall rules to allow or deny traffic to and from the VM's instances based on specified configuration and used GCP cloud CDN to deliver content from GCP cache locations drastically improving user experience and latency.
• Used GCP App Engine for deploying and scaling web applications and services developed with Java. Leveraged GCP cloud services such as Compute, auto-scaling, and VPC to build secure, highly scalable, and flexible systems that handled expected and unexpected load bursts.
• Utilized Terraform scripts to automate previously launched manual instances, implementing Infrastructure as code principles for AWS infrastructure management, including execution plans, resource graph visualization, and change automation.
• Written Terraform scripts from scratch for building Dev, Staging, Prod, and DR environments.
• Worked on writing Ansible role which combined with Terraform and Terragrunt that deploys Elastic Beanstalk applications.
• Configured software and services using Ansible Playbooks, added users to Identity access and management, and created S3 buckets to hold deployment files.
• Worked with Ansible to automate the process of deploying and testing the new builds in each environment setting up a new code and configuring machines and servers.
• Created additional Docker Slave Nodes for Jenkins using custom Docker Images and pulled them to ECR. Worked on all major components of Docker like Docker Daemon, Hub, Images, and Registry.
• Deployment to AWS with Chef, Ansible, and Terraform towards AWS cloud or GCP and used Terraform for infrastructure provisioning and created custom solutions with Chef or Puppet configurations.
• Utilized Chef Enterprise and Chef Open Source, Chef DK, Chef Workstation, Chef Server, Chef-Client.
• Using Kubernetes to manage containerized applications using its nodes, ConfigMaps, selector, and Services, and deployed application containers as Pods.
• Integrated Kubernetes with network, storage, and security to provide comprehensive infrastructure and orchestrated containers across multiple hosts. Using Kubernetes, I have controlled and automated application deployments and updates and orchestrated deployment.
• Worked on Jenkins to automate building and deploying various applications based on Java, NodeJS, docker, React JS, and Python.
• Involved in planning and setting up Continuous Integration for various properties on Jenkins with Commit, Assembly, Deploy, and Cloud jobs.
• Deployed and configured version-controlling repositories like Git with branching, forks, tagging, and notifications. and administering GitHub.
• Involved in ensuring the efficient functioning of data storage and processing functions by following company security policies and best practices in Cloud security.
• Successfully integrated Istio into containerized applications to enhance security and observability. Utilized Istio's mutual TLS (mTLS) capabilities to establish secure communication between services.
• Worked with the development team with Maven and Worked on authoring pom.xml files, performing releases with the Maven release plugin, and managing Maven repositories.
• Integrated SonarQube with the CICD tools to ensure continuous code quality inspection. Additionally worked on tools like Fortify, and Coverity.
• Data warehouse migration to AWS Redshift using Apache Kafka and Mirror Maker for data migration.
• Implemented security measures, including authentication, authorization, encryption, and access controls, to safeguard Kafka clusters and sensitive data.
• Set up logging configurations in Nginx to track and analyze web traffic. Integrated Nginx logs with monitoring tools for real-time analysis.
• Installed Twistlock vulnerability security scanning tool and integrated it with Jenkins and OpenShift runtime containers.
• Installed and configured the Splunk monitoring tool, while using it for monitoring network services and host resources. Continuously monitor the performance of the applications in the production environment using Prometheus and Graphana. Designed, developed, and implemented multi-tiered Splunk log collection solutions.
• Worked on operating hybrid environments and monitored cloud environments using Datadog, and AppDynamics for the performance of the applications.
• Utilized Cloud Watch to monitor resources such as EC2, CPU memory, Amazon RDS DB services, Dynamo DB tables, and Elastic Block Store (EBS) volumes to set alarms for notification or automated actions and to monitor logs for a better understanding and operation of the systems.
• Utilized AWS Guard Duty to address security concerns, notifying the security team via Slack. Implemented AWS Lambda to establish guardrails for compliance and cost management. Linked CloudWatch with GuardDuty rules to categorize security severity levels from 2 to 8.9.
• Implemented automated security checks using AWS security tools like DevSecOps, AWS Config, AWS Security Hub, and AWS Trusted Advisor to identify and address vulnerabilities during development and deployment.
• Assisted in the integration of DevSecOps pipeline components, using a code repository, an artifact repository, a security assessment platform, and an orchestrated integration and delivery platform to enable automated application building, testing, securing, and deployment.
• Implemented Redmine for effective documentation, and used Gantt charts and milestone tracking for efficient release planning and seamless team coordination in DevOps workflows.
• Used programming languages like Python, and Golang to create APIs and integrated software stack.
• Implemented Security Scans like Static and Dynamic Application testing at each layer of the DevOps life cycle and converted the existing DevOps methodologies/workflows to the DevSecOps model.
• Created VMs, set up VM priorities, and created templates and Snapshots.
• Developed automation scripting in Python to deploy and manage Java applications across Linux servers.
• Implemented Shell, Perl, and Python scripts for release and build automation.
• Developed PowerShell scripts to automate the project creation, setting permissions for users, and groups in TFS.
• Worked in the deployment of Java applications through WebLogic/WebSphere Application servers. Environment & Tools: VPC, S3, AMIs, RDS, GCP Cloud CDN, RDS, Route 53, IAM, VPC, GCP App Engine, EFS, Docker, S3 Buckets, CloudWatch, EBS, Terraform, Ansible, Jenkins, Docker, Git, Redmine, Twistlock, Splunk, AWS GuardDuty, Grafana, SonarQube, Debian, SonarQube, Python, Shell, Perl, Python. Sr. Azure Cloud Engineer JAN 2022-JULY 2022
Client: VOYA FINANCIAL
PHILADELPHIA, PA
Responsibilities:
• Designed, configured, and deployed Microsoft Azure for a multitude of applications utilizing the Azure stack (Including Computer, Web and mobile, Blobs, ADF, Resource Groups, Azure SQL DW, Cloud Services, and ARM focusing on high- availability, Redundancy, fault tolerance, and auto-scaling).
• Configured Azure web apps, Azure App Services, Azure Application Insights, Azure Application Gateway, Azure DNS, Azure Traffic Manager, and Azure Network Watcher.
• Designed, Planned, and created Azure VMs, Implemented, and connected to on-premises environments by managing a Cross Premises Azure VNets. Set up Azure VMs to meet security requirements like software-based functions (firewall, WAN optimization, and intrusion detection).
• Managed VM Backup and Recovery from a Recovery Services Vault using Azure PowerShell and Azure Portal. Involved in creating, validating, and reviewing solutions for data center migration to Azure cloud environment.
• Developed various build and deployment scenarios such as jobs to build from various branches and deploy tasks to the development server, QA server, and Staging/Production server using Azure DevOps.
• Integrated Databricks with various Azure services such as Azure Data Lake Storage, Azure SQL Database, and Azure Blob Storage to create a seamless data ecosystem.
• Proficiently managed SQL Server databases, ensuring optimal performance, security, and data integrity. Implemented robust backup and recovery strategies, minimizing downtime and data loss in critical situations.
• Initiated data engineering projects leveraging Azure File, effectively storing, and managing large datasets in a cloud environment.
• Demonstrated in configuring and utilizing Azure Backup services, guaranteeing data availability and disaster recovery in the cloud.
• Collaborated with the Kafka Admin team to establish Kafka clusters in QA and Production.
• Successfully designed and maintained Windows Distributed File System (DFS) solutions, enhancing file access and replication across the organization.
• Provided guidance on DFS architecture and optimization, ensuring efficient data distribution and redundancy.
• Successfully integrated the Milvus vector database into a real-time DevOps project, enabling efficient similarity search and retrieval of complex data structures.
• Leveraged Milvus to process and analyze incoming data streams, enhancing the project's capabilities in delivering real- time insights and recommendations.
• Successfully deployed and managed Mobile Device Management (MDM) solutions using Jamf, ensuring secure configuration, application deployment, and remote management of mobile devices.
• Created preliminary PowerShell code for moving Azure Classic workloads to the Azure Resource Manager version.
• Built development and test environments for different microservices by provisioning Kubernetes clusters on Azure using Docker, Ansible, and Terraform.
• Deployed and monitored Microservices using Pivotal Cloud Foundry (PCF), managed domains and routes with the cloud foundry also worked on PCF by using Docker Swarm and deployed spring boot applications.
• Worked on creating and deploying Azure Infrastructure as a Code using Terraform modules and ARM Templates.
• Wrote Ansible playbooks from scratch in YAML. Installing, setting up, and troubleshooting Ansible.
• Updated the existing scripts to Ansible playbooks to install configurations on multiple servers in Azure.
• Used Ansible Tower, which provides an easy-to-use dashboard and role-based access control, so that it is easier to allow individual teams access to use Ansible for their deployments.
• Developed Ansible scripts for automated server provisioning and Docker images for isolation, reducing the time between provisioning and deployment from over 2 hours to less than 10 minutes.
• Worked on GitHub to store the code and integrated it into Ansible to deploy the playbooks and manage servers and Docker containers with OS/Applications/Services/Packages and keep them in a loop.
• Used Ansible Control server to deploy playbooks to the machines and systems in the inventory.
• Set up chef repo, chef workstations, and chef nodes and managed multiple cookbooks in Chef, and implemented roles, and templates in Chef.
• Integrated Jenkins with various DevOps tools Nexus, SonarQube, Ansible and used the CI/CD system of Jenkins on Kubernetes container environment, utilizing Kubernetes and Docker for the runtime environment for the CI/CD system to build and test, and deploy.
• Used Kubernetes to deploy scale, load balance, scale and manage Docker containers with multiple namespace versions.
• Prototype CI/CD system with GitLab utilizing Kubernetes and docker for the runtime environment for the CI/CD systems to build test and deploy.
• Coordinated and assisted developers with establishing and applying appropriate branching, labeling/naming conventions using GitLab and analyzed and resolved conflicts related to merging of source code for Git.
• Building docker images using Azure pipelines and push to Artifactory and deploy to OpenShift containers using Kubernetes for the Microservices.
• Used build tools like Maven, and Gradle for multiple projects and integrated them with SonarQube for code quality inspection.
• Implemented and customized security policies within the selected CSPM tool to meet industry standards, aligned with the company's security needs, and cater to the specific cloud services and resources in use, while ensuring compatibility with the existing cloud infrastructure.
• Integrated Security Information and Event Management (SIEM) systems with various applications and services to enhance security monitoring and incident response capabilities.
• Installed, Configured, and Managed Monitoring Tools Splunk, and Nagios for Resource Monitoring/Network Monitoring/Log Trace Monitoring and used JIRA and Redmine for change control & and ticketing.
• Utilized App Dynamics and Dynatrace for thorough performance monitoring of application server and database resources under virtual user load.
• Worked in designing and implementing continuous integration systems using Azure DevOps by creating pipelines using Python and Shell scripts.
• Involved in writing Power Shell scripts for managing day-to-day transactions and automation of routine tasks.
• Provided 24 x 7 production support and development environments. Ability to communicate requirements effectively to team members.
Environment & Tools:
Azure web apps, Azure Application Insights, Application Gateway, DNS, Traffic Manager, and Network Watcher, Azure VNets, Azure PowerShell and Azure Portal, Jenkins, GitLab, Kubernetes, Docker, Docker Images, OpenShift, GitHub, GitLab, Ansible, Splunk, JIRA, Ansible Tower, Nagios, App Dynamics, Dynatrace, Ansible scripts, Azure DevOps, Python. DevOps Engineer SEPT 2019 – DEC 2021
Client: SEI INVESTMENTS
PHILADELPHIA, PA
Responsibilities:
• Built and configured a virtual data center in the Amazon Web Services cloud to support Enterprise Data Warehouse hosting including Virtual Private Cloud (VPC), Public and Private Subnets, Security Groups, Route Tables, and Elastic Load Balancer.
• Created Users, Groups, Roles, Policies, and Identity providers in AWS Identity Access Management (IAM) for improved login authentication. Involved in creating AWS AMI, have used Hash Corp Packer to create and manage the AMI's.
• Created Users, Groups, Roles, Policies, and Identity providers in AWS Identity Access Management (IAM) for improved login authentication. Involved in creating AWS AMI, and have used HashiCorp Packer to create and manage the AMI.
• Developed AWS Cloud Formation templates to create custom-sized VPC, EMR, Dynamo DB, subnets, EC2 instances, ELB, and security groups. Designed Network Security Groups (NSGs) to control inbound and outbound traffic access to network interfaces (NICs), VMs, and subnets.
• Worked on Kinesis Data Streams, and Kinesis Firehouse, and integrated with AWS Lambda for serverless data collection. Deployed AWS Lambda code from Amazon S3 buckets. Created a Lambda Deployment function and configured it to receive events from the S3 bucket.
• Created Snowflake Schemas by normalizing the dimension tables as appropriate and creating a Dimension named Demographic as a subset of the Customer Dimension.
• Performed bulk load of JSON data from S3 bucket to Snowflake. Used Snowflake functions to perform semi-structured data parsing entirely with SQL statements.
• Worked on Google Cloud Platform (GCP) services including cloud storage, cloud SQL, Compute Engine, cloud load balancing, stack driver monitoring, Cloud Armor, and cloud deployment manager.
• Designed and distributed Data across all the Nodes and Clusters on different availability zones in AWS Redshift and PostgreSQL and automated the infrastructure using Terraform in the AWS console.
• Configured Ansible to manage AWS workflow environments and automate the build process for core AMIs used by all application deployments including Auto scaling, and Cloud formation scripts.
• Managed and created autoscaling groups on top of AWS instances and also managed configuration and policies to CloudWatch Alarms and SNS messages by using Forseti.
• Maintained Artifacts in binary repositories using JFrog Artifactory and pushed new Artifacts by configuring the Jenkins project that uses the Jenkins Artifactory plugin.
• Responsible for installation & and configuration of AWS Code Build to support various Java builds and Jenkins plugins to automate continuous builds and publishing Docker images to the JFrog repository.
• Involved in building pipeline to drive all microservices builds to Docker registry and deploy to Kubernetes using AWS Code Build, Code Deploy, and Code Pipeline.
• Worked with Scheduling, deploying, and managing container replicas onto a node using Kubernetes and created Kubernetes clusters, worked with Helm charts running on the same cluster resources, and managed releases of Helm packages.
• Used OpenShift for creating new Projects, Services for load balancing and adding them to Routes to be accessible from outside, troubleshooting pods through ssh and logs, and modification of Buildconfigs, and templates.
• Deployed multiple Java applications in Kubernetes using EKS and OpenShift to maintain high availability.
• Configured pipelines on Jenkins & Concourse CI with integrating HashiCorp Vault for secrets, dynamic injection of environment-specific keys, and blue/green and canary style traffic shaping.
• Used Maven as a build tool on Java projects for the development of build artifacts on the source code by including necessary dependencies.
• Managed the source code for various applications in Bitbucket and Configured it with Jenkins to trigger automated builds.
• Integrated SonarQube with AWS Code Pipeline for continuous inspection of code quality and analysis with SonarQube scanner for Maven.
• Migrated application services from the Heroku Platform and deployed them into the AWS Cloud Platform.
• Monitored resources and applications using AWS cloud watch including creating alarms to monitor metrics such as EBS, EC2, ELB, RDS, and S3, and configuring notifications for the alarms generated based on events defined.
• Involved in Automation by using Python, Shell, and bash scripting and developed YAML scripts from scratch.
• Created alerts and monitoring dashboards using Prometheus and Grafana for microservices deployed in AWS.
• Tracking and prioritizing issues and new features for later releases of software using Redmine, JIRA, and Confluence. Environment & Tools: AWS, AWS Cloud watch, Git, Bitbucket, Kinesis, Snowflake, S3, AWS Lambda, Dynamo DB, GCP, Cloud Armor, Heroku Platform, Bamboo, Ansible, Terraform, Maven, Jenkins, Docker, JFrog Artifactory, Kubernetes, EKS, Helm charts, Openshift, Concourse CI, HashiCorp Packer, HashiCorp Vault, AWS Redshift, PostgreSQL, SonarQube, Python, YAML, Prometheus, Grafana, JIRA, Confluence.
Build and Release Engineer MAY 2017-AUG 2019
Client: coMakeIT
HYDERABAD, INDIA
Responsibilities:
• Designed
Contact this candidate