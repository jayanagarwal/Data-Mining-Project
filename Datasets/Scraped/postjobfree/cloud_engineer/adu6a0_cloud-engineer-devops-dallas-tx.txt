Pranav Kumar
AWS Cloud Engineer
adu6a0@r.postjobfree.com
+1-469-***-****
linkedin.com/in/pranav-k-aa96b1143
Professional Summary:
●Around 7+ years of Experience in IT professional as an AWS-Cloud Engineer with skills Certified Solution Architect in various domains like Education, Finance, Telecommunications, Energy Technologies., but not limited to capturing applications’ functional and nonfunctional requirements, mapping requirements to a set of AWS services- data, compute, storage, management, network, microservices and security,
●Experience in AWS Cloud Computing services, such as EC2, S3, EBS, VPC,Subnet, ELB, Route53, Cloud Watch,Security Groups,Nacl, EKS, IAM, CloudFront, RDS and Glacier.
● As an AWS / Azure-DevOps Cloud Data Engineer, having the strong exposure on providing solutions to current configuration management tools in AWS and developing CI/CD Pipeline and commitment of delivering quality products.
●Hands-on experience with Amazon Web Services - EC2, S3, API Gateways and Networking Services like VPC, Route53,Understanding of SSL/TLS security, AWS certificate manager.
●Provisioned
●Good experience with creating and managing DNS records in AWS Route53, Configure security groups and lock down the various authorized subnet and IP addresses in AWS.
●Good experience with AWS Simple Queue Services(SQS), AWS Simple Notification Services, SMTP and Cloudwatch. Monitor application metrics.
●Good experience with AWS Lambda Services - System Manager Deploy Python, Java Applications.
●Build servers using AWS Importing volumes, launching EC2, RDS, creating security groups,Auto-scaling, load balancers (ELBs) in the defined virtual private connection.
●Responsible for designing IAM Policies, SCP Policies needed to provide privileges to users making sure it follows the principle of least privilege. Configure AWS Multi-Factor Authentication in IAM to implement 2 step authentication of user's access using Google Authenticator and AWS Virtual MFA.
●Good exposure on DDoS-resilient architectures using AWS WAF and shield Installation of CloudEndure agent on Windows and Linux Servers for data replication in preparation for migration to AWS.
●Notifying Automate cost optimization in cloud infrastructure by using AWS Instance Scheduler, Amazon S3 Intelligent Tiering,Glacier and Auto-scaling by CUR reports.
●Good Experience with DevOps CI/CD tools Jenkins, Docker, Kubernetes, Jira, Git and GitHub. Hands On experience on Cloud Deployments Planning and Implementations.
●Worked on RHEL 8.0 linux distribution servers.
●Good experience with Cloud Logging Services - Route53 Access logs, VPC logs, S3 access logs and CloudTrail to monitor various API Calls.
●Creating solution proposals in line with an organization’s goals around performance, scalability, elasticity, security, management and monitoring and creating these solutions while democratizing advanced technologies.
●Capable of handling multiple tasks and working efficiently both in a group and as an individual to meet deadlines.
●Actively participating in all Agile SCRUM ceremonies - Backlogs grooming, Sprint Planning, Spring Priority sessions with Product, Managers and Team.
●Good experience with migration tools Data sync, Migration hub, EFS, MGN Application migration through source,replicate,test,recovery mode and cutover into end to end migration .

Skill Matrix:

CI/CD Tool
Jenkins, Code commit/artifact/build/deploy/Pipeline,cloud9
Containerization
Docker, ECS, ECR, EKS, Kubernetes,AKS
Configuration Tool
Ansible, AWS Lambda, AWS SSM Parameter Store
Tracking Tool
CloudWatch, CloudTrail, Certificate Manager,EC2 dashboard, Nagios,AWS SNS, AWS SQS, AWS S3,Apptio, Cloudability
Operating Systems
Linux, Unix, Ubuntu and Windows Flavors
Web Servers
Apache Web-Server & Tomcat, NGINX.
AWS Services
Compute, Storage, Database, Networking-Content Delivery, Analytics,Sage-maker,EC2,VPC,Developer,Tools,RedShift,Glue,gluestudio,Datasync,MGN,Migrationhub,EFS
Version Control System
Git with GitHub, Code Commit,Pipeline.
Build Automation Tool
AWS CloudFormation, Terraform.
Languages
Shell Scripting, YAML, Playbooks,Python.
Density-Access Management
AWS IAM, Active Directory, AWS Workspaces, AWS Secrets Manager, AWS SSO, Identity Federation,Jenkins matrix management, Palo Alto, f5
Governance & Compliance
AWS Config Rules, AWS Organization,AWS Control Tower, AWS Trusted Advisor, AWS Well,Architected Tool, AWS Budgets, AWS License Manager, Cloud HSM, Resource Group Tag editor,data lake,Airflow.Dag,Appstream.
Vulnerability Tools
Rapid7 ICS, Qualys, CloudApptio
Visualization Tools
Grafana, Tableau, Kibana, Confluence,XL Graphs.

Project Summary

Client: Pearson Higher Ed, 330 Hudson New York Mar 2022 to Jan 2023
Role: AWS Cloud Data Engineer
Description: More than 140 million users worldwide trust Pearson products and services. Whether it’s upskilling in the workplace, getting ahead in school, making the grade at university, or learning a new language, for pearson products and services as an engineer team we helped pearson teams to realize the spend amount on cloud activities and Potential Savings reports every week
Responsibilities
●Pearson mostly concentrated in cost optimizing and team tracking for higher management VP level.
●Worked with Finops team to track ROI, Spend to help potential savings through XL workbooks, Through Cloudability Apptio Saas AI tool ICS, Flexera FinOps tool.
●Engaged with product management to understand the product requirements and create technical specs as per requirement.
●Generating the Weekly, Bi-Weekly report, Monthly reports to review the potential savings vs actual savings.
●Provided solutions to achieve Tagging compliance by Resource group Tag editor tableau and tagging analysis, Cost Compliance through cost explorer, Vulnerability compliance through Qualys VMDR .
●Design and manage public and private cloud infrastructures using AWS, which includes VPC, EC2, S3, CloudFront, Elastic File, RDS, Direct Connect, Route53, Cloud Watch, Cloud Trail, Cloud gaurd and IAM. Operations were automated using CloudFormation.
●Deploy AWS Infrastructure with IAC using Terraform as well as Cloud Formation on some of the legacy applications
●Target deployments of AWS infrastructure to dev, ga and prod environments using Terraform code through pipelines.
●Worked on cost optimization to generate CUR reports to management.
●Good experience with Qualys VMDR, Rapid7 ICS, Flexera Cloud Apptio maintaining 300+ accounts through Tableau, Grafana, to help higher management to understand compliance score to track the teams weekly basis.
●Tagging compliance using tableau,cost compliance using cur report python scripts,vulnerability compliance Qualys VMDR.
●Develop scripts for AWS orchestration, maintenance, and expansion of AWS infrastructure.
●Branch, tag, and maintain the version across the environments using SCM tools like GIT, Subversion (SVN) and TFS.
●Create Docker images using a Dockerfile and Docker container snapshots, remove images, manage Docker volumes as well as Docker Host..
●Established monitoring for microservice based applications using Grafana and Prometheus,Tableau.
●Implement security measures AWS provides and employ key concepts of IAM.
●Ingest data through AWS Kinesis Data Stream and Firehose from various sources to S3.
●Add support for Amazon AWS. S3 and RDS to host static/media files and the database into Amazon Cloud.

Environment: AWS, VPC, Apptio Cloudability, Flexera, IAM,Azure repo S3, RDS, git, tortoise git, AWS CLI, Qualys, Grafana, Tableau, Rapid7 ICS,VS code, Python3, EC2, EBS, VPC, Subnets, Elastic Cache, Elastic-IP’s, SNS, Amazon Lambda, Resource group Tag editor,Tag policies, Terraform, EKS, ECS, security Hub, guard duty. aws configCloudWatch, Trusted Advisor,Cost Explorer,Control Tower, Landing zone, Organization Sandbox

Client: GE Power, Atlanta, GA Sep 2021 to Mar 2022
Role: Sr. Cloud / DevOps Engineer
Description: General Electric Company (GE) is an American multinational conglomerate founded in 1892, and incorporated in New York state and headquartered in Boston. The company operated in sectors including healthcare, aviation, power, renewable energy, digital industry, additive manufacturing and venture capital and finance,but has since divested from several areas, now primarily consisting of the second, third and fourth segments.
Responsible
●Design and manage public and private cloud infrastructures using AWS, which includes VPC, EC2, S3, CloudFront, Elastic File, RDS, Direct Connect, Route53, Cloud Watch, Cloud Trail, and IAM. Operations were automated using CloudFormation.
●Demonstrated potential cost reductions improvements.
●Vast experience in cloud native platforms such including Kubernetes, docker containers and OpenShift.
●Collaborated with prospective clients to prepare effective product marketing strategies and drive business
●development.
●Increased pipeline by providing presales and post sales to customers.
●Performed high level assessment of functional and technical requirements to the client to address the needs of
●monitoring the health of the overall infrastructure.
●Team Lead managing multiple agile teams, working on so many large-scale projects including a cloud file
●exchange project where we set out to build an application that will ensure the secure transfer of files between
●third party vendors and other applications between our client networks
●Implemented CI/CD pipeline with Docker, Jenkins GitHub and building AWS AMI pipelines for those
●servers.
●Massive work with a strong team of architectures, backend developers to gather function and non-functional
Environment: Linux, Java, SQL, AWS, VMWare, Python Bash, Subversion, Apache Kafka, Cloud Foundry,Shell, ANT, Chef, JIRA, Jenkins, Docker, Ansible.

Client: Comcast Telecommunications,Denver Colorado. Aug 20 to Sep 21
Role: Sr. AWS Cloud Engineer
Description: Comcast Corporation (formerly registered as Comcast Holdings) is an American telecommunications conglomerate headquartered in Philadelphia, Pennsylvania. It is the second-largest broadcasting and cable television company in the world by revenue and the largest pay-TV company, the largest cable TV company and largest home Internet service provider in the United States, and the nation's third-largest home telephone service provider.

Comcast owns and operates the Xfinity residential cable communications subsidiary, Comcast Business, a commercial services provider, Xfinity Mobile, an MVNO of Verizon, over-the-air national broadcast network channels.

Responsibilities:
●Created EC2 server/migrated using ldap and active directory ADDS/SS0 in AWS and Installed NextCloud Open Source file-sharing software like Google Drive on Ec2 Instance and responsible for HA, security, whiteboard and backup
●Configured roles & Policies in the server with Linux administration dependencies
●Configured SSL certificate for the Comcast test server using route53
●Configure apache2 for Comcast, Managing the Comcast EC2 servers.
●Experience in VMWare Disaster Recovery, Backup, and Security Processes.
●Work with the Server Engineering teams to standardize the provisioning and decommissioning process for Windows servers and ESXi hosts
●Experience on configuring storage arrays LUN’s and VMDK volumes for ESXi host and virtual machines
●Well versed in ITIL Processes (Change, Incident, Problem, Service Desk Operations and managements)
●Server H/W setup and Storage Integration for logs storage in S3
●Handled all the aspects of administration tasks such as day-to-day site monitoring and maintenance, Installation, Configuration, Deploying Applications, Troubleshooting, Load Balancing, Performance Tuning and Maintenance of Tomcat
●Installed Application Open Source Nextcloud File Server and Apache (httpd) Web Servers.
●Closely work with other team members in Comcast (Onsite and offshore).
●Backed up and Restored Files and File Systems. CA Arcserve,etc.
●Linux Server Installation and Troubleshooting through putty config conversion tool
●MSSQL/ MySql database support for escalated cases
●Built and maintained VmWare/ RedHat RHEV Server,Packages, Patches and Device Management.

Environment: Elastic Beanstalk,Java, Spring boot, Jenkins, Git, AWS (VPC,EC2,RDS, MySql Trusted Advisor,Cost Explorer), Elastic Cache, Elastic-IP’s, NAT Gateway, SNS, Route 53, Amazon Lambda,,Tag policies, EKS, ECS, security Hub, guard duty.aws config, jenkins,Apache2.

Client:Thrivent Finance, Minnesota Feb 19- Jul 20
Role:Sr Cloud/DevOps Engineer,
Description: Thrivent is a Fortune 500 diversified financial services organization, providing advice, investments, insurance, banking and generosity programs and solutions so people can make the most of all they've been given.
Responsibilities:
●To take care of the script which implements and manages effective service daemons, services automation under root /etc directory for continuous execution and test environment in support of user activities.
●Hands-on experience on shell script. Automated execution by using the loops conceptWriting build scripts using Shell scripting writing a loop for performance tuning the file to run every 5 minutes automatically to make end user work hassle free
●Data Warehouse ETL Job operations and support
●Created User accounts, Roles and granting required access permissions and privileges to the users.
●Teradata support activities, managing data loads and working with production support teams for maintenance schedules
●Extensively used mysql Data pump Exports/Imports utility for moving data among the servers.
●Worked with thrivent team members support groups to resolve production issues and participated on application operations reviews for new application landing on the support model
●Provided additional support for Amazon AWS. S3 and RDS to host static/media files and the database into Amazon Cloud.
Environment: Elastic Beanstalk,Spring boot, Jenkins, Git, AWS (VPC,EC2,RDS, MySql Trusted Advisor,Cost Explorer),AWS CLI, Grafana, Tableau,VS code, EC2, EBS, VPC, Subnets, Route Tables, Security Groups, ELB, Auto scaling, CloudFront, Cloud Formation, Elastic Cache, Elastic-IP’s, NAT Gateway, SNS, Route 53, Amazon Lambda, Resource group Tag editor,Tag policies, EKS, ECS, security Hub, guard duty.aws config,

Client: Mercedes Benz Financial Services, Farmington Hills, MI. Aug’2017-Jan’2019
Role:Cloud-Ops Engineer,
Description: Mercedes-Benz Financial Services provides finance, lease and insurance services for the dealers and customers of Mercedes-Benz cars and vans.

●Played a key role in migrating the existing on-premises applications to AWS VPC.
●Responsible for Well-Architected Application on AWS using Auto Scaling, Lambda, SQS, SNS, ELB, Caching and database layer, as necessary.
●AWS resource tagging to identify and categorize resources by function, environment, platform criteria.
●Created AWS Multi-Factor Authentication (MFA) for instance RDP/SSH login, worked with teams to lock down security groups.
●AWS data backup (snapshot, AMI creation) techniques, along with data-at-rest security within AWS.
●Created a Lambda Deployment function and configured it to receive events from your S3 bucket.
●Developed API for using AWS Lambda to manage the servers and run the code in the AWS.
●Setted Up of Cloud Watch alarms, setting up CloudTrail, creating cloud formation templates, creating S3 buckets
●Review fixes the security loopholes and applies security at all layers following the AWS's Trusted Advisor.
●Created CloudWatch alarms that take care of EC2 Auto Recovery and Auto Reboot for instances that fail System Status Check and Instance Status Check.
●Deploy VM's using Hashicorp Terraform. Build Docker hosts using Terraform for deployments.
●Troubleshoot and fix Hashicorp Terraform code which is Hashicorp Terraform underneath.
●Experience in using Jenkins to build pipelines to drive all microservice builds out to the Docker registry and then deployed to Kubernetes, Created Pods and managed using Kubernetes.
●Involved in development of Test environment on Docker containers and configuring the Docker containers using Kubernetes.
●Worked on JIRA for defect/issues logging & tracking and documented all my work using ONE-CONFLUENCE.
●Integrated services like GitHub, AWS CodePipeline, Jenkins, and AWS Elastic Beanstalk to create a deployment pipeline.
●Good Experience in architecting and configuring secure cloud VPC using private and public networks through subnets in AWS.
●Amazon IAM service enabled to grant permissions and resources to users. Managed roles and permissions of users with the help of AWS IAM.
●Worked on user administration setup, maintaining account, monitoring system performance using Nagios and Tivoli.
●Patched Nagios to be able to multiple active checks with nrpe, to allow for up to 10 host and 100 k services from one Linux machine.
Environment: AWS, VPC, IAM, S3, RDS, git, tortoise git, AWS CLI, Qualys, Grafana, Tableau, Rapid7 ICS,VS code,Azure Migration service, blob storage, jenkins,Apache2, CloudWatch, Trusted Advisor,Cost Explorer.

Client:cure.fit (cult.fit) Bangalore,Karnataka Jan 16 to July 17
Role:AWS-DevOps Engineer
Description: Cult.fit (formerly cure. fit or Curefit) is a health and fitness company offering digital and offline experiences across fitness, nutrition, and mental well-being. With the aim to make fitness fun and easy, cult. fit gives workouts a whole new meaning with a range of trainer-led, group workout classes.
Responsibilities:
●Build the entire CI/CD pipeline build, test and Deploy using Git and Jenkins through upstream and downstream projects, set-up the SNS service to get continuous updates when pipeline made changes. Set-up the Elastic beanstalk for better infra visibility for end user
●Implement and manage effective build automation, continuous integration and test environment in support of Software development life cycle activities
●Worked on spinning different EC2 instances required for running servers with zero downtime by implementing autoscaling, load balancers, Web Servers and AWS Marketplace servers for Dev, non-prod and Production environments
●Worked on Creating Identity Access Management policies (IAM) for user identities.
●Configured Elastic Load Balancer with Autoscaling groups to route traffic between
multiple EC2 instances. Configured multiple VPCs with private and public subnets
●Worked on creating S3 bucket policies (ACLs, User based policies and Bucket policies)
●Setup S3 lifecycle policies to transition data between different storage tires and used S3 versioning and cross region replication
●Configured AWS CloudTrail logs and Configured S3 buckets with required policies to secure CloudTrail logs
●Worked on setting up VPC peering between VPCs present in same region, different
regions and VPCs present in different AWS accounts
●Used Trusted Advisor, Cost Explorer for Resource Cost Optimization, analyzing security gaps and to Identify opportunities to improve infrastructure performance tuning
●Settedup Budgets on Billing dashboard to keep tab on cost incurred by different tags and different accounts and Used Cost Allocation Tags and Cost Explorer for reviewing costs and usage pattern
●Worked with custom AMIs, added tags and modified AMI permissions
Environment: AWS, VPC, IAM, S3, RDS, git, tortoise git, AWS CLI, Python3, EC2, EBS, VPC, Subnets, Route Tables, guard duty. aws config, Azure DevOps, Azure Migration service, blob storage, jenkins,Apache2.
Contact this candidate