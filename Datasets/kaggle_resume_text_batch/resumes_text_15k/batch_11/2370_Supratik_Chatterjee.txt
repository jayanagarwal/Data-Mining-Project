




































Supratik Chatterjee+91 8097030306
kitorpus@gmail.com
Career Objective
Seeking a position in your organization to enhance and utilize my technical and communication skills and ability that offer professional growth while being resourceful and innovative.
Experience Summary
8 years 7 months of experience in developing java and big data based applications spanning all the aspects of software development life cycle phases including analysis, design, coding, testing, implementation and maintenance. Mainly involved in Analyzing use cases/requirements from different Clients providing Solutions, Implementing and Developing Big Data Platforms on Majorly Cloudera Distributions. Having around 4.5 Years of experience in Java/J2EE application development including agile methodology. 4.2 years of experience in Big Data Technologies including Hadoop Ecosystem, Kafka, Storm, Cassandra.
Technical Skills
· Technologies: Java, BigData
· Languages: Core Java, J2EE, Hadoop,Scala,Spark
· Web Development: HTML, JSP, JavaScript, CSS
· Framework:  Spring Web MVC 3.6
· Web Services: Apache CXF, SOAP UI
· Database: MySQL, Hive, Cassandra
· Application Servers:JBoss
· Hadoop Components: Oozie, Pig, Sqoop, Coordinator,Kafka
· Build Tools: Maven,SBT
· Continuous Integration: Jenkins,
· Version Control: Tortoise SVN, Git
· Virtual Machine: Vagrant, Sandbox
· Operating System: Windows, Linux
· Software Development Methodologies: Agile, Water Fall Model
Experience
Accenture 
Lloyds Bank(Spark,kafka,Sqoop)
December 2017-July 2018. Pune
· Implemented Kafka Consumer for different topics.

· Created sqoop framework to fetch data from other relational data source and store them in HDFS.

· Created Spark job for data Consumption and Transformation data as per requirement.
Manulife Financial Corp (HIVE,OOZIE)
April 2017-2017December. Pune
· Migration of all the clients’ data transformations, and by implication the underlying data, from its EDW to a Hadoop cluster. Builds an EDL (Enterprise Data Lake) where the EDW provides robust reporting services on aggregated data imported from a Hadoop cluster that also provides advanced analytics on data transformed within the cluster -- all in a single abstracted view of business data to our client’s business units.

· Working on the data processing by taking the data from HDFS raw zone and populate them in Hive tables at (Curated and Dal layer)to be ready for analytics using the Hadoop stack and scheduling everything through Oozie Jobs.

· Created oozie workflows to generate and do processing in Hive tables and finally export the data at edge node using Ssh workflow scripts.

· Worked on Partitions, Buckets in Hive and designed both Managed and External tables in Hive to optimize performance.

· Participate in peer-reviews of solution designs, related code and development.

· End to End handling of Projects as well as application requests.
Sanofi (HIVE,OOZIE,SQOOP)
July 2016-April 2017. Pune
· Implemented Hadoop architecture as per the project requirement.
· Created Workflows with sqoop to fetch the data from other API’s and store them into HDFS
· Design the HDFS folder structure according to Informatica and Tibco requirement.

· Created Sqoop job to load and unload data to HDFS to external Oracle database.

· Used Oozie to automate the execution.

· Used Hive queries to transform the data inside HDFS with the help of UDF.
BestBuy (HIVE,OOZIE,PIG,KAFKA,STORM,MICROSERVICES,CASSANDRA)
Jun 2014 –July 2016 MUMBAI

· Leverage Hadoop components to convert raw unstructured data into structured information which can be used for analysis

· Developed Oozie workflows and Falcon processes which convert the raw data into parseable information using Cascading

· Enabled real time streaming of user clicks information using Storm and Kafka to generate faster recommendations

· Developed a framework for Data Tagging which will help to track any change that has occurred in the data, who is responsible for the change, and at what stage of processing the data did that change occur

· Developed a number of micro services using RxJava and Netflix Karyon (web container) which store / retrieve data from Cassandra DB.

· Wrote complex queries in Hive to fetch data from multiple tables to create result sets as per requirements.

· Wrote pig scripts to load and transform data.

· Used Sqoop scripts to import and export data to and from external database.
BestBuy (Portal Migration)
JAN 2014 – JUN 2014 MUMBAI
· Migrated applications from WebSphere to JBoss EAP to reduce infrastructure costs

· Acquired knowledge of both WebSphere and JBoss EAP

· Converted legacy code written on Spring Portlets architecture to Spring Web MVC without breaking functionality.

· Designed and developed a Web UI from scratch for customers to raise an online request to replace products purchased from BestBuy.

· Created a generic Java API to consume web services using Apache CXF to reduce effort
UPS (Core java and support)
June 2012-Jan 2014. Bangalore

· UPS is a leading player in the US market to provide logistics and freight movement from one location to another. 

· Worked on the Online Brokerage Platform solution, which provides a end to end delivery model for clients.

· Developed the Password module using J2EE, which performs several checks on the previous passwords and sends an auto generated password to user mail and prompt the user to change the password.

· Handled the support as well which helped in understanding of the business domain also resolved tickets in timely manner.

· Interacted with BA when a new SCR is provided.

JPMorgan (Unit Testing, UI development)
April 2010-May 2012. Mumbai

· The ATS Inquiry Management Service (IMS) will enable the entire inquiry management workflow, including initiation, approval, communication, and resolution of inquiries

· These inquiries may be initiated by external ATS customers or 3rd parties (system generated inquiries). It is an engine, through which there are multi ways to create a Service Request message, edit a Service Request, Approve or Reject a Service Request, and Submit the Service Request for processing.

· IMS also provides capability for Searching and Viewing Service Requests and Transactions. 

· Developed UI widgets with DOJO. Created Junit test class.

· Design the UI as per the document.

· System testing and integrated testing for ATS Payments application

Education
Bachelor of Science (Electronics)

· AGC College of Arts, Science and Commerce (Pune University)

JUNE 2003 - July 2006,

PUNE

· MCA (Master in Computer Applications)

IMCC(Pune University)

August 2006 - July 2006,

PUNE
Personal Details
· Date of Birth : 3rd  January, 1986
· Known Languages: English, Hindi, Bengali, Marathi.

