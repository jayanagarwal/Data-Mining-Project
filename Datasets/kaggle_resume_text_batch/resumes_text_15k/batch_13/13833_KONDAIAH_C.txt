






































CURRICULUM VIATE

 KONDAIAH C
Mobile: +91-9059275150
Email:kondaiahchennaboina@gmail.com
Experience Summary:

           Having 3+ years of experience as Java Developer and Hadoop Developer,
· Experienced in Hadoop (HDFS, Map Reduce) and Hadoop Ecosystem (Hive, Sqoop, Pig, Oozie) and Spark.
· Good understanding and Hands on experience on Core Java.
· Very good understanding/knowledge of Spark, Hadoop Architecture and various components such as HDFS, Map Reduce and Spark Core, Spark SQL.
· Experience in importing and exporting the data using SQOOP from HDFS to Relational Database systems and vice-versa.
· Experience in Writing HIVE queries, very good understanding of partitions and bucketing concepts in hive and designed both Managed and External tables in hive for optimized performance
· Exposure to relational databases like SQL and MySQL.
· Efficient in building Spark, Scala and map Reduce scripts.
· Knowledge on Kafka and Spark Streaming.
· Team Player with Good communication and interpersonal skills.

· Involved in debugging, troubleshooting and ensuring fast, smooth functionality of critical applications.

Professional Experience:
· Currently working as Software Engineer in Cyient Limited from Feb-2015 to till date.

Technical Skills:
Big-Data 

:  
Hadoop, Spark, HDFS, Hive, Pig, Sqoop, HBase, Oozie, Kafka.
Hadoop Distribution
:  
Cloudera

Programming Languages    :
Core Java, Scala

Databases

:
MySQL, Oracle 10g

Tools
                                : 
 WinScp, Putty, Intellij, Eclipse, Zeppelin, Hue.
Educational Profile:

· B.Tech (CSE) from KSRM College of Engineering, JNTU Anantapur in 2014. 
· Diploma in Computer Engineering from State Board of Technical Education & Training, AP in 2011.
Project Details:

Project #1:

Title

:   
Emars

Role   

:   
Hadoop Developer

Tools 

: 
HDFS , Hive, Sqoop, Spark, Scala, Oracle, Oozie.
Team Size
: 
5

Period

:   
Jan 2018-Till now.

Description:

Emars application is used for providing the credit and debit advices from multiple sources to the down streams or clients .It is also used for having a track of historical billing and remittance advices history to analyse the trend of the customer in banking .Based on the historical data the information like the investments the investments and other details are understood by the client .There are several types of inputs from different up streams. The data is analyzed and provided in a meaningful format for the down streams .The reports are shown generated by Tableau team. 

Roles and Responsibilities:
· Perform big data processing using Hadoop, Sqoop, Hive and , Spark Rdds and SparkSQL. 
· Extracted data from Oracle Databases to HDFS using sqoop.
· Implementing Hive tables to store the processed results in tabular format and written Hive Scripts to transform and aggregate.
· Experience in using Avro, Parquet and .CSV  file formats.
· Writing the script files for processing data and loading to HDFS.
· Analyze the different types of transaction (Debit/Credit/Failure) in monthly or yearly basis. And also to process the historic data.

· Coordinating with team members for system design, integration, application maintenance.
Project #2:

Title
 
: 
Macy’s – Analytics .

Client

: 
Macy’s
Role   

: 
Hadoop Developer

Team Size
: 
12
Software
: 
Hdfs, Sqoop, Hive, Pig, Oozie, Java, Mysql .

Duration
:
 May 2016 to Dec 2017

Description:

Macy’s is one of the leading retail customer based out of USA. Initially Target retailer is been competing with four retailers are Amazon, Lowes, Dillard’s. That means, Lowes customer is making the price level comparison of the products with competitor retailers. To extract the details, target retailer is "crawling" the competitor data to Lowes local database (mysql) known as "web crawling" activity.
Responsibilities:

· Implement Sqoop scripts to ingest the data from the RDBMS to load into HDFS.

· Replaced default Derby metadata storage system for Hive with MySQL system.

· Creating Hive tables and working on them using Hive QL 

· Implement Map-Reduce jobs to validate and analyze data and Executes queries using Hive.

Project #3:

Title

:   
SPARS

Tools 

: 
Java/j2ee, JDBC, Oracle 10g, Eclipse

Team size
: 
6

Role   

:   
Java Developer
Period

:  
 Feb 2015-Apri 2016.
Description:


SPARS application is used for tracking and providing access to various business entities in organization. The Visitors from overseas or the current employees of the particular business location can request for access for a particular entity in a business division .The Request once raised goes through the approval process before proving the access. This application helps both in raising and approving the requests

Roles and Responsibilities:
· Resolved application and connectivity problems in Java programs.

· Major contribution in the development of Access request module.

· Unit tested and documented website applications and code.

·     Worked with SFTP connectivity for fetching the employees details in generating the reports  for  the client 
Declaration:

                       I hereby declare that the information furnished above is true to the best of my knowledge.

Place:
Bangalore 


                                       


 

 (Kondaiah C)
