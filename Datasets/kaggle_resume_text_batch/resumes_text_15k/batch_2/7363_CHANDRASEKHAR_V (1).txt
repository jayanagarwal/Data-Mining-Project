

















































CHANDRASEKHAR V                                                                                             
    Email:51424809.sekhar@gmail.com
    Mobile No: +91-9845277655
PROFILE
· Having Overall 6 years of IT experience.
· Having 4 years of experience in Java/J2EE and 2 years of experience Big Data technologies, Hadoop eco-system, Apache Spark & Scala. 
· Expertise with the tools in Big Data Ecosystem including HDFS, MapReduce, Hive,  Sqoop, Hbase, Spark, Scala, Oozie and Yarn.
· I Have 4 Years of application/product development experience in Java and J2EE technologies like Core java, JSP, Servlets, Struts,Spring,Hibernate, Java Script and Oracle.
· Familiar with various Life Cycle Models like Waterfall & Agile Methodologies
· Experience in migrating the data using Sqoop from HDFS to Relational Database System and vice-versa per client's requirement
· Hands on experience in using configuration management tools like GIT and SVN
· Ability to perform at a high level, meet deadlines, adaptable to ever changing priorities
· Good communication, interpersonal, analytical skills, and strong ability to perform as part of team
· Hard working and ready to learn new concepts
PROFESSIONAL EXPERIENCE
· Currently working as a Lead engineer in HCL Technologies, Chennai.
TECHNICAL DETAILS
	Big Data/Hadoop Technologies
	MapReduce, HDFS, Hive, Yarn, Sqoop, Spark, Scala, Oozie.

	NO SQL Databases
	Hbase

	Languages / Environment
	Core Java, Jdbc, Jsp,  Scala, SQL,  Hive QL 

	Databases
	ORACLE11g&MYSQL

	Tools & Utilities
	Eclipse, Putty, Filezilla & Vmware

	Configuration Management Tools
	GIT and SVN

	Operating Systems
	Windows, UBUNTU & CENTOS


ACADEMIC QUALIFICATION
· MCA (Master of Computer Application) from Andhra University.
PROJECT DETAILS
Project#1:
	Project Name:
	PUMP RETAIL  AND PUMP FAULT PREDICTION

	Client
	Grundfos, Denmark

	Role
	Hadoop Developer

	Duration
	May 2017 to till date

	Team Size
	7

	Technologies
	CDH 5.10.0, HDFS, Spark , Scala, Oracle 11g, HIVE, SQOOP

	Environment
	Linux


Project Description: 
     

This is migration project. Purpose of this project is to store terabytes of structure data and transactional log information generated by the system. These log files are produced on weekly basis. These log files and structure data have to be parsed by set of rules defined in the various formats. Initially these were loaded into the database and retrieving these rules is time consuming activity. With the solution based on the open source BigData (Hadoop), we reduced the time for the whole process. Data will be stored in Hadoop distributed file system and processed using Scala and Spark. Which intern includes getting the data from the websites, process the files to obtain the analysed information from all the logs, Extract various reports out of this information and Export the information for further processing. This will help to meet the client requirement of revenue increase by delivering comprehensive data, advanced analytics.
Responsibilities: 
· Analysing requirements document
· Moved structure data into HDFS location using Sqoop.

· Responsible for building scalable distributed data solutions using Hadoop

· Preparing unit test cases 
· Worked with GIT (Source code version con).

· Worked with JIRA Tracker tool.
· Participating regular Scrum meetings held by Scrum Manager.
· Interacting with clients.
Project#2:
	Project Name:
	Consumer Data Analytics Product Data Intelligence For Grundfos

	Client
	Grundfos, Denmark.

	Role
	Hadoop Developer

	Duration
	Jan 2016- Apr 2017

	Team Size
	5

	Technologies
	CDH 5.8.0, HDFS, Map reduce, Core Java, HIVE, SQOOP

	Environment
	Linux


Project Description: 
                                           Purpose of this project is to store terabytes of transactional log information generated by the system. These log files are produced on weekly basis. These log files have to be parsed by set of rules defined in the various formats. Initially these were loaded into the database and retrieving these rules is time consuming activity. With the solution based on the open source BigData (Hadoop), we reduced the time for the whole process. Data will be stored in Hadoop distributed file system and processed using Map reduce. Finally analyzed data would store in Hive external table. 
Responsibilities:
· Analysing requirements document
· Moved all log files generated by various devices and websites into HDFS location
· Written the MR programs to process the HDFS data.
· Setup Hive with MySQL as a Remote Metastore
· Created required external hive tables and load the data into it
· Implemented Junit test cases to cover 100% code coverage
· Involved in releasing process to Integration and QA
Project#3:
	Project Name
	Grundfos  Product Software Upgrade

	Client
	Grundfos, Denmark.

	Role
	 Java Developer

	Duration
	Aug 2014 to Dec 2015

	Team Size
	5

	Technologies
	Core Java, Jsp, Rest Api, Mysql, Tomcat, HTML, CSS, JavaScript, JQuery.

	IDE
	Eclipse

	Environment
	Linux


Project Description: 
                Grundfos is a pump manufacture company in Denmark. This project is used by Grundfos service engineers. Grundfos provides different kind of products. When Grundfos pump customer raise product software upgrade request. Then Grundfos service engineer’s takes laptop and upgrade the product. 
Responsibilities:
· Understanding the requirements.
· Coding and Implementation through Design pattern.
· Implemented Junit test cases.

· Worked with GIT (Source code version con).

· Worked with JIRA Tracker tool.
· Participating regular Scrum meetings.
· Interacting with clients.
Project#4:
	Project Name
	Grundfos Products

	Client
	Grundfos, Denmark.

	Role
	 Java Developer

	Duration
	July 2012 to July 2014

	Team Size
	10

	Technologies
	Core Java, Jsp, Rest Api, Mysql, Tomcat, HTML, CSS, JavaScript, JQuery.

	IDE
	Eclipse

	Environment
	Windows


Project Description: 
                Grundfos is a pump manufacture company in Denmark. Grundfos provides different kind of pumps like Drinking Water Distribution, Industrial products, Heating, Air-conditioning, Wastewater, Motors… Worked on Product selection it contains below product selection.

Product selection:

  -Search by name or number, -Replacement suggestions, - Pump sizing, -Complete product catalogue
Product information:
- Specifications, - Service parts
Responsibilities:
· Understanding the requirements.
· Coding and Implementation through Design pattern.
· Implemented Junit test cases.

· Worked with GIT (Source code version con).

· Worked with JIRA Tracker tool.
· Participating regular Scrum meetings.
· Interacting with clients.

