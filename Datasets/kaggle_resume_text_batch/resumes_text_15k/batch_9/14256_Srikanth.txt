







































Srikanth  
                    Mobile: +91 - 8142627300 
                                                                                   

        Email: srikanth021988@gmail.com

Professional Summary

· Having 4.9 years of experience, out of which 2.8 years’ experience in Hadoop & Spark development and 2.1 years of experience in Oracle & Core Java & Php, Html5.

· In depth understanding/knowledge of Hadoop Architecture and various components such as HDFS and Map Reduce concepts.
· Experience in working with Map Reduce programs using Hadoop for working with Big Data to analyze large data sets efficiently

· Good experience creating real time data streaming solutions using Spark Core, Spark SQL & Data Frames, and Spark Streaming.

· Hands on experience with Big Data core components and Ecosystem (Spark, Spark SQL, Hadoop, HDFS, MapReduce, YARN, Zookeeper, Hive, Pig, Sqoop, Flume, Oozie, hbase).

· Experience in importing and exporting data using Sqoop from HDFS to Relational DataBase Systems (RDBMS) and from RDBMS to HDFS.

· Hands on experience in writing pig Latin scripts and pig commands.

· Good knowledge of No-SQL databases- HBASE.

· Wrote custom UDF’s for extending Hive and Pig core functionality

· Development experience with IDE’s Eclipse and NetBeans.

· Experience in Cloudera & Horton Works Distributions.

·    Extensive experience in working with different databases such as Oracle, RDBMS, MySQL.

· Ability to adapt to evolving technology strong sense of responsibility and accomplishment.

· Experience in Integrating Hive With Hbase & Pig. 

· Excellent problem solving skills with a strong technical background and result oriented team player with excellent communication and interpersonal skills
Technical Skills:

Big data Skills

:
Spark, Hadoop, HDFS, Map Reduce, YARN, Zookeeper, Hive, Base,

Pig, Sqoop, Flume, Oozie.

No Sql DataBases

:
Hbase

Databases


:
Mysql, Oracle

Languages


:
Python, Scala, Core Java

Scripting Languages
:
HTML5, CSS3, Bootstrap, JavaScript, Php.

Web Application Server
:
Apache Tomcat.

IDE’s


:
Eclipse

Platforms

            :           Ubuntu, Centos, Windows

Educational Background

· MCA from Kakatiya University with 65%.

· B.S.C from Kakatiya University with 62%.

Working Experience
· Currently working as a Bigdata Engineer in Capgemini , Bangalore. From March 2018 to Till Date.
· Previously Worked as a Hadoop Developer in Infosytech Solutions Inc. From December 2015 to February 2018.

·   Previously Worked as a Web Deveploer in Brio Factors Technologies Pvt Limited Hyd    from November 2013 to December 2015.

Project Summary:

Project#1:
Title                   :  LF AB
Period               :   March 2018–Till Date  
Team size         :   6
Technologies    : Hadoop , Hdfs, Hive, Sqoop, Spark sql, spark core, Flume, Oracle, Pig.
Role                  :
Hadoop developer
Description      :  LFAB (Länsförsäkringar), or literally County Insurance, is a Swedish group of customer owned insurance companies.  The purpose of the project is to store large amount  data generated by the LFAB website and to store in fast processing frame work. The solution chosen for this Hadoop .The data will be stored in Hadoop  file system and processed using Hive which includes getting the large amount of  data from the website.  The company's task is to offer total solutions based on different combinations of non-life insurance, accident and medical insurance, life assurance, pensionsaving plans, fund savings and various banking services. extract various reports  of the product, and analyze the demand of the product  for business development.

Responsibilities:      
· Responsible for building scalable distributed data solutions using Hadoop.

· Loading data from File System into a Spark RDD. 

· Developed Spark SQL statements for processing data.

· Imported data using Sqoop to load data from Oracle to HDFS on regular basis from various sources. 

· Written multiple Map Reduce programs to power data for extraction, transformation and aggregation from multiple file formats including   JSON, CSV other compressed file formats.

· Reviewed the HDFS usage and system design for future scalability and fault-tolerance.

· Involved in loading data from LINUX file system to HDFS.

· Loaded and transformed large sets of structured, semi structured and unstructured data in various formats like text, XML and JSON. 

· Optimized Map/Reduce Jobs to use HDFS efficiently by using various compression mechanisms. 

· Developed Hive  UDFs for manipulating the data according to Business Requirements 

· Responsible for creating Hive tables based on business requirements 

· Implemented Partitioning, Dynamic Partitions and Buckets in HIVE for efficient data access.

· Loaded data into NoSQL database HBase. 

· Exported the Analyzed data into relational databases using Sqoop for visualization and to generate reports for the BI team. 

Project#2:
Title                   :  Takealot
Period               :   June 2016–January 2018.
Team size         :   6
Technologies   : Hadoop ,Hdfs, MapReduce ,Hive, Sqoop, Spark sql, spark core, Flume, MySQL, Pig.
Role                  :
Hadoop developer
Description      :   Takealot is one of the Leading Online shopping Portal from South Africa. The purpose of the project is to store large amount of log information data generated by the ecommerce website and to store in fast processing frame work. The solution chosen for this Hadoop .The data will be stored in Hadoop  file system and processed using Hive which includes getting the large amount of  data from the website, process to obtain product and country information, extract various reports  of the product, and analyze the demand of the product  for business development.

Responsibilities:      
· Responsible for building scalable distributed data solutions using Hadoop.

· Loading data from File System into a Spark RDD. 

· Applying transformations to the data into a Pair RDD.

· Developed Spark SQL statements for processing data.

· Imported data using Sqoop to load data from MySQL to HDFS on regular basis from various sources. 

· Written multiple Map Reduce programs to power data for extraction, transformation and aggregation from multiple file formats including XML, JSON, CSV other compressed file formats.

· Reviewed the HDFS usage and system design for future scalability and fault-tolerance.

· Involved in loading data from LINUX file system to HDFS.

· Loaded and transformed large sets of structured, semi structured and unstructured data in various formats like text, XML and JSON. 

· Defined job flows and developed simple to complex Map Reduce jobs as per the requirement. 

· Optimized Map/Reduce Jobs to use HDFS efficiently by using various compression mechanisms. 

· Developed Hive  UDFs for manipulating the data according to Business Requirements 

· Responsible for creating Hive tables based on business requirements 

· Implemented Partitioning, Dynamic Partitions and Buckets in HIVE for efficient data access.

· Loaded data into NoSQL database HBase. 

· Exported the Analyzed data into relational databases using Sqoop for visualization and to generate reports for the BI team. 

Project#3:
Project Name
:  Case Management System
Period                 :  January 2015 –June 2016.

Team size           :  6
Role                
:  Hadoop Developer
Environment 
: Hadoop (HDFS, Map Reduce), Pig, Hive, Sqoop, Hbase, Flume.
Description:

The Case Management System is a shared database of all our client information and consists of four models

Marketing , Lead Generation, Accounts, HR modules. CMS allows us to work remotely with all documents & case data integrated into a single System giving to several significant advantages over doing things the old fashioned way. The CMS allows all users on a network to share the same information at the same time. The application which is currently running on MySQL system needs to be migrated to Big Data Environment.
Roles & Responsibilities:

· Analyzing the requirements and preparing for best solutions.

·  Involved in importing data from RDBMS to HDFS using Sqoop.

·  Using Pig to perform data Validation on the data ingested using Sqoop

·  Data stored on HDFS and processed by using various tools in Hadoop.

·  Using Hive focused on finding frequency of patients joining in particular season.

·  Involved in creating Udf for business logic.

· Utilizing pig on this data-set for further insights.
