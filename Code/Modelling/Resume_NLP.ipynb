{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7734646-a77d-4a4e-93f4-c8e9bebe8f01",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the English model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resume_parser_env\\lib\\site-packages\\spacy\\__init__.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.ufunc size changed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneural\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prefer_gpu, require_gpu\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcli\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m info \u001b[38;5;28;01mas\u001b[39;00m cli_info\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resume_parser_env\\lib\\site-packages\\thinc\\neural\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# coding: utf8\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unicode_literals\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_classes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resume_parser_env\\lib\\site-packages\\thinc\\neural\\_classes\\model.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyOps, CupyOps\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Memory\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resume_parser_env\\lib\\site-packages\\thinc\\neural\\train.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam, linear_decay\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTrainer\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcfg):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resume_parser_env\\lib\\site-packages\\thinc\\neural\\optimizers.pyx:14\u001b[0m, in \u001b[0;36minit thinc.neural.optimizers\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\resume_parser_env\\lib\\site-packages\\thinc\\neural\\ops.pyx:1\u001b[0m, in \u001b[0;36minit thinc.neural.ops\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"spaCy and the English model are successfully installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620d9312-816c-49c2-92ea-bb6cd1078965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7217a7b1-4a8e-4e2a-985f-38593f44eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_sections(text):\n",
    "    # Initialize a dictionary to hold extracted information\n",
    "    sections = {\n",
    "        'Name': '',\n",
    "        'Contact': '',\n",
    "        'Professional Summary': '',\n",
    "        'Skills': [],\n",
    "        'Education': [],\n",
    "        'Experience': [],\n",
    "        'Certifications': [],\n",
    "        'Accomplishments': [],\n",
    "        'Achievements': [],\n",
    "        'Hobbies': [],\n",
    "        'Languages': [],\n",
    "        'LinkedIn': '',\n",
    "        'GitHub': ''\n",
    "    }\n",
    "    \n",
    "    # Ensure text is a string to avoid issues with NaN\n",
    "    if not isinstance(text, str):\n",
    "        return sections  # Return empty sections if the text is invalid\n",
    "    \n",
    "    # Split text into paragraphs\n",
    "    paragraphs = text.split('\\n')\n",
    "\n",
    "    # Define keywords for sections (case-insensitive)\n",
    "    name_keywords = ['Name']\n",
    "    contact_keywords = ['Phone', 'Email', 'Contact']\n",
    "    summary_keywords = ['PROFESSIONAL SUMMARY', 'Summary']\n",
    "    skills_keywords = ['SKILLS', 'Technical Skills']\n",
    "    education_keywords = ['EDUCATION', 'Education', 'Degrees']\n",
    "    experience_keywords = ['PROFESSIONAL EXPERIENCE', 'EXPERIENCE']\n",
    "    certifications_keywords = ['CERTIFICATES', 'CERTIFICATIONS']\n",
    "    accomplishments_keywords = ['ACCOMPLISHMENTS', 'Achievements']\n",
    "    hobbies_keywords = ['HOBBIES', 'Interests']\n",
    "    languages_keywords = ['LANGUAGES', 'Language']\n",
    "    linkedin_keywords = ['LinkedIn', 'linkedin.com']\n",
    "    github_keywords = ['GitHub', 'github.com']\n",
    "\n",
    "    current_section = None\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        paragraph = paragraph.strip()  # Clean up whitespace\n",
    "        \n",
    "        # Skip empty lines\n",
    "        if not paragraph:\n",
    "            continue\n",
    "        \n",
    "        # Extract LinkedIn and GitHub links directly\n",
    "        if any(kw.lower() in paragraph.lower() for kw in linkedin_keywords):\n",
    "            sections['LinkedIn'] = paragraph\n",
    "            continue\n",
    "        if any(kw.lower() in paragraph.lower() for kw in github_keywords):\n",
    "            sections['GitHub'] = paragraph\n",
    "            continue\n",
    "\n",
    "        # Extract Name and Contact information\n",
    "        if any(kw.lower() in paragraph.lower() for kw in name_keywords) or re.match(r'\\b[A-Za-z]+\\s[A-Za-z]+', paragraph):\n",
    "            sections['Name'] = paragraph\n",
    "            continue\n",
    "        if any(kw.lower() in paragraph.lower() for kw in contact_keywords) or re.search(r'\\b\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b', paragraph):\n",
    "            sections['Contact'] = paragraph\n",
    "            continue\n",
    "        \n",
    "        # Check if paragraph starts a new section\n",
    "        if any(kw.lower() in paragraph.lower() for kw in summary_keywords):\n",
    "            current_section = 'Professional Summary'\n",
    "            continue\n",
    "        elif any(kw.lower() in paragraph.lower() for kw in skills_keywords):\n",
    "            current_section = 'Skills'\n",
    "            continue\n",
    "        elif any(kw.lower() in paragraph.lower() for kw in education_keywords):\n",
    "            current_section = 'Education'\n",
    "            continue\n",
    "        elif any(kw.lower() in paragraph.lower() for kw in experience_keywords):\n",
    "            current_section = 'Experience'\n",
    "            continue\n",
    "        elif any(kw.lower() in paragraph.lower() for kw in certifications_keywords):\n",
    "            current_section = 'Certifications'\n",
    "            continue\n",
    "        elif any(kw.lower() in paragraph.lower() for kw in accomplishments_keywords):\n",
    "            current_section = 'Accomplishments'\n",
    "            continue\n",
    "        elif any(kw.lower() in paragraph.lower() for kw in hobbies_keywords):\n",
    "            current_section = 'Hobbies'\n",
    "            continue\n",
    "        elif any(kw.lower() in paragraph.lower() for kw in languages_keywords):\n",
    "            current_section = 'Languages'\n",
    "            continue\n",
    "\n",
    "        # Extract Professional Summary from the first few lines (before any section headers)\n",
    "        if current_section == 'Professional Summary' and len(paragraph) > 0:\n",
    "            sections['Professional Summary'] += paragraph + ' '\n",
    "        \n",
    "        # Add content to the current section\n",
    "        if current_section == 'Skills':\n",
    "            sections['Skills'].append(paragraph)\n",
    "        elif current_section == 'Education':\n",
    "            sections['Education'].append(paragraph)\n",
    "        elif current_section == 'Experience':\n",
    "            sections['Experience'].append(paragraph)\n",
    "        elif current_section == 'Certifications':\n",
    "            sections['Certifications'].append(paragraph)\n",
    "        elif current_section == 'Accomplishments':\n",
    "            sections['Accomplishments'].append(paragraph)\n",
    "        elif current_section == 'Hobbies':\n",
    "            sections['Hobbies'].append(paragraph)\n",
    "        elif current_section == 'Languages':\n",
    "            sections['Languages'].append(paragraph)\n",
    "    \n",
    "    # Ensure 'N/A' if any section is empty\n",
    "    for key in sections.keys():\n",
    "        if key in ['Name', 'Contact', 'LinkedIn', 'GitHub', 'Professional Summary'] and not sections[key]:\n",
    "            sections[key] = 'N/A'\n",
    "        elif not sections[key]:\n",
    "            sections[key] = ['N/A']\n",
    "\n",
    "    return sections\n",
    "\n",
    "# Read CSV file\n",
    "df = pd.read_csv('resumes_data.csv')  # Replace with your CSV file path\n",
    "\n",
    "# Handle NaN values in the 'Resume' column\n",
    "# df['Resume'].fillna('', inplace=True) # This is deprecated\n",
    "df['Resume'] = df['Resume'].fillna('')\n",
    "\n",
    "resume_data = df['Resume'].tolist()\n",
    "\n",
    "# Process each resume and extract sections\n",
    "extracted_data = []\n",
    "for text in resume_data:\n",
    "    extracted_data.append(extract_sections(text))\n",
    "\n",
    "# Create a DataFrame to store structured data\n",
    "structured_df = pd.DataFrame(extracted_data)\n",
    "\n",
    "# Save the structured data to a new CSV file\n",
    "structured_df.to_csv('structured_resumes.csv', index=False)\n",
    "print(\"Structured data has been extracted and saved to 'structured_resumes.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a6ab60-f3fd-4deb-8b13-373b017943fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
