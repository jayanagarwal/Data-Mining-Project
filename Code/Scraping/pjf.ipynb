{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time, os\n",
    "\n",
    "class PJF():\n",
    "    def __init__(self):\n",
    "        self.search_url = []\n",
    "        self.cv_urls = []\n",
    "        self.keywords = ''\n",
    "\n",
    "    def load_cv_data(self, path:str):\n",
    "        # Check if the file exists\n",
    "        try:\n",
    "            self.cv_data = pd.read_csv(path, sep=';')\n",
    "        except FileNotFoundError:\n",
    "            self.cv_data = pd.DataFrame(columns=['Keyword', 'ID', 'Link', 'Title', 'Location', 'PostedDate'])\n",
    "        \n",
    "    def search(self, allwords:str='', nowords:str='', title:str='', textwords:str='', location:str='', radius:int=500, r:int=100):\n",
    "        for p in range(1, 6):\n",
    "            _url = \"https://www.postjobfree.com/resumes?q=\"+allwords.replace(' ', '+')+\"&n=\"+nowords.replace(' ', '+')+\"&t=\"+title.replace(' ', '+')+\"&d=\"+textwords.replace(' ', '+')+\"&l=\"+location.replace(' ', '+')+\"&radius=\"+str(radius)+\"&r=\"+str(r)+\"&p=\"+str(p)\n",
    "            self.search_url.append(_url)\n",
    "\n",
    "        for url in self.search_url:\n",
    "            try:\n",
    "                self.get_resume_links(url)\n",
    "            except Exception as e:\n",
    "                print(\"Error in scraping\", url)\n",
    "            time.sleep(0.5)\n",
    "        print(\"Total number of resume:\", len(self.cv_urls))\n",
    "\n",
    "    def scraping(self):\n",
    "        # Initialize the dataframe or load the existing one\n",
    "        self.load_cv_data('cv_data.csv')\n",
    "        for i, link in enumerate(self.cv_urls):\n",
    "            print(f\"{self.keywords}: Scraping link {i+1} / {len(self.cv_urls)}\")\n",
    "            if len(self.cv_data) > 0:\n",
    "                unique_links = self.cv_data['Link'].unique()\n",
    "                # Loop through the links\n",
    "                if link in unique_links:\n",
    "                    print(f\"Link {i+1} already scraped\")\n",
    "                    continue\n",
    "                else:\n",
    "                    self.get_resume_content(link)\n",
    "            else:\n",
    "                self.get_resume_content(link)\n",
    "                \n",
    "\n",
    "    def search_and_scraping(self, allwords:str='', nowords:str='', title:str='', textwords:str='', location:str='', radius:int=500, r:int=100):\n",
    "        self.keywords = title\n",
    "        self.search(allwords, nowords, title, textwords, location, radius, r)\n",
    "        self.scraping()\n",
    "\n",
    "    def get_resume_links(self, search_url:str):\n",
    "        page = requests.get(search_url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        link_elements = soup.find_all('h3', class_='itemTitle')\n",
    "        for link in link_elements:\n",
    "            _combined_link = \"https://www.postjobfree.com\" + link.a['href']\n",
    "            if _combined_link not in self.cv_urls:\n",
    "                self.cv_urls.append(_combined_link)\n",
    "            else:\n",
    "                print(\"Duplicate link found\", link.a['href'])\n",
    "\n",
    "    def save_to_new_txt(self, text:str, filename:str):\n",
    "        # Define the directory path\n",
    "        title = self.keywords.replace(' ', '_').lower()\n",
    "        directory = f'.../postjobfree/{title}'\n",
    "        # Ensure the directory exists\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        \n",
    "        # Create the file and write text\n",
    "        file_path = os.path.join(directory, f\"{filename}.txt\")\n",
    "        try:\n",
    "            with open(file_path, 'w') as f:\n",
    "                f.write(text)\n",
    "            print(f\"File saved successfully at {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving the file: {e}\")\n",
    "\n",
    "    def get_resume_content(self, resume_link:str):\n",
    "        try:\n",
    "            page = requests.get(resume_link)\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            _title = soup.find('h1').text\n",
    "            _location = soup.find('a', class_='colorLocation').text\n",
    "            _posted_data = soup.find('span', class_='colorDate').text\n",
    "            _text = [element.text for element in soup.find_all('div', class_='normalText')]\n",
    "            _text = '\\n'.join(_text)\n",
    "            _text = _text.replace(';', ',')\n",
    "\n",
    "            # Save _text to a text file\n",
    "            self.save_to_new_txt(_text, resume_link[35:].replace(\"/\", \"_\"))\n",
    "            \n",
    "            # Fix: wrap scalar values in lists to create a single-row DataFrame\n",
    "            _df = pd.DataFrame({\n",
    "                'Keyword': [self.keywords],\n",
    "                'ID': [resume_link[35:].replace('/', '_')],\n",
    "                'Link': [resume_link],\n",
    "                'Title': [_title],\n",
    "                'Location': [_location],\n",
    "                'PostedDate': [_posted_data],\n",
    "            })\n",
    "\n",
    "            # Concatenate with the existing cv_data\n",
    "            self.cv_data = pd.concat([self.cv_data, _df], ignore_index=True)\n",
    "            # Save the dataframe to a csv file\n",
    "            self.cv_data.to_csv('cv_data.csv', index=False, sep=';')\n",
    "            print(f\"Scraped link:\", resume_link[27:])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"Error in scraping {resume_link}\")\n",
    "        time.sleep(0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResumeCraw = PJF()\n",
    "keywords = ['Software Engineer', 'Frontend Developer', 'Backend Developer', 'Full Stack Developer', 'Mobile Developer', \n",
    "            'DevOps Engineer', 'Embedded Systems Engineer', 'Data Scientist', 'Data Engineer', 'Data Analyst', 'Machine Learning Engineer', \n",
    "            'AI Research Scientist', 'Product Manager', 'Technical Program Manager', 'UX/UI Designer', 'Interaction Designer', 'Cloud Engineer', \n",
    "            'Site Reliability Engineer (SRE)', 'Infrastructure Engineer', 'Network Engineer', 'Cybersecurity Engineer', 'Security Analyst', 'Penetration Tester', \n",
    "            'Information Security Manager', 'QA Engineer', 'Test Automation Engineer', 'Performance Engineer', 'Research Scientist', 'Algorithm Engineer', 'IT Support Specialist', \n",
    "            'Systems Administrator', 'Database Administrator (DBA)', 'Robotics Engineer', 'NLP Engineer', 'Computer Vision Engineer', 'Blockchain Developer', 'Cryptography Engineer', \n",
    "            'Technical Writer', 'AI Operations Engineer', 'Game Developer', 'Game Engine Developer', 'Solutions Architect', 'Technical Consultant', 'BI Developer', 'BI Analyst',\n",
    "            'Product Manager', 'Technical Program Manager', 'Project Manager', 'Engineering Manager', 'Development Manager', 'IT Manager', \n",
    "            'Operations Manager', 'Quality Assurance Manager', 'Data Science Manager', 'Data Engineering Manager', 'Security Manager', 'Sales Engineering Manager', \n",
    "            'Customer Success Manager', 'Marketing Manager', 'Technical Support Manager', 'Change Manager', 'DevOps Manager', 'Compliance Manager', 'Release Manager', \n",
    "            'Business Intelligence Manager', 'Digital Marketing Manager', 'HR Manager', 'Talent Acquisition Manager', 'Training and Development Manager', 'Finance Manager', \n",
    "            'Risk Manager', 'Strategy Manager', 'Product Marketing Manager'\n",
    "            ]\n",
    "for i, keyword in enumerate(keywords):\n",
    "    print(f\"Scraping keyword {i+1} / {len(keywords)}\")\n",
    "    ResumeCraw.search_and_scraping(title=keyword)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "booksage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
